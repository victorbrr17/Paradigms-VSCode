{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6899a31-5c75-4956-b87f-d39bb7ae42de",
   "metadata": {},
   "source": [
    "# Practical Activity 1 (**PRA1**)\n",
    "\n",
    "## Evaluable Practical Exercise\n",
    "\n",
    "<u>General considerations</u>:\n",
    "\n",
    "- The proposed solution cannot use methods, functions or parameters declared **_deprecated_** in future versions.\n",
    "- This activity must be carried out on a **strictly individual** basis. Any indication of copying will be penalized with a failure for all parties involved and the possible negative evaluation of the subject in its entirety.\n",
    "- It is necessary for the student to indicate **all the sources** that she/he has used to carry out the PRA. If not, the student will be considered to have committed plagiarism, being penalized with a failure and the possible negative evaluation of the subject in its entirety.\n",
    "\n",
    "<u>Delivery format</u>:\n",
    "\n",
    "- Some exercises may require several minutes of execution, so the delivery must be done in **Notebook format** and in **HTML format**, where the code, results and comments of each exercise can be seen. You can export the notebook to HTML from the menu File $\\to$ Download as $\\to$ HTML.\n",
    "- There is a special type of cell to hold text. This type of cell will be very useful to answer the different theoretical questions posed throughout the activity. To change the cell type to this type, in the menu: Cell $\\to$ Cell Type $\\to$ Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1d0b9-51a4-4e3c-9bd8-0a8fd47d5a0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Name and surname: Victor Brao Ruiz </strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f1545-5222-41a9-9a84-f7ed1d93cf4b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Blackjack environment is part of the Gymnasium's [Toy Text](https://gymnasium.farama.org/environments/toy_text/) environments. Blackjack is a card game where the goal is to beat the dealer by obtaining cards that sum to closer to 21 (without going over 21) than the dealer's cards.\n",
    "\n",
    "The card values are, as depicted in the following figure:\n",
    "- Face cards (Jack, Queen, King) have a point value of **10**.\n",
    "- Aces can either count as **11** (called a \"usable ace\") or **1**.\n",
    "- Numerical cards (**2-9**) have a value equal to their number.\n",
    "\n",
    "<img src=\"./figs/BlackJackCards.png\" />\n",
    "\n",
    "Game Dynamics:\n",
    "1. The game starts with the dealer having one face up and one face down card, while the player has two face up cards. All cards are drawn from an infinite deck (i.e. with replacement).\n",
    "2. The player has a total sum of cards. They can request additional cards (**hit**) until they decide to stop (**stick**) or exceed 21 (**bust**), which results in an immediate loss.\n",
    "3. After the player decides to stick, the dealer reveals their face-down card and draws cards until their total is 17 or greater. If the dealer goes bust, the player wins.\n",
    "4. If neither the player nor the dealer goes bust, the winner is whoever has a sum closer to 21.\n",
    "\n",
    "Further information could be found at:\n",
    "- Gymnasium [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/)\n",
    "\n",
    "In order to initialize the environment, we will use `natural=True` to give an additional reward for starting with a natural blackjack, i.e. starting with an ace and ten (sum is 21), as depicted in the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "acf8ef26-79d7-4600-abb4-bd82cf12f8fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('Blackjack-v1', natural=True, sab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a6d932fb-fbf0-4d42-a50d-f6887211e1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(2) \n",
      "Observation space is Tuple(Discrete(32), Discrete(11), Discrete(2)) \n"
     ]
    }
   ],
   "source": [
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77be78-4195-4282-81d2-3ccb63dd9e87",
   "metadata": {},
   "source": [
    "## Part 1. Naïve Policy\n",
    "\n",
    "Implement an agent that carries out the following deterministic policy: \n",
    "- The agent will **stick** if it gets a score of 20 or 21.\n",
    "- Otherwise, it will **hit**.\n",
    "\n",
    "<u>Questions</u> (**1 point**): \n",
    "1. Using this agent, simulate 100,000 games and calculate the agent's return (total accumulated reward).\n",
    "2. Additionally, calculate the % of wins, natural wins, losses and draws. \n",
    "3. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d1131d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accumulated reward: -33285.5\n",
      "Win percentage: 29.32%\n",
      "Natural win percentage: 4.21%\n",
      "Loss percentage: 64.71%\n",
      "Draw percentage: 5.97%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def naive_policy(player_sum):\n",
    "    return 0 if player_sum >= 20 else 1  # 0=stick, 1=hit\n",
    "\n",
    "\n",
    "n_episodes = 100000\n",
    "total_reward = 0\n",
    "wins = 0\n",
    "natural_wins = 0\n",
    "losses = 0\n",
    "draws = 0\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        player_sum = state[0]\n",
    "        action = naive_policy(player_sum)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if reward == 1.0:\n",
    "        wins += 1\n",
    "    elif reward == 1.5:\n",
    "        natural_wins += 1\n",
    "        wins += 1\n",
    "    elif reward == 0.0:\n",
    "        draws += 1\n",
    "    elif reward == -1.0:\n",
    "        losses += 1\n",
    "\n",
    "print(f\"Total accumulated reward: {total_reward}\")\n",
    "print(f\"Win percentage: {wins / n_episodes * 100:.2f}%\")\n",
    "print(f\"Natural win percentage: {natural_wins / n_episodes * 100:.2f}%\")\n",
    "print(f\"Loss percentage: {losses / n_episodes * 100:.2f}%\")\n",
    "print(f\"Draw percentage: {draws / n_episodes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf10db1",
   "metadata": {},
   "source": [
    "With this first basic policy implementation we have a loss percentage of almost 65%, with a win % of 29.43%, which also includes the % of natural wins (a 4.13%). As far as I know, the % of natural wins is the only one that souldn't change much in the future, as the probabilities of a natural blackjack are independent of the policy chosen. The results seem pretty consisistent with the policy we have implemented. Of course, the accumulated reward is just a reflection of this low win rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b30e3-dd3d-4fcd-a6e4-baaf764d8f88",
   "metadata": {},
   "source": [
    "## Part 2. Monte Carlo method\n",
    "\n",
    "The objective of this section is to estimate the optimal policy using Monte Carlo methods. Specifically, you can choose and implement one of the algorithms related to _Control using MC methods_ (with ''exploring starts'' or without ''exploring starts'', both on-policy or off-policy).\n",
    "\n",
    "<u>Questions</u> (**2.5 points**): \n",
    "1. Implement the selected algorithm and justify your choice.\n",
    "2. Comment and justify all the parameters, such as:\n",
    "- Number of episodes\n",
    "- Discount factor\n",
    "- Etc.\n",
    "3. Implement a function that prints on the screen the optimal policy found for each state (similar to the figure in Section 3.1).\n",
    "4. Using the trained agent, simulate 100,000 games and calculate the agent's return (total accumulated reward).\n",
    "5. Additionally, calculate the % of wins, natural wins, losses and draws."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa0aae",
   "metadata": {},
   "source": [
    "For this part i will implement a On-policy first-visit Monte Carlo Control with $\\epsilon$-soft policy, without exploring starts. The justification is that its a simple policy that can be useful for a game like blackjack, with a small state space where we can mantain a table of Q values. And $\\epsilon$-soft ensures all actions have a non-zero probability, so we ensure exploration.\n",
    "\n",
    "I will start with 100.000 episodes, just to check if its working and end with 500.000, the same used for the CliffWalking example, that should be enough. Discount Factor = 1 as in blackjack we care about the only reward at the end of each episode. For epsilon, i will start with a value of Epsilon = 0.1 just to ensure exploration. (maybe to be changed in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "32a6904e-2cd2-4f95-b2ce-4fb8a9dd7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, n_actions):\n",
    "   \n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(n_actions, dtype=float) * epsilon / n_actions\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "def mc_control_on_policy_epsilon_greedy(env, num_episodes, discount=1.0, epsilon=0.1):\n",
    "\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "        episode = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # First-visit Monte Carlo update\n",
    "        visited_state_actions = set()\n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = discount * G + reward\n",
    "            if (state, action) not in visited_state_actions:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1.0\n",
    "                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "                visited_state_actions.add((state, action))\n",
    "\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b1de6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "num_episodes = 500000\n",
    "discount_factor = 1.0\n",
    "epsilon = 0.1\n",
    "\n",
    "Q_mc, policy = mc_control_on_policy_epsilon_greedy(env, num_episodes, discount_factor, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0c088441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy with usable ace (S=stick, H=hit):\n",
      "[['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'H' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'S' 'H' 'S' 'S' 'S' 'S']\n",
      " ['H' 'H' 'H' 'H' 'S' 'H' 'S' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']]\n",
      "\n",
      "Policy without usable ace (S=stick, H=hit):\n",
      "[['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['H' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['H' 'S' 'S' 'S' 'S' 'S' 'H' 'H' 'S' 'S']\n",
      " ['H' 'H' 'S' 'S' 'S' 'S' 'H' 'H' 'H' 'H']\n",
      " ['H' 'S' 'S' 'S' 'S' 'S' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'S' 'S' 'S' 'S' 'H' 'H' 'H' 'H']\n",
      " ['H' 'S' 'H' 'S' 'H' 'S' 'H' 'H' 'H' 'H']]\n"
     ]
    }
   ],
   "source": [
    "#Display policy\n",
    "def print_policy(Q):\n",
    "    usable_ace = np.full((10, 10), \" \")\n",
    "    no_usable_ace = np.full((10, 10), \" \")\n",
    "\n",
    "    for player_sum in range(12, 22):\n",
    "        for dealer_card in range(1, 11):\n",
    "            for ace in [True, False]:\n",
    "                state = (player_sum, dealer_card, ace)\n",
    "                if state in Q:\n",
    "                    action = np.argmax(Q[state])\n",
    "                    symbol = \"S\" if action == 0 else \"H\"\n",
    "                    if ace:\n",
    "                        usable_ace[player_sum - 12, dealer_card - 1] = symbol\n",
    "                    else:\n",
    "                        no_usable_ace[player_sum - 12, dealer_card - 1] = symbol\n",
    "\n",
    "    print(\"\\nPolicy with usable ace (S=stick, H=hit):\")\n",
    "    print(np.flipud(usable_ace))\n",
    "    print(\"\\nPolicy without usable ace (S=stick, H=hit):\")\n",
    "    print(np.flipud(no_usable_ace))\n",
    "\n",
    "print_policy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "53e5d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total accumulated reward: -15564.0\n",
      "Win percentage: 42.88%\n",
      "Natural win percentage: 4.16%\n",
      "Loss percentage: 48.07%\n",
      "Draw percentage: 9.05%\n"
     ]
    }
   ],
   "source": [
    "#Metrics\n",
    "episodes_eval = 500000\n",
    "total_reward = 0\n",
    "wins = 0\n",
    "natural_wins = 0\n",
    "losses = 0\n",
    "draws = 0\n",
    "\n",
    "for _ in range(episodes_eval):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state])\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    total_reward += reward\n",
    "    if reward == 1.0:\n",
    "        wins += 1\n",
    "    elif reward == 1.5:\n",
    "        natural_wins += 1\n",
    "        wins += 1\n",
    "    elif reward == 0.0:\n",
    "        draws += 1\n",
    "    elif reward == -1.0:\n",
    "        losses += 1\n",
    "\n",
    "print(f\"\\nTotal accumulated reward: {total_reward}\")\n",
    "print(f\"Win percentage: {wins / episodes_eval * 100:.2f}%\")\n",
    "print(f\"Natural win percentage: {natural_wins / episodes_eval * 100:.2f}%\")\n",
    "print(f\"Loss percentage: {losses / episodes_eval * 100:.2f}%\")\n",
    "print(f\"Draw percentage: {draws / episodes_eval * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c84c1",
   "metadata": {},
   "source": [
    "Now the performance has improved a lot (if we take into account that the optimal result will be around 50%), and the full printed policy kind of reminds of the optimal policy from Sutton & Barto. Again, the % of wins includes both normal wins and natural wins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb6acf-25c2-4f01-b2da-ca7b16435266",
   "metadata": {},
   "source": [
    "## Part 3. TD learning\n",
    "\n",
    "The objective of this section is to estimate the optimal policy using TD learning methods. Specifically, you have to implement the **SARSA algorithm**.\n",
    "\n",
    "<u>Questions</u> (**2.5 points**): \n",
    "1. Implement the algorithm.\n",
    "2. Comment and justify all the parameters.\n",
    "3. Print on the screen the optimal policy found for each state.\n",
    "4. Using the trained agent, simulate 100,000 games and calculate the agent's return (total accumulated reward).\n",
    "5. Additionally, calculate the % of wins, natural wins, losses and draws."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc8e54",
   "metadata": {},
   "source": [
    "For the SARSA algorithm I will train with 500.000 episodes, same as before, as it looks like its working well. Discount Factor = 1 as in blackjack we care about the only reward at the end of each episode. For epsilon, i will start with a value of Epsilon = 0.1 and a Learning Rate = 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6aaa4fee-c8e8-4ffa-b8e9-5c75cde0aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
    "    probs = np.ones(nA) * epsilon / nA\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += 1.0 - epsilon\n",
    "    return probs\n",
    "\n",
    "\n",
    "def SARSA(env, episodes=500000, alpha=0.01, gamma=1.0, epsilon=0.1):\n",
    "\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    for i_episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
    "        action = np.random.choice(np.arange(env.action_space.n), p=probs)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if not done:\n",
    "                next_probs = epsilon_greedy_policy(Q, next_state, env.action_space.n, epsilon)\n",
    "                next_action = np.random.choice(np.arange(env.action_space.n), p=next_probs)\n",
    "                td_target = reward + gamma * Q[next_state][next_action]\n",
    "            else:\n",
    "                td_target = reward\n",
    "\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_error\n",
    "\n",
    "            state = next_state\n",
    "            if not done:\n",
    "                action = next_action\n",
    "\n",
    "    policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bfa4d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics\n",
    "def evaluate_policy(env, policy, n_episodes=500000):\n",
    "    total_reward = 0\n",
    "    wins = 0\n",
    "    natural_wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.get(state, 1)  # default = Hit\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        if reward == 1.0:\n",
    "            wins += 1\n",
    "        elif reward == 1.5:\n",
    "            natural_wins += 1\n",
    "            wins += 1\n",
    "        elif reward == 0.0:\n",
    "            draws += 1\n",
    "        elif reward == -1.0:\n",
    "            losses += 1\n",
    "\n",
    "    print(f\"\\nEvaluation results over {n_episodes} episodes:\")\n",
    "    print(f\"Total accumulated reward: {total_reward}\")\n",
    "    print(f\"Win percentage: {wins / n_episodes * 100:.2f}%\")\n",
    "    print(f\"Natural win percentage: {natural_wins / n_episodes * 100:.2f}%\")\n",
    "    print(f\"Loss percentage: {losses / n_episodes * 100:.2f}%\")\n",
    "    print(f\"Draw percentage: {draws / n_episodes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "67aec607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display policy\n",
    "\n",
    "def print_policy2(policy):\n",
    "    usable_ace = np.full((10, 10), \" \")\n",
    "    no_usable_ace = np.full((10, 10), \" \")\n",
    "\n",
    "    for player_sum in range(12, 22):\n",
    "        for dealer_card in range(1, 11):\n",
    "            for ace in [True, False]:\n",
    "                state = (player_sum, dealer_card, ace)\n",
    "                if state in Q:\n",
    "                    action = np.argmax(Q[state])\n",
    "                    symbol = \"S\" if action == 0 else \"H\"\n",
    "                    if ace:\n",
    "                        usable_ace[player_sum - 12, dealer_card - 1] = symbol\n",
    "                    else:\n",
    "                        no_usable_ace[player_sum - 12, dealer_card - 1] = symbol\n",
    "\n",
    "    print(\"\\nPolicy with usable ace (S=stick, H=hit):\")\n",
    "    print(np.flipud(usable_ace))\n",
    "    print(\"\\nPolicy without usable ace (S=stick, H=hit):\")\n",
    "    print(np.flipud(no_usable_ace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8a71e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy with usable ace (S=stick, H=hit):\n",
      "[['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'H' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'S' 'H' 'S' 'S' 'S' 'S']\n",
      " ['H' 'H' 'H' 'H' 'S' 'H' 'S' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']]\n",
      "\n",
      "Policy without usable ace (S=stick, H=hit):\n",
      "[['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['H' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S']\n",
      " ['H' 'S' 'S' 'S' 'S' 'S' 'H' 'H' 'S' 'S']\n",
      " ['H' 'H' 'S' 'S' 'S' 'S' 'H' 'H' 'H' 'H']\n",
      " ['H' 'S' 'S' 'S' 'S' 'S' 'H' 'H' 'H' 'H']\n",
      " ['H' 'H' 'S' 'S' 'S' 'S' 'H' 'H' 'H' 'H']\n",
      " ['H' 'S' 'H' 'S' 'H' 'S' 'H' 'H' 'H' 'H']]\n",
      "\n",
      "Evaluation results over 500000 episodes:\n",
      "Total accumulated reward: -16089.0\n",
      "Win percentage: 42.81%\n",
      "Natural win percentage: 4.17%\n",
      "Loss percentage: 48.11%\n",
      "Draw percentage: 9.08%\n"
     ]
    }
   ],
   "source": [
    "#Training and printing policy\n",
    "Q_sarsa, policy = SARSA(env, episodes=500000, alpha=0.01, gamma=1.0, epsilon=0.1)\n",
    "print_policy2(policy)\n",
    "evaluate_policy(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c77db3-3fd1-4454-8079-e2eb44942758",
   "metadata": {},
   "source": [
    "## Part 4. Comparison of the algorithms\n",
    "\n",
    "In this section, we will make a comparison among the algorithms.\n",
    "\n",
    "We will compare the performance of the algorithms when changing the number of episodes, the discount factor and the *learning rate* values (in the case of the SARSA method).\n",
    "\n",
    "For each exercise, the results must be presented and justified.\n",
    "\n",
    "**Note**: \n",
    "- It is recommended to run the simulations multiple times for each exercise, as these are random, and to comment on the most frequent result or the average of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1e4c9-29a8-4552-a463-e4e6cf2b7e91",
   "metadata": {},
   "source": [
    "### 4.1. Comparison to the optimal policy\n",
    "\n",
    "The optimal policy for this problem, described by [Sutton & Barto](http://incompleteideas.net/book/the-book-2nd.html) is depicted in the following image:\n",
    "\n",
    "<img src=\"./figs/optimal.png\" style=\"width: 800px;\" />\n",
    "\n",
    "<u>Questions</u> (**1 point**): \n",
    "- Compare the _optimal_ policies of the naïve, Monte Carlo and SARSA methods to the optimal one provided by Sutton & Barto.\n",
    "- Comment on the results and justify your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2ddbbf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(12, 1, True): 1,\n",
       " (12, 2, True): 1,\n",
       " (12, 3, True): 1,\n",
       " (12, 4, True): 1,\n",
       " (12, 5, True): 1,\n",
       " (12, 6, True): 1,\n",
       " (12, 7, True): 1,\n",
       " (12, 8, True): 1,\n",
       " (12, 9, True): 1,\n",
       " (12, 10, True): 1,\n",
       " (13, 1, True): 1,\n",
       " (13, 2, True): 1,\n",
       " (13, 3, True): 1,\n",
       " (13, 4, True): 1,\n",
       " (13, 5, True): 1,\n",
       " (13, 6, True): 1,\n",
       " (13, 7, True): 1,\n",
       " (13, 8, True): 1,\n",
       " (13, 9, True): 1,\n",
       " (13, 10, True): 1,\n",
       " (14, 1, True): 1,\n",
       " (14, 2, True): 1,\n",
       " (14, 3, True): 1,\n",
       " (14, 4, True): 1,\n",
       " (14, 5, True): 1,\n",
       " (14, 6, True): 1,\n",
       " (14, 7, True): 1,\n",
       " (14, 8, True): 1,\n",
       " (14, 9, True): 1,\n",
       " (14, 10, True): 1,\n",
       " (15, 1, True): 1,\n",
       " (15, 2, True): 1,\n",
       " (15, 3, True): 1,\n",
       " (15, 4, True): 1,\n",
       " (15, 5, True): 1,\n",
       " (15, 6, True): 1,\n",
       " (15, 7, True): 1,\n",
       " (15, 8, True): 1,\n",
       " (15, 9, True): 1,\n",
       " (15, 10, True): 1,\n",
       " (16, 1, True): 1,\n",
       " (16, 2, True): 1,\n",
       " (16, 3, True): 1,\n",
       " (16, 4, True): 1,\n",
       " (16, 5, True): 1,\n",
       " (16, 6, True): 1,\n",
       " (16, 7, True): 1,\n",
       " (16, 8, True): 1,\n",
       " (16, 9, True): 1,\n",
       " (16, 10, True): 1,\n",
       " (17, 1, True): 1,\n",
       " (17, 2, True): 1,\n",
       " (17, 3, True): 1,\n",
       " (17, 4, True): 1,\n",
       " (17, 5, True): 1,\n",
       " (17, 6, True): 1,\n",
       " (17, 7, True): 1,\n",
       " (17, 8, True): 1,\n",
       " (17, 9, True): 1,\n",
       " (17, 10, True): 1,\n",
       " (18, 1, True): 1,\n",
       " (18, 2, True): 0,\n",
       " (18, 3, True): 0,\n",
       " (18, 4, True): 0,\n",
       " (18, 5, True): 0,\n",
       " (18, 6, True): 0,\n",
       " (18, 7, True): 0,\n",
       " (18, 8, True): 0,\n",
       " (18, 9, True): 1,\n",
       " (18, 10, True): 1,\n",
       " (19, 1, True): 0,\n",
       " (19, 2, True): 0,\n",
       " (19, 3, True): 0,\n",
       " (19, 4, True): 0,\n",
       " (19, 5, True): 0,\n",
       " (19, 6, True): 0,\n",
       " (19, 7, True): 0,\n",
       " (19, 8, True): 0,\n",
       " (19, 9, True): 0,\n",
       " (19, 10, True): 0,\n",
       " (20, 1, True): 0,\n",
       " (20, 2, True): 0,\n",
       " (20, 3, True): 0,\n",
       " (20, 4, True): 0,\n",
       " (20, 5, True): 0,\n",
       " (20, 6, True): 0,\n",
       " (20, 7, True): 0,\n",
       " (20, 8, True): 0,\n",
       " (20, 9, True): 0,\n",
       " (20, 10, True): 0,\n",
       " (21, 1, True): 0,\n",
       " (21, 2, True): 0,\n",
       " (21, 3, True): 0,\n",
       " (21, 4, True): 0,\n",
       " (21, 5, True): 0,\n",
       " (21, 6, True): 0,\n",
       " (21, 7, True): 0,\n",
       " (21, 8, True): 0,\n",
       " (21, 9, True): 0,\n",
       " (21, 10, True): 0,\n",
       " (12, 1, False): 1,\n",
       " (12, 2, False): 1,\n",
       " (12, 3, False): 1,\n",
       " (12, 4, False): 0,\n",
       " (12, 5, False): 0,\n",
       " (12, 6, False): 0,\n",
       " (12, 7, False): 1,\n",
       " (12, 8, False): 1,\n",
       " (12, 9, False): 1,\n",
       " (12, 10, False): 1,\n",
       " (13, 1, False): 1,\n",
       " (13, 2, False): 0,\n",
       " (13, 3, False): 0,\n",
       " (13, 4, False): 0,\n",
       " (13, 5, False): 0,\n",
       " (13, 6, False): 0,\n",
       " (13, 7, False): 1,\n",
       " (13, 8, False): 1,\n",
       " (13, 9, False): 1,\n",
       " (13, 10, False): 1,\n",
       " (14, 1, False): 1,\n",
       " (14, 2, False): 0,\n",
       " (14, 3, False): 0,\n",
       " (14, 4, False): 0,\n",
       " (14, 5, False): 0,\n",
       " (14, 6, False): 0,\n",
       " (14, 7, False): 1,\n",
       " (14, 8, False): 1,\n",
       " (14, 9, False): 1,\n",
       " (14, 10, False): 1,\n",
       " (15, 1, False): 1,\n",
       " (15, 2, False): 0,\n",
       " (15, 3, False): 0,\n",
       " (15, 4, False): 0,\n",
       " (15, 5, False): 0,\n",
       " (15, 6, False): 0,\n",
       " (15, 7, False): 1,\n",
       " (15, 8, False): 1,\n",
       " (15, 9, False): 1,\n",
       " (15, 10, False): 1,\n",
       " (16, 1, False): 1,\n",
       " (16, 2, False): 0,\n",
       " (16, 3, False): 0,\n",
       " (16, 4, False): 0,\n",
       " (16, 5, False): 0,\n",
       " (16, 6, False): 0,\n",
       " (16, 7, False): 1,\n",
       " (16, 8, False): 1,\n",
       " (16, 9, False): 1,\n",
       " (16, 10, False): 1,\n",
       " (17, 1, False): 0,\n",
       " (17, 2, False): 0,\n",
       " (17, 3, False): 0,\n",
       " (17, 4, False): 0,\n",
       " (17, 5, False): 0,\n",
       " (17, 6, False): 0,\n",
       " (17, 7, False): 0,\n",
       " (17, 8, False): 0,\n",
       " (17, 9, False): 0,\n",
       " (17, 10, False): 0,\n",
       " (18, 1, False): 0,\n",
       " (18, 2, False): 0,\n",
       " (18, 3, False): 0,\n",
       " (18, 4, False): 0,\n",
       " (18, 5, False): 0,\n",
       " (18, 6, False): 0,\n",
       " (18, 7, False): 0,\n",
       " (18, 8, False): 0,\n",
       " (18, 9, False): 0,\n",
       " (18, 10, False): 0,\n",
       " (19, 1, False): 0,\n",
       " (19, 2, False): 0,\n",
       " (19, 3, False): 0,\n",
       " (19, 4, False): 0,\n",
       " (19, 5, False): 0,\n",
       " (19, 6, False): 0,\n",
       " (19, 7, False): 0,\n",
       " (19, 8, False): 0,\n",
       " (19, 9, False): 0,\n",
       " (19, 10, False): 0,\n",
       " (20, 1, False): 0,\n",
       " (20, 2, False): 0,\n",
       " (20, 3, False): 0,\n",
       " (20, 4, False): 0,\n",
       " (20, 5, False): 0,\n",
       " (20, 6, False): 0,\n",
       " (20, 7, False): 0,\n",
       " (20, 8, False): 0,\n",
       " (20, 9, False): 0,\n",
       " (20, 10, False): 0,\n",
       " (21, 1, False): 0,\n",
       " (21, 2, False): 0,\n",
       " (21, 3, False): 0,\n",
       " (21, 4, False): 0,\n",
       " (21, 5, False): 0,\n",
       " (21, 6, False): 0,\n",
       " (21, 7, False): 0,\n",
       " (21, 8, False): 0,\n",
       " (21, 9, False): 0,\n",
       " (21, 10, False): 0}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "opt_usable_ace = np.array([\n",
    "    [\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"H\",\"H\"],\n",
    "    [\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\"],\n",
    "    [\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\"],\n",
    "    [\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\"]\n",
    "])\n",
    "\n",
    "opt_no_usable_ace = np.array([\n",
    "    [\"H\",\"H\",\"H\",\"S\",\"S\",\"S\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"S\",\"S\",\"S\",\"S\",\"S\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"S\",\"S\",\"S\",\"S\",\"S\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"S\",\"S\",\"S\",\"S\",\"S\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"H\",\"S\",\"S\",\"S\",\"S\",\"S\",\"H\",\"H\",\"H\",\"H\"],\n",
    "    [\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\"],\n",
    "    [\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\"],\n",
    "    [\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\"],\n",
    "    [\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\"],\n",
    "    [\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\",\"S\"]\n",
    "])\n",
    "\n",
    "def convert_matrix(matrix, usable_ace):\n",
    "    policy_dict = {}\n",
    "    for i, player_row in enumerate(matrix):\n",
    "        player_sum = i + 12\n",
    "        for j, action in enumerate(player_row):\n",
    "            dealer_card = j + 1\n",
    "            policy_dict[(player_sum, dealer_card, usable_ace)] = 0 if action == 'S' else 1\n",
    "    return policy_dict\n",
    "\n",
    "optimal_policy = {}\n",
    "optimal_policy.update(convert_matrix(opt_usable_ace, usable_ace=True))\n",
    "optimal_policy.update(convert_matrix(opt_no_usable_ace, usable_ace=False))\n",
    "optimal_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6148630a-af42-4b8a-9754-088340fa65a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agreements for Naive: 130\n",
      "Number of agreements for MC: 186\n",
      "Number of agreements for SARSA: 184\n",
      "Number of disagreements for Naive: 70\n",
      "Number of disagreements for MC: 14\n",
      "Number of disagreements for SARSA: 16\n",
      "MC agreement %: 93.0\n",
      "SARSA agreement %: 92.0\n",
      "Naive agreement %: 65.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy_from_Q_or_map(Q_or_map):\n",
    "\n",
    "    def fn(state):\n",
    "        if state in Q_or_map:\n",
    "            val = Q_or_map[state]\n",
    "            if isinstance(val, (list, np.ndarray)):\n",
    "                return int(np.argmax(val))\n",
    "            else:\n",
    "                return int(val)\n",
    "        player_sum = state[0]\n",
    "        return 0 if player_sum >= 20 else 1\n",
    "    return fn\n",
    "\n",
    "policy_mc_fn = policy_from_Q_or_map(Q_mc)\n",
    "policy_sarsa_fn = policy_from_Q_or_map(Q_sarsa)\n",
    "policy_naive_fn = lambda s: 0 if s[0] >= 20 else 1\n",
    "\n",
    "def compare_to_reference(policy_fn, reference_map):\n",
    "\n",
    "    states = []\n",
    "    actions_learned = []\n",
    "    actions_ref = []\n",
    "    disagreements = []\n",
    "\n",
    "    for player_sum in range(12, 22):\n",
    "        for dealer_card in range(1, 11):\n",
    "            for ace in [True, False]:\n",
    "                s = (player_sum, dealer_card, ace)\n",
    "                a_learned = policy_fn(s)\n",
    "                a_ref = reference_map.get(s, None)\n",
    "                if a_ref is None:\n",
    "                    # if reference not provided for this state, skip or treat as unknown\n",
    "                    continue\n",
    "                states.append(s)\n",
    "                actions_learned.append(a_learned)\n",
    "                actions_ref.append(a_ref)\n",
    "                if a_learned != a_ref:\n",
    "                    disagreements.append(s)\n",
    "\n",
    "    total = len(states)\n",
    "    agree = sum(1 for i in range(len(states)) if actions_learned[i] == actions_ref[i])\n",
    "    return {\n",
    "        \"total_states_compared\": total,\n",
    "        \"agreement_count\": agree,\n",
    "        \"agreement_pct\": agree / total * 100.0,\n",
    "        \"disagreements\": disagreements\n",
    "    }\n",
    "\n",
    "\n",
    "res_mc = compare_to_reference(policy_mc_fn, optimal_policy)\n",
    "res_sarsa = compare_to_reference(policy_sarsa_fn, optimal_policy)\n",
    "res_naive = compare_to_reference(policy_naive_fn, optimal_policy)\n",
    "\n",
    "\n",
    "print(\"Number of agreements for Naive:\", res_naive[\"agreement_count\"])\n",
    "print(\"Number of agreements for MC:\", res_mc[\"agreement_count\"])\n",
    "print(\"Number of agreements for SARSA:\", res_sarsa[\"agreement_count\"])\n",
    "\n",
    "print(\"Number of disagreements for Naive:\", len(res_naive[\"disagreements\"]))\n",
    "print(\"Number of disagreements for MC:\", len(res_mc[\"disagreements\"]))\n",
    "print(\"Number of disagreements for SARSA:\", len(res_sarsa[\"disagreements\"]))\n",
    "\n",
    "print(\"MC agreement %:\", res_mc[\"agreement_pct\"])\n",
    "print(\"SARSA agreement %:\", res_sarsa[\"agreement_pct\"])\n",
    "print(\"Naive agreement %:\", res_naive[\"agreement_pct\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e4a86-98bc-4feb-84b0-5cf85a27ca17",
   "metadata": {},
   "source": [
    "### 4.2. Influence of the Number of Episodes\n",
    "\n",
    "Conduct a study by varying the number of episodes in each of the algorithms.\n",
    "\n",
    "<u>Questions</u> (**1 point**): \n",
    "- Train each algorithm multiple times with 100,000, 1,000,000, and 5,000,000 episodes and average the results.\n",
    "- Indicate how the **number of episodes** influences the convergence of each algorithm by calculating the number of states where the policy differs from the optimal one, as well as the average return obtained after playing 100,000 games following each training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3c3d3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy_fn, n_games=100000):\n",
    "    total_reward = 0\n",
    "    wins = natural_wins = losses = draws = 0\n",
    "    for i in range(n_games):\n",
    "        state, info = env.reset()\n",
    "        terminated = truncated = False\n",
    "        while not (terminated or truncated):\n",
    "            action = policy_fn(state)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        if reward == 1.0:\n",
    "            wins += 1\n",
    "        elif reward == 1.5:\n",
    "            natural_wins += 1\n",
    "            wins += 1\n",
    "        elif reward == 0.0:\n",
    "            draws += 1\n",
    "        elif reward == -1.0:\n",
    "            losses += 1\n",
    "\n",
    "    avg_reward = total_reward / n_games\n",
    "    return {\n",
    "        \"avg_reward\": avg_reward,\n",
    "        \"win_pct\": wins / n_games * 100,\n",
    "        \"natural_win_pct\": natural_wins / n_games * 100,\n",
    "        \"loss_pct\": losses / n_games * 100,\n",
    "        \"draw_pct\": draws / n_games * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c4c7e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 100,000 episodes: \n",
      "\n",
      "Monte Carlo (100,000 episodes):\n",
      "  Avg reward: -0.041\n",
      "  Win%: 42.61, Natural win%: 4.19, Loss%: 48.80, Draw%: 8.59\n",
      "  Agreement with optimal policy: 88.00%\n",
      "\n",
      "SARSA (100,000 episodes):\n",
      "  Avg reward: -0.036\n",
      "  Win%: 42.73, Natural win%: 4.11, Loss%: 48.37, Draw%: 8.90\n",
      "  Agreement with optimal policy: 90.00%\n",
      "\n",
      "Naive Policy(ignore):\n",
      "  Avg reward: -0.334\n",
      "  Win%: 29.30, Natural win%: 4.18, Loss%: 64.79, Draw%: 5.91\n",
      "  Agreement with optimal policy: 65.00%\n",
      "\n",
      "Training with 1,000,000 episodes: \n",
      "\n",
      "Monte Carlo (1,000,000 episodes):\n",
      "  Avg reward: -0.023\n",
      "  Win%: 43.10, Natural win%: 4.22, Loss%: 47.50, Draw%: 9.40\n",
      "  Agreement with optimal policy: 97.00%\n",
      "\n",
      "SARSA (1,000,000 episodes):\n",
      "  Avg reward: -0.031\n",
      "  Win%: 42.80, Natural win%: 4.02, Loss%: 47.91, Draw%: 9.29\n",
      "  Agreement with optimal policy: 97.50%\n",
      "\n",
      "Naive Policy(ignore):\n",
      "  Avg reward: -0.336\n",
      "  Win%: 29.19, Natural win%: 4.18, Loss%: 64.92, Draw%: 5.89\n",
      "  Agreement with optimal policy: 65.00%\n",
      "\n",
      "Training with 5,000,000 episodes: \n",
      "\n",
      "Monte Carlo (5,000,000 episodes):\n",
      "  Avg reward: -0.031\n",
      "  Win%: 42.88, Natural win%: 4.14, Loss%: 48.07, Draw%: 9.05\n",
      "  Agreement with optimal policy: 99.00%\n",
      "\n",
      "SARSA (5,000,000 episodes):\n",
      "  Avg reward: -0.033\n",
      "  Win%: 42.48, Natural win%: 4.18, Loss%: 47.90, Draw%: 9.62\n",
      "  Agreement with optimal policy: 93.50%\n",
      "\n",
      "Naive Policy(ignore):\n",
      "  Avg reward: -0.334\n",
      "  Win%: 29.32, Natural win%: 4.23, Loss%: 64.84, Draw%: 5.84\n",
      "  Agreement with optimal policy: 65.00%\n"
     ]
    }
   ],
   "source": [
    "episodes_list = [100000, 1000000, 5000000]\n",
    "n_eval_games = 100000\n",
    "discount_factor = 1.0\n",
    "epsilon = 0.1\n",
    "alpha = 0.01\n",
    "\n",
    "\n",
    "for num_episodes in episodes_list:\n",
    "    print(f\"\\nTraining with {num_episodes:,} episodes: \")\n",
    "\n",
    "    \n",
    "    Q_mc, policy = mc_control_on_policy_epsilon_greedy(env, num_episodes, discount_factor, epsilon)\n",
    "    policy_mc_fn = policy_from_Q_or_map(Q_mc)\n",
    "\n",
    "    \n",
    "    Q_sarsa, policy = SARSA(env, num_episodes, alpha=alpha, gamma=discount_factor, epsilon=epsilon)\n",
    "    policy_sarsa_fn = policy_from_Q_or_map(Q_sarsa)\n",
    "\n",
    "    eval_mc = evaluate_policy(policy_mc_fn, n_eval_games)\n",
    "    eval_sarsa = evaluate_policy(policy_sarsa_fn, n_eval_games)\n",
    "    eval_naive = evaluate_policy(lambda s: 0 if s[0] >= 20 else 1, n_eval_games)\n",
    "\n",
    "    res_mc = compare_to_reference(policy_mc_fn, optimal_policy)\n",
    "    res_sarsa = compare_to_reference(policy_sarsa_fn, optimal_policy)\n",
    "    res_naive = compare_to_reference(lambda s: 0 if s[0] >= 20 else 1, optimal_policy)\n",
    "\n",
    "    print(f\"\\nMonte Carlo ({num_episodes:,} episodes):\")\n",
    "    print(f\"  Avg reward: {eval_mc['avg_reward']:.3f}\")\n",
    "    print(f\"  Win%: {eval_mc['win_pct']:.2f}, Natural win%: {eval_mc['natural_win_pct']:.2f}, Loss%: {eval_mc['loss_pct']:.2f}, Draw%: {eval_mc['draw_pct']:.2f}\")\n",
    "    print(f\"  Agreement with optimal policy: {res_mc['agreement_pct']:.2f}%\")\n",
    "\n",
    "    print(f\"\\nSARSA ({num_episodes:,} episodes):\")\n",
    "    print(f\"  Avg reward: {eval_sarsa['avg_reward']:.3f}\")\n",
    "    print(f\"  Win%: {eval_sarsa['win_pct']:.2f}, Natural win%: {eval_sarsa['natural_win_pct']:.2f}, Loss%: {eval_sarsa['loss_pct']:.2f}, Draw%: {eval_sarsa['draw_pct']:.2f}\")\n",
    "    print(f\"  Agreement with optimal policy: {res_sarsa['agreement_pct']:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nNaive Policy(ignore):\")\n",
    "    print(f\"  Avg reward: {eval_naive['avg_reward']:.3f}\")\n",
    "    print(f\"  Win%: {eval_naive['win_pct']:.2f}, Natural win%: {eval_naive['natural_win_pct']:.2f}, Loss%: {eval_naive['loss_pct']:.2f}, Draw%: {eval_naive['draw_pct']:.2f}\")\n",
    "    print(f\"  Agreement with optimal policy: {res_naive['agreement_pct']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbfc6b9-a71e-4665-b124-88bfd6390aef",
   "metadata": {},
   "source": [
    "### 4.3. Influence of the Discount Factor\n",
    "\n",
    "Conduct a study by varying the *discount factor* in each of the algorithms.\n",
    "\n",
    "<u>Questions</u> (**1 point**):\n",
    "- Run the algorithms with *discount factor* = 0.1, 0.5, 0.9 and the rest of the parameters the same as in previous exercises. \n",
    "- Describe the changes in the optimal policy, comparing the result obtained with the result of previous exercises (*discount factor* = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a09e716e-5234-4e16-8a74-febda9a0b0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with discount factor γ = 0.1: \n",
      "\n",
      "Monte Carlo (γ = 0.1):\n",
      "  Avg reward: -0.031\n",
      "  Win%: 42.93, Natural win%: 4.09, Loss%: 48.08, Draw%: 8.99\n",
      "  Agreement with optimal policy (γ=1.0): 95.50%\n",
      "\n",
      "SARSA (γ = 0.1):\n",
      "  Avg reward: -0.030\n",
      "  Win%: 42.80, Natural win%: 4.25, Loss%: 47.96, Draw%: 9.24\n",
      "  Agreement with optimal policy (γ=1.0): 94.50%\n",
      "\n",
      "Training with discount factor γ = 0.5: \n",
      "\n",
      "Monte Carlo (γ = 0.5):\n",
      "  Avg reward: -0.034\n",
      "  Win%: 42.80, Natural win%: 4.15, Loss%: 48.28, Draw%: 8.92\n",
      "  Agreement with optimal policy (γ=1.0): 95.50%\n",
      "\n",
      "SARSA (γ = 0.5):\n",
      "  Avg reward: -0.030\n",
      "  Win%: 43.01, Natural win%: 4.16, Loss%: 48.07, Draw%: 8.91\n",
      "  Agreement with optimal policy (γ=1.0): 95.50%\n",
      "\n",
      "Training with discount factor γ = 0.9: \n",
      "\n",
      "Monte Carlo (γ = 0.9):\n",
      "  Avg reward: -0.025\n",
      "  Win%: 43.24, Natural win%: 4.15, Loss%: 47.80, Draw%: 8.97\n",
      "  Agreement with optimal policy (γ=1.0): 97.00%\n",
      "\n",
      "SARSA (γ = 0.9):\n",
      "  Avg reward: -0.025\n",
      "  Win%: 43.11, Natural win%: 4.07, Loss%: 47.62, Draw%: 9.27\n",
      "  Agreement with optimal policy (γ=1.0): 96.50%\n"
     ]
    }
   ],
   "source": [
    "discount_factors = [0.1, 0.5, 0.9]\n",
    "num_episodes = 1000000\n",
    "n_eval_games = 100000\n",
    "epsilon = 0.1\n",
    "alpha = 0.01\n",
    "\n",
    "\n",
    "for gamma in discount_factors:\n",
    "    print(f\"\\nTraining with discount factor γ = {gamma}: \")\n",
    "    \n",
    "    Q_mc, _ = mc_control_on_policy_epsilon_greedy(env, num_episodes, gamma, epsilon)\n",
    "    policy_mc_fn = policy_from_Q_or_map(Q_mc)\n",
    "    \n",
    "    Q_sarsa, policy = SARSA(env, num_episodes, alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "    policy_sarsa_fn = policy_from_Q_or_map(Q_sarsa)\n",
    "\n",
    "    eval_mc = evaluate_policy(policy_mc_fn, n_eval_games)\n",
    "    eval_sarsa = evaluate_policy(policy_sarsa_fn, n_eval_games)\n",
    "\n",
    "    res_mc = compare_to_reference(policy_mc_fn, optimal_policy)\n",
    "    res_sarsa = compare_to_reference(policy_sarsa_fn, optimal_policy)\n",
    "\n",
    "    print(f\"\\nMonte Carlo (γ = {gamma}):\")\n",
    "    print(f\"  Avg reward: {eval_mc['avg_reward']:.3f}\")\n",
    "    print(f\"  Win%: {eval_mc['win_pct']:.2f}, Natural win%: {eval_mc['natural_win_pct']:.2f}, Loss%: {eval_mc['loss_pct']:.2f}, Draw%: {eval_mc['draw_pct']:.2f}\")\n",
    "    print(f\"  Agreement with optimal policy (γ=1.0): {res_mc['agreement_pct']:.2f}%\")\n",
    "\n",
    "    print(f\"\\nSARSA (γ = {gamma}):\")\n",
    "    print(f\"  Avg reward: {eval_sarsa['avg_reward']:.3f}\")\n",
    "    print(f\"  Win%: {eval_sarsa['win_pct']:.2f}, Natural win%: {eval_sarsa['natural_win_pct']:.2f}, Loss%: {eval_sarsa['loss_pct']:.2f}, Draw%: {eval_sarsa['draw_pct']:.2f}\")\n",
    "    print(f\"  Agreement with optimal policy (γ=1.0): {res_sarsa['agreement_pct']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354749d4",
   "metadata": {},
   "source": [
    "Here i expected the results to improve as the df incremented, but only the Monte Carlo algorithm improves, SARSA works a little bit worse as the df value goes up to 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9afe8b5-8a12-4c1d-a9ab-75da38fe5aee",
   "metadata": {},
   "source": [
    "### 4.4. Influence of the Learning Rate\n",
    "\n",
    "Conduct a study by varying the learning rate in the *SARSA* algorithm.\n",
    "\n",
    "<u>Questions</u> (**1 point**):\n",
    "- Run the *SARSA* algorithm with the following *learning rate* values: 0.001, 0.01, 0.1, and 0.9.\n",
    "- Analyze the differences with the results obtained previously in terms of the number of errors relative to the optimal policy and the accumulated reward for every 100,000 episodes played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1408beff-0204-42a2-b50f-6c380f24c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning rate α = 0.001: \n",
      "\n",
      "SARSA (Learning rate = 0.001):\n",
      "  Avg reward: -0.032\n",
      "  Win%: 42.72, Natural win%: 4.14, Loss%: 47.97, Draw%: 9.30\n",
      "  Agreement with optimal policy (γ=1.0): 97.00%\n",
      "\n",
      "Training with learning rate α = 0.01: \n",
      "\n",
      "SARSA (Learning rate = 0.01):\n",
      "  Avg reward: -0.031\n",
      "  Win%: 42.68, Natural win%: 4.11, Loss%: 47.86, Draw%: 9.46\n",
      "  Agreement with optimal policy (γ=1.0): 95.50%\n",
      "\n",
      "Training with learning rate α = 0.1: \n",
      "\n",
      "SARSA (Learning rate = 0.1):\n",
      "  Avg reward: -0.039\n",
      "  Win%: 42.41, Natural win%: 4.24, Loss%: 48.44, Draw%: 9.15\n",
      "  Agreement with optimal policy (γ=1.0): 87.00%\n",
      "\n",
      "Training with learning rate α = 0.9: \n",
      "\n",
      "SARSA (Learning rate = 0.9):\n",
      "  Avg reward: -0.060\n",
      "  Win%: 41.41, Natural win%: 3.80, Loss%: 49.34, Draw%: 9.25\n",
      "  Agreement with optimal policy (γ=1.0): 82.00%\n"
     ]
    }
   ],
   "source": [
    "discount_factor = 1\n",
    "num_episodes = 1000000\n",
    "n_eval_games = 100000\n",
    "epsilon = 0.1\n",
    "alpha = [0.001, 0.01, 0.1, 0.9]\n",
    "\n",
    "\n",
    "for lr in alpha:\n",
    "    print(f\"\\nTraining with learning rate α = {lr}: \")\n",
    "\n",
    "    Q_sarsa, policy = SARSA(env, num_episodes, alpha=lr, gamma=discount_factor, epsilon=epsilon)\n",
    "    \n",
    "    policy_sarsa_fn = policy_from_Q_or_map(Q_sarsa)\n",
    "\n",
    "\n",
    "    eval_sarsa = evaluate_policy(policy_sarsa_fn, n_eval_games)\n",
    "\n",
    "    res_sarsa = compare_to_reference(policy_sarsa_fn, optimal_policy)\n",
    "\n",
    "    print(f\"\\nSARSA (Learning rate = {lr}):\")\n",
    "    print(f\"  Avg reward: {eval_sarsa['avg_reward']:.3f}\")\n",
    "    print(f\"  Win%: {eval_sarsa['win_pct']:.2f}, Natural win%: {eval_sarsa['natural_win_pct']:.2f}, Loss%: {eval_sarsa['loss_pct']:.2f}, Draw%: {eval_sarsa['draw_pct']:.2f}\")\n",
    "    print(f\"  Agreement with optimal policy (γ=1.0): {res_sarsa['agreement_pct']:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34e32c",
   "metadata": {},
   "source": [
    "The SARSA algorithm looks to be working worse and worse the bigger the learning rate is, which makes sense. I thought alpha = 0.001 would be too small but apparently, for 1000000 episodes it ends up converging, and it's the one that works the best. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
