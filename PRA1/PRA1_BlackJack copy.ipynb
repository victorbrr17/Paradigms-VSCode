{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6899a31-5c75-4956-b87f-d39bb7ae42de",
   "metadata": {},
   "source": [
    "# Practical Activity 1 (**PRA1**)\n",
    "\n",
    "## Evaluable Practical Exercise\n",
    "\n",
    "<u>General considerations</u>:\n",
    "\n",
    "- The proposed solution cannot use methods, functions or parameters declared **_deprecated_** in future versions.\n",
    "- This activity must be carried out on a **strictly individual** basis. Any indication of copying will be penalized with a failure for all parties involved and the possible negative evaluation of the subject in its entirety.\n",
    "- It is necessary for the student to indicate **all the sources** that she/he has used to carry out the PRA. If not, the student will be considered to have committed plagiarism, being penalized with a failure and the possible negative evaluation of the subject in its entirety.\n",
    "\n",
    "<u>Delivery format</u>:\n",
    "\n",
    "- Some exercises may require several minutes of execution, so the delivery must be done in **Notebook format** and in **HTML format**, where the code, results and comments of each exercise can be seen. You can export the notebook to HTML from the menu File $\\to$ Download as $\\to$ HTML.\n",
    "- There is a special type of cell to hold text. This type of cell will be very useful to answer the different theoretical questions posed throughout the activity. To change the cell type to this type, in the menu: Cell $\\to$ Cell Type $\\to$ Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1d0b9-51a4-4e3c-9bd8-0a8fd47d5a0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Name and surname: Victor Brao Ruiz </strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f1545-5222-41a9-9a84-f7ed1d93cf4b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Blackjack environment is part of the Gymnasium's [Toy Text](https://gymnasium.farama.org/environments/toy_text/) environments. Blackjack is a card game where the goal is to beat the dealer by obtaining cards that sum to closer to 21 (without going over 21) than the dealer's cards.\n",
    "\n",
    "The card values are, as depicted in the following figure:\n",
    "- Face cards (Jack, Queen, King) have a point value of **10**.\n",
    "- Aces can either count as **11** (called a \"usable ace\") or **1**.\n",
    "- Numerical cards (**2-9**) have a value equal to their number.\n",
    "\n",
    "<img src=\"./figs/BlackJackCards.png\" />\n",
    "\n",
    "Game Dynamics:\n",
    "1. The game starts with the dealer having one face up and one face down card, while the player has two face up cards. All cards are drawn from an infinite deck (i.e. with replacement).\n",
    "2. The player has a total sum of cards. They can request additional cards (**hit**) until they decide to stop (**stick**) or exceed 21 (**bust**), which results in an immediate loss.\n",
    "3. After the player decides to stick, the dealer reveals their face-down card and draws cards until their total is 17 or greater. If the dealer goes bust, the player wins.\n",
    "4. If neither the player nor the dealer goes bust, the winner is whoever has a sum closer to 21.\n",
    "\n",
    "Further information could be found at:\n",
    "- Gymnasium [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/)\n",
    "\n",
    "In order to initialize the environment, we will use `natural=True` to give an additional reward for starting with a natural blackjack, i.e. starting with an ace and ten (sum is 21), as depicted in the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acf8ef26-79d7-4600-abb4-bd82cf12f8fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('Blackjack-v1', natural=True, sab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6d932fb-fbf0-4d42-a50d-f6887211e1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(2) \n",
      "Observation space is Tuple(Discrete(32), Discrete(11), Discrete(2)) \n"
     ]
    }
   ],
   "source": [
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77be78-4195-4282-81d2-3ccb63dd9e87",
   "metadata": {},
   "source": [
    "## Part 1. Naïve Policy\n",
    "\n",
    "Implement an agent that carries out the following deterministic policy: \n",
    "- The agent will **stick** if it gets a score of 20 or 21.\n",
    "- Otherwise, it will **hit**.\n",
    "\n",
    "<u>Questions</u> (**1 point**): \n",
    "1. Using this agent, simulate 100,000 games and calculate the agent's return (total accumulated reward).\n",
    "2. Additionally, calculate the % of wins, natural wins, losses and draws. \n",
    "3. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1131d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accumulated reward: -32916.5\n",
      "Win percentage: 29.54%\n",
      "Natural win percentage: 4.14%\n",
      "Loss percentage: 64.52%\n",
      "Draw percentage: 5.94%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def naive_policy(player_sum):\n",
    "    return 0 if player_sum >= 20 else 1  # 0=stick, 1=hit\n",
    "\n",
    "\n",
    "n_episodes = 100000\n",
    "total_reward = 0\n",
    "wins = 0\n",
    "natural_wins = 0\n",
    "losses = 0\n",
    "draws = 0\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        player_sum = state[0]\n",
    "        action = naive_policy(player_sum)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if reward == 1.0:\n",
    "        wins += 1\n",
    "    elif reward == 1.5:\n",
    "        natural_wins += 1\n",
    "        wins += 1\n",
    "    elif reward == 0.0:\n",
    "        draws += 1\n",
    "    elif reward == -1.0:\n",
    "        losses += 1\n",
    "\n",
    "print(f\"Total accumulated reward: {total_reward}\")\n",
    "print(f\"Win percentage: {wins / n_episodes * 100:.2f}%\")\n",
    "print(f\"Natural win percentage: {natural_wins / n_episodes * 100:.2f}%\")\n",
    "print(f\"Loss percentage: {losses / n_episodes * 100:.2f}%\")\n",
    "print(f\"Draw percentage: {draws / n_episodes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf10db1",
   "metadata": {},
   "source": [
    "With this first basic policy implementation we have a loss percentage of almost 65%, with a win % of 29.43%, which also includes the % of natural wins (a 4.13%). As far as I know, the % of natural wins is the only one that souldn't change much in the future, as the probabilities of a natural blackjack are independent of the policy chosen. The results seem pretty consisistent with the policy we have implemented. Of course, the accumulated reward is just a reflection of this low win rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b30e3-dd3d-4fcd-a6e4-baaf764d8f88",
   "metadata": {},
   "source": [
    "## Part 2. Monte Carlo method\n",
    "\n",
    "The objective of this section is to estimate the optimal policy using Monte Carlo methods. Specifically, you can choose and implement one of the algorithms related to _Control using MC methods_ (with ''exploring starts'' or without ''exploring starts'', both on-policy or off-policy).\n",
    "\n",
    "<u>Questions</u> (**2.5 points**): \n",
    "1. Implement the selected algorithm and justify your choice.\n",
    "2. Comment and justify all the parameters, such as:\n",
    "- Number of episodes\n",
    "- Discount factor\n",
    "- Etc.\n",
    "3. Implement a function that prints on the screen the optimal policy found for each state (similar to the figure in Section 3.1).\n",
    "4. Using the trained agent, simulate 100,000 games and calculate the agent's return (total accumulated reward).\n",
    "5. Additionally, calculate the % of wins, natural wins, losses and draws."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa0aae",
   "metadata": {},
   "source": [
    "For this part i will implement a On-policy first-visit Monte Carlo Control with $\\epsilon$-soft policy, without exploring starts. The justification is that its a simple policy that can be useful for a game like blackjack, with a small state space where we can mantain a table of Q values. And $\\epsilon$-soft ensures all actions have a non-zero probability, so we ensure exploration.\n",
    "\n",
    "I will start with 100.000 episodes, just to check if its working and end with 500.000, the same used for the CliffWalking example, that should be enough. Discount Factor = 1 as in blackjack we care about the only reward at the end of each episode. For epsilon, i will start with a value of Epsilon = 0.1 just to ensure exploration. (maybe to be changed in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ae9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative version\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "n_episodes = 100000\n",
    "gamma = 1.0\n",
    "epsilon = 0.1\n",
    "\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "def epsilon_greedy_policy(state):\n",
    "    \"\"\"Return an action following ε-greedy policy derived from Q.\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "# Monte Carlo Control (On-Policy ε-soft)\n",
    "returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "returns_count = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    episode_memory = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Generate one episode\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_memory.append((state, action, reward))\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # Compute returns and update Q\n",
    "    G = 0\n",
    "    visited_state_actions = set()\n",
    "    for state, action, reward in reversed(episode_memory):\n",
    "        G = gamma * G + reward\n",
    "        if (state, action) not in visited_state_actions:\n",
    "            returns_sum[state][action] += G\n",
    "            returns_count[state][action] += 1\n",
    "            Q[state][action] = returns_sum[state][action] / returns_count[state][action]\n",
    "            visited_state_actions.add((state, action))\n",
    "\n",
    "# Derive the final deterministic (greedy) policy\n",
    "def optimal_policy(state):\n",
    "    return np.argmax(Q[state])\n",
    "\n",
    "print(\"MC Control training complete.\")\n",
    "\n",
    "#Display policy\n",
    "def print_policy(Q):\n",
    "    usable_ace = np.zeros((10, 10), dtype=str)\n",
    "    no_usable_ace = np.zeros((10, 10), dtype=str)\n",
    "\n",
    "    for player_sum in range(12, 22):\n",
    "        for dealer_card in range(1, 11):\n",
    "            for ace in [True, False]:\n",
    "                state = (player_sum, dealer_card, ace)\n",
    "                if state in Q:\n",
    "                    action = np.argmax(Q[state])\n",
    "                    symbol = \"S\" if action == 0 else \"H\"\n",
    "                    if ace:\n",
    "                        usable_ace[player_sum - 12, dealer_card - 1] = symbol\n",
    "                    else:\n",
    "                        no_usable_ace[player_sum - 12, dealer_card - 1] = symbol\n",
    "\n",
    "    print(\"Policy with usable ace (S=stick, H=hit):\")\n",
    "    print(np.flipud(usable_ace))\n",
    "    print(\"\\nPolicy without usable ace (S=stick, H=hit):\")\n",
    "    print(np.flipud(no_usable_ace))\n",
    "\n",
    "print_policy(Q)\n",
    "\n",
    "#Metrics\n",
    "episodes_eval = 100000\n",
    "total_reward = 0\n",
    "wins = 0\n",
    "natural_wins = 0\n",
    "losses = 0\n",
    "draws = 0\n",
    "\n",
    "for _ in range(episodes_eval):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        action = np.argmax(Q[state])\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "    if reward == 1.0:\n",
    "        wins += 1\n",
    "    elif reward == 1.5:\n",
    "        natural_wins += 1\n",
    "        wins += 1\n",
    "    elif reward == 0.0:\n",
    "        draws += 1\n",
    "    elif reward == -1.0:\n",
    "        losses += 1\n",
    "\n",
    "print(f\"Total accumulated reward: {total_reward}\")\n",
    "print(f\"Win percentage: {wins / episodes_eval * 100:.2f}%\")\n",
    "print(f\"Natural win percentage: {natural_wins / episodes_eval * 100:.2f}%\")\n",
    "print(f\"Loss percentage: {losses / episodes_eval * 100:.2f}%\")\n",
    "print(f\"Draw percentage: {draws / episodes_eval * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb6acf-25c2-4f01-b2da-ca7b16435266",
   "metadata": {},
   "source": [
    "## Part 3. TD learning\n",
    "\n",
    "The objective of this section is to estimate the optimal policy using TD learning methods. Specifically, you have to implement the **SARSA algorithm**.\n",
    "\n",
    "<u>Questions</u> (**2.5 points**): \n",
    "1. Implement the algorithm.\n",
    "2. Comment and justify all the parameters.\n",
    "3. Print on the screen the optimal policy found for each state.\n",
    "4. Using the trained agent, simulate 100,000 games and calculate the agent's return (total accumulated reward).\n",
    "5. Additionally, calculate the % of wins, natural wins, losses and draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa4fee-c8e8-4ffa-b8e9-5c75cde0aef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c77db3-3fd1-4454-8079-e2eb44942758",
   "metadata": {},
   "source": [
    "## Part 4. Comparison of the algorithms\n",
    "\n",
    "In this section, we will make a comparison among the algorithms.\n",
    "\n",
    "We will compare the performance of the algorithms when changing the number of episodes, the discount factor and the *learning rate* values (in the case of the SARSA method).\n",
    "\n",
    "For each exercise, the results must be presented and justified.\n",
    "\n",
    "**Note**: \n",
    "- It is recommended to run the simulations multiple times for each exercise, as these are random, and to comment on the most frequent result or the average of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1e4c9-29a8-4552-a463-e4e6cf2b7e91",
   "metadata": {},
   "source": [
    "### 4.1. Comparison to the optimal policy\n",
    "\n",
    "The optimal policy for this problem, described by [Sutton & Barto](http://incompleteideas.net/book/the-book-2nd.html) is depicted in the following image:\n",
    "\n",
    "<img src=\"./figs/optimal.png\" style=\"width: 800px;\" />\n",
    "\n",
    "<u>Questions</u> (**1 point**): \n",
    "- Compare the _optimal_ policies of the naïve, Monte Carlo and SARSA methods to the optimal one provided by Sutton & Barto.\n",
    "- Comment on the results and justify your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148630a-af42-4b8a-9754-088340fa65a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177e4a86-98bc-4feb-84b0-5cf85a27ca17",
   "metadata": {},
   "source": [
    "### 4.2. Influence of the Number of Episodes\n",
    "\n",
    "Conduct a study by varying the number of episodes in each of the algorithms.\n",
    "\n",
    "<u>Questions</u> (**1 point**): \n",
    "- Train each algorithm multiple times with 100,000, 1,000,000, and 5,000,000 episodes and average the results.\n",
    "- Indicate how the **number of episodes** influences the convergence of each algorithm by calculating the number of states where the policy differs from the optimal one, as well as the average return obtained after playing 100,000 games following each training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a296163-fdf1-4f83-837c-9c7e95946805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cbfc6b9-a71e-4665-b124-88bfd6390aef",
   "metadata": {},
   "source": [
    "### 4.3. Influence of the Discount Factor\n",
    "\n",
    "Conduct a study by varying the *discount factor* in each of the algorithms.\n",
    "\n",
    "<u>Questions</u> (**1 point**):\n",
    "- Run the algorithms with *discount factor* = 0.1, 0.5, 0.9 and the rest of the parameters the same as in previous exercises. \n",
    "- Describe the changes in the optimal policy, comparing the result obtained with the result of previous exercises (*discount factor* = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e716e-5234-4e16-8a74-febda9a0b0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9afe8b5-8a12-4c1d-a9ab-75da38fe5aee",
   "metadata": {},
   "source": [
    "### 4.4. Influence of the Learning Rate\n",
    "\n",
    "Conduct a study by varying the learning rate in the *SARSA* algorithm.\n",
    "\n",
    "<u>Questions</u> (**1 point**):\n",
    "- Run the *SARSA* algorithm with the following *learning rate* values: 0.001, 0.01, 0.1, and 0.9.\n",
    "- Analyze the differences with the results obtained previously in terms of the number of errors relative to the optimal policy and the accumulated reward for every 100,000 episodes played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408beff-0204-42a2-b50f-6c380f24c8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
