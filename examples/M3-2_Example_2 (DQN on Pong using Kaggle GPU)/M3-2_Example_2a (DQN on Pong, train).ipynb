{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Yb47zJQglm"
   },
   "source": [
    "# **Deep Reinforcement Learning**\n",
    "\n",
    "# M3-2 Deep Q-Networks\n",
    "\n",
    "## Example of DQN implementation on Pong environment\n",
    "\n",
    "Below we will see a simple example that will allow us to understand the concepts introduced in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maCHXYw1G8Km"
   },
   "source": [
    "## Pong environment\n",
    "\n",
    "The [Pong](https://gymnasium.farama.org/environments/atari/pong/) environment is part of the [Atari environments](https://gymnasium.farama.org/environments/atari/). Please read that page first for general information.\n",
    "\n",
    "You control the right paddle, you compete against the left paddle controlled by the computer. You each try to keep deflecting the ball away from your goal and into your opponent’s goal.\n",
    "\n",
    "<center><img src=\"https://ale.farama.org/_images/pong.gif\"/></center>\n",
    "\n",
    "For a more detailed documentation, see the [AtariAge page](https://atariage.com/manual_html_page.php?SoftwareLabelID=587)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will load the environment. It is important to note that in this case, we will load the \"preliminary\" version of the environment, which belongs to the [Gym](https://github.com/openai/gym) framework (instead of [Gymnasium](https://gymnasium.farama.org/index.html)).\n",
    "\n",
    "To install this environment, we need to execute the following command:\n",
    "> pip install gym==0.25.0\n",
    "\n",
    "And all related packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:06.966391Z",
     "iopub.status.busy": "2024-10-24T11:09:06.965908Z",
     "iopub.status.idle": "2024-10-24T11:09:30.137748Z",
     "shell.execute_reply": "2024-10-24T11:09:30.136587Z",
     "shell.execute_reply.started": "2024-10-24T11:09:06.966351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.25.0 in /opt/conda/lib/python3.10/site-packages (from gym[atari]==0.25.0) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.0->gym[atari]==0.25.0) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.0->gym[atari]==0.25.0) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.0->gym[atari]==0.25.0) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.7.5 in /opt/conda/lib/python3.10/site-packages (from gym[atari]==0.25.0) (0.7.5)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.7.5->gym[atari]==0.25.0) (6.4.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license] in /opt/conda/lib/python3.10/site-packages (0.6.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]) (8.1.7)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]) (2.32.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install gym[atari]==0.25.0\n",
    "!pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dependencies are installed, we load them and initialize the `PongNoFrameskip-v4` environment.\n",
    "\n",
    "There are several Pong environments, with minor differences among them. See [Pong](https://gymnasium.farama.org/environments/atari/pong/) page for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:30.140347Z",
     "iopub.status.busy": "2024-10-24T11:09:30.140007Z",
     "iopub.status.idle": "2024-10-24T11:09:30.331403Z",
     "shell.execute_reply": "2024-10-24T11:09:30.330395Z",
     "shell.execute_reply.started": "2024-10-24T11:09:30.140312Z"
    },
    "id": "FA1Y5VCv20XZ",
    "outputId": "571f131d-20ca-4243-88c2-fa1f18d08d11",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gym version 0.25.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# version\n",
    "print(\"Using Gym version {}\".format(gym.__version__))\n",
    "\n",
    "ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "test_env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the GPU model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:30.333043Z",
     "iopub.status.busy": "2024-10-24T11:09:30.332707Z",
     "iopub.status.idle": "2024-10-24T11:09:31.454520Z",
     "shell.execute_reply": "2024-10-24T11:09:31.453433Z",
     "shell.execute_reply.started": "2024-10-24T11:09:30.332997Z"
    },
    "id": "VjUM99rEKFNt",
    "outputId": "a8a265ff-6f3b-4419-8132-6efaa27616e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 24 11:09:31 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   74C    P0             29W /   70W |     271MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   39C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy3RBfwzHRQq"
   },
   "source": [
    "## Data preprocessing (Wrappers)\n",
    "\n",
    "The original observations, provided by the environment, are:\n",
    "- images of $210 \\times 160$ in RGB colour\n",
    "- Thus, we represent each observation using a numpy array of `(210, 160, 3) dtype=int8'\n",
    "\n",
    "However, we have some problems:\n",
    "1. The observations include **parts of the screen** that are not rellevant.\n",
    "2. Number of states is: $256^{(210 \\times 160 \\times 3)}$ = $256^{100800}$! So, **reducing the image** size could help!\n",
    "3. The images of the environment are in **color (RGB)**, but does color really provide any information?\n",
    "4. In a single image it is not possible to know the **dynamics of the game** (i.e. direction and speed of the ball). Therefore, we must consider a sequence of several consecutive images to understand what is happening in the game.\n",
    "\n",
    "We will use several wrappers to transform the observations. Specifically, we want to get (from the environment) observations with the following characteristics:\n",
    "- Grayscale images\n",
    "- Resolution $84 \\times 84$\n",
    "- Float images $\\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./figs/preprocessing-1.jpg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"./figs/preprocessing-2.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:31.458037Z",
     "iopub.status.busy": "2024-10-24T11:09:31.457662Z",
     "iopub.status.idle": "2024-10-24T11:09:31.488778Z",
     "shell.execute_reply": "2024-10-24T11:09:31.487816Z",
     "shell.execute_reply.started": "2024-10-24T11:09:31.457998Z"
    },
    "id": "nPi1lHINMuSu"
   },
   "outputs": [],
   "source": [
    "# OpenAI Gym Wrappers\n",
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "import gym.spaces\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    print(\"MaxAndSkipEnv        : {}\".format(env.observation_space.shape))\n",
    "    env = FireResetEnv(env)\n",
    "    print(\"FireResetEnv         : {}\".format(env.observation_space.shape))\n",
    "    env = ProcessFrame84(env)\n",
    "    print(\"ProcessFrame84       : {}\".format(env.observation_space.shape))\n",
    "    env = ImageToPyTorch(env)\n",
    "    print(\"ImageToPyTorch       : {}\".format(env.observation_space.shape))\n",
    "    env = BufferWrapper(env, 4)\n",
    "    print(\"BufferWrapper        : {}\".format(env.observation_space.shape))\n",
    "    env = ScaledFloatFrame(env)\n",
    "    print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "def print_env_info(name, env):\n",
    "    obs = env.reset()\n",
    "    print(\"*** {} Environment ***\".format(name))\n",
    "    print(\"Observation shape: {}, type: {} and range [{},{}]\".format(obs.shape, obs.dtype, np.min(obs), np.max(obs)))\n",
    "    print(\"Observation sample:\\n{}\".format(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_env` function applies all the wrappers to the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the observations from the **standard** and **wrapped** environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:31.491109Z",
     "iopub.status.busy": "2024-10-24T11:09:31.490785Z",
     "iopub.status.idle": "2024-10-24T11:09:31.700700Z",
     "shell.execute_reply": "2024-10-24T11:09:31.699783Z",
     "shell.execute_reply.started": "2024-10-24T11:09:31.491075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Standard Environment ***\n",
      "Observation shape: (210, 160, 3), type: uint8 and range [0,228]\n",
      "Observation sample:\n",
      "[[[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]]\n",
      "\n",
      " [[109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  ...\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]]\n",
      "\n",
      " [[109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  ...\n",
      "  [109 118  43]\n",
      "  [109 118  43]\n",
      "  [109 118  43]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  ...\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]]\n",
      "\n",
      " [[ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  ...\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]]\n",
      "\n",
      " [[ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  ...\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]\n",
      "  [ 53  95  24]]]\n"
     ]
    }
   ],
   "source": [
    "# standar Env\n",
    "env = gym.make(ENV_NAME)\n",
    "print_env_info(\"Standard\", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:31.702092Z",
     "iopub.status.busy": "2024-10-24T11:09:31.701793Z",
     "iopub.status.idle": "2024-10-24T11:09:31.907381Z",
     "shell.execute_reply": "2024-10-24T11:09:31.906498Z",
     "shell.execute_reply.started": "2024-10-24T11:09:31.702061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipEnv        : (210, 160, 3)\n",
      "FireResetEnv         : (210, 160, 3)\n",
      "ProcessFrame84       : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "BufferWrapper        : (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n",
      "*** Wrapped Environment ***\n",
      "Observation shape: (4, 84, 84), type: float32 and range [0.0,0.6352941393852234]\n",
      "Observation sample:\n",
      "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.34117648 0.34117648 0.34117648 ... 0.34117648 0.34117648 0.34117648]\n",
      "  [0.34117648 0.34117648 0.34117648 ... 0.34117648 0.34117648 0.34117648]\n",
      "  [0.34117648 0.34117648 0.34117648 ... 0.34117648 0.34117648 0.34117648]\n",
      "  ...\n",
      "  [0.34117648 0.34117648 0.34117648 ... 0.34117648 0.34117648 0.34117648]\n",
      "  [0.34117648 0.34117648 0.34117648 ... 0.34117648 0.34117648 0.34117648]\n",
      "  [0.56078434 0.56078434 0.56078434 ... 0.56078434 0.56078434 0.56078434]]]\n"
     ]
    }
   ],
   "source": [
    "# wrapped Env\n",
    "env = make_env(ENV_NAME)\n",
    "print_env_info(\"Wrapped\", env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN5Ps-fUHHeG"
   },
   "source": [
    "## Neural network architecture\n",
    "\n",
    "The following code will implement the NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:31.909014Z",
     "iopub.status.busy": "2024-10-24T11:09:31.908616Z",
     "iopub.status.idle": "2024-10-24T11:09:31.915322Z",
     "shell.execute_reply": "2024-10-24T11:09:31.914171Z",
     "shell.execute_reply.started": "2024-10-24T11:09:31.908971Z"
    },
    "id": "h6B8v-Qh5Ykk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn        \n",
    "import torch.optim as optim \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:31.916915Z",
     "iopub.status.busy": "2024-10-24T11:09:31.916614Z",
     "iopub.status.idle": "2024-10-24T11:09:32.128456Z",
     "shell.execute_reply": "2024-10-24T11:09:32.127739Z",
     "shell.execute_reply.started": "2024-10-24T11:09:31.916878Z"
    },
    "id": "IvZMDEFBXAMC",
    "outputId": "a1e94895-74cb-4450-b1b9-4e97d4ac47ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipEnv        : (210, 160, 3)\n",
      "FireResetEnv         : (210, 160, 3)\n",
      "ProcessFrame84       : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "BufferWrapper        : (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "def make_DQN(input_shape, output_shape):\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64*7*7, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, output_shape)\n",
    "    )\n",
    "    return net\n",
    "\n",
    "test_env = make_env(ENV_NAME)\n",
    "test_net = make_DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_nYIY1AHeqq"
   },
   "source": [
    "## Experience Replay and Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFaMmDKqYmo4"
   },
   "source": [
    "First, we design a class to implement the **experience replay buffer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:32.130591Z",
     "iopub.status.busy": "2024-10-24T11:09:32.129752Z",
     "iopub.status.idle": "2024-10-24T11:09:32.138533Z",
     "shell.execute_reply": "2024-10-24T11:09:32.137575Z",
     "shell.execute_reply.started": "2024-10-24T11:09:32.130544Z"
    },
    "id": "Y79CNYsjY4w0"
   },
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, BATCH_SIZE):\n",
    "        indices = np.random.choice(len(self.buffer), BATCH_SIZE, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARQJsNdbHwgX"
   },
   "source": [
    "## Deep Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhv3Yf-aW7UW"
   },
   "source": [
    "Define the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:32.142628Z",
     "iopub.status.busy": "2024-10-24T11:09:32.142120Z",
     "iopub.status.idle": "2024-10-24T11:09:32.153489Z",
     "shell.execute_reply": "2024-10-24T11:09:32.152679Z",
     "shell.execute_reply.started": "2024-10-24T11:09:32.142581Z"
    },
    "id": "AGwHC9dyXoPd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "MEAN_REWARD_BOUND = 19.0 \n",
    "NUMBER_OF_REWARDS_TO_AVERAGE = 10          \n",
    "\n",
    "GAMMA = 0.99       \n",
    "\n",
    "BATCH_SIZE = 32  \n",
    "LEARNING_RATE = 1e-4           \n",
    "\n",
    "EXPERIENCE_REPLAY_SIZE = 10000            \n",
    "SYNC_TARGET_NETWORK = 1000     \n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 0.999985\n",
    "EPS_MIN = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQDV04ktY3xs"
   },
   "source": [
    "Define the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:32.154767Z",
     "iopub.status.busy": "2024-10-24T11:09:32.154499Z",
     "iopub.status.idle": "2024-10-24T11:09:32.165380Z",
     "shell.execute_reply": "2024-10-24T11:09:32.164583Z",
     "shell.execute_reply.started": "2024-10-24T11:09:32.154738Z"
    },
    "id": "YdAKFiMWZw90"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_replay_buffer):\n",
    "        self.env = env\n",
    "        self.exp_replay_buffer = exp_replay_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.current_state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_ = np.array([self.current_state])\n",
    "            state = torch.tensor(state_).to(device)\n",
    "            q_vals = net(state)\n",
    "            _, act_ = torch.max(q_vals, dim=1)\n",
    "            action = int(act_.item())\n",
    "\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.current_state, action, reward, is_done, new_state)\n",
    "        self.exp_replay_buffer.append(exp)\n",
    "        self.current_state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        \n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMQVyeKhCCCM"
   },
   "source": [
    "## Training\n",
    "\n",
    "Train a DQN model on the Pong environment using the parameters and architecture we have previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:32.166813Z",
     "iopub.status.busy": "2024-10-24T11:09:32.166499Z",
     "iopub.status.idle": "2024-10-24T11:09:33.484183Z",
     "shell.execute_reply": "2024-10-24T11:09:33.483244Z",
     "shell.execute_reply.started": "2024-10-24T11:09:32.166782Z"
    },
    "id": "BCBQhXLfNeUG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcasasr\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241024_110932-09rfa0pg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcasasr/M3-2_Example_2a/runs/09rfa0pg' target=\"_blank\">wandering-armadillo-3</a></strong> to <a href='https://wandb.ai/jcasasr/M3-2_Example_2a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcasasr/M3-2_Example_2a' target=\"_blank\">https://wandb.ai/jcasasr/M3-2_Example_2a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcasasr/M3-2_Example_2a/runs/09rfa0pg' target=\"_blank\">https://wandb.ai/jcasasr/M3-2_Example_2a/runs/09rfa0pg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcasasr/M3-2_Example_2a/runs/09rfa0pg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ba085fe99f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(project=\"M3-2_Example_2a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:33.485586Z",
     "iopub.status.busy": "2024-10-24T11:09:33.485303Z",
     "iopub.status.idle": "2024-10-24T11:09:33.491741Z",
     "shell.execute_reply": "2024-10-24T11:09:33.490701Z",
     "shell.execute_reply.started": "2024-10-24T11:09:33.485554Z"
    },
    "id": "ipurwYpa6iKn",
    "outputId": "e79482cd-312a-47e0-eb34-72836d033356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training starts at  2024-10-24 11:09:33.486951\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\">>> Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgpmAtchZwM_"
   },
   "source": [
    "Main bucle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T11:09:33.493261Z",
     "iopub.status.busy": "2024-10-24T11:09:33.492967Z",
     "iopub.status.idle": "2024-10-24T12:15:13.855147Z",
     "shell.execute_reply": "2024-10-24T12:15:13.853855Z",
     "shell.execute_reply.started": "2024-10-24T11:09:33.493230Z"
    },
    "id": "qEoc2PWmM2mu",
    "outputId": "0e5f5940-9f64-4566-a8c8-85fa509c6080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipEnv        : (210, 160, 3)\n",
      "FireResetEnv         : (210, 160, 3)\n",
      "ProcessFrame84       : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "BufferWrapper        : (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n",
      "Frame:847 | Total games:1 | Mean reward: -21.000  (epsilon used: 0.99)\n",
      "Frame:1690 | Total games:2 | Mean reward: -21.000  (epsilon used: 0.97)\n",
      "Frame:2453 | Total games:3 | Mean reward: -21.000  (epsilon used: 0.96)\n",
      "Frame:3216 | Total games:4 | Mean reward: -21.000  (epsilon used: 0.95)\n",
      "Frame:4025 | Total games:5 | Mean reward: -21.000  (epsilon used: 0.94)\n",
      "Frame:4816 | Total games:6 | Mean reward: -21.000  (epsilon used: 0.93)\n",
      "Frame:5699 | Total games:7 | Mean reward: -21.000  (epsilon used: 0.92)\n",
      "Frame:6462 | Total games:8 | Mean reward: -21.000  (epsilon used: 0.91)\n",
      "Frame:7285 | Total games:9 | Mean reward: -21.000  (epsilon used: 0.90)\n",
      "Frame:8076 | Total games:10 | Mean reward: -21.000  (epsilon used: 0.89)\n",
      "Frame:8917 | Total games:11 | Mean reward: -21.000  (epsilon used: 0.87)\n",
      "Frame:9680 | Total games:12 | Mean reward: -21.000  (epsilon used: 0.86)\n",
      "Frame:10443 | Total games:13 | Mean reward: -21.000  (epsilon used: 0.86)\n",
      "Frame:11400 | Total games:14 | Mean reward: -21.000  (epsilon used: 0.84)\n",
      "Frame:12418 | Total games:15 | Mean reward: -20.900  (epsilon used: 0.83)\n",
      "Frame:13209 | Total games:16 | Mean reward: -20.900  (epsilon used: 0.82)\n",
      "Frame:14438 | Total games:17 | Mean reward: -20.500  (epsilon used: 0.81)\n",
      "Frame:15652 | Total games:18 | Mean reward: -20.100  (epsilon used: 0.79)\n",
      "Frame:16443 | Total games:19 | Mean reward: -20.100  (epsilon used: 0.78)\n",
      "Frame:17629 | Total games:20 | Mean reward: -20.000  (epsilon used: 0.77)\n",
      "Frame:18640 | Total games:21 | Mean reward: -20.000  (epsilon used: 0.76)\n",
      "Frame:19504 | Total games:22 | Mean reward: -19.900  (epsilon used: 0.75)\n",
      "Frame:20451 | Total games:23 | Mean reward: -19.800  (epsilon used: 0.74)\n",
      "Frame:21292 | Total games:24 | Mean reward: -19.700  (epsilon used: 0.73)\n",
      "Frame:22468 | Total games:25 | Mean reward: -19.600  (epsilon used: 0.71)\n",
      "Frame:23291 | Total games:26 | Mean reward: -19.600  (epsilon used: 0.71)\n",
      "Frame:24299 | Total games:27 | Mean reward: -19.900  (epsilon used: 0.69)\n",
      "Frame:25230 | Total games:28 | Mean reward: -20.300  (epsilon used: 0.68)\n",
      "Frame:26211 | Total games:29 | Mean reward: -20.200  (epsilon used: 0.67)\n",
      "Frame:27004 | Total games:30 | Mean reward: -20.300  (epsilon used: 0.67)\n",
      "Frame:27767 | Total games:31 | Mean reward: -20.300  (epsilon used: 0.66)\n",
      "Frame:28885 | Total games:32 | Mean reward: -20.200  (epsilon used: 0.65)\n",
      "Frame:29858 | Total games:33 | Mean reward: -20.300  (epsilon used: 0.64)\n",
      "Frame:30909 | Total games:34 | Mean reward: -20.400  (epsilon used: 0.63)\n",
      "Frame:32110 | Total games:35 | Mean reward: -20.600  (epsilon used: 0.62)\n",
      "Frame:33056 | Total games:36 | Mean reward: -20.600  (epsilon used: 0.61)\n",
      "Frame:33995 | Total games:37 | Mean reward: -20.700  (epsilon used: 0.60)\n",
      "Frame:35060 | Total games:38 | Mean reward: -20.600  (epsilon used: 0.59)\n",
      "Frame:36246 | Total games:39 | Mean reward: -20.600  (epsilon used: 0.58)\n",
      "Frame:37357 | Total games:40 | Mean reward: -20.600  (epsilon used: 0.57)\n",
      "Frame:38209 | Total games:41 | Mean reward: -20.600  (epsilon used: 0.56)\n",
      "Frame:39569 | Total games:42 | Mean reward: -20.600  (epsilon used: 0.55)\n",
      "Frame:40945 | Total games:43 | Mean reward: -20.300  (epsilon used: 0.54)\n",
      "Frame:42121 | Total games:44 | Mean reward: -20.200  (epsilon used: 0.53)\n",
      "Frame:43608 | Total games:45 | Mean reward: -19.700  (epsilon used: 0.52)\n",
      "Frame:44863 | Total games:46 | Mean reward: -19.400  (epsilon used: 0.51)\n",
      "Frame:46505 | Total games:47 | Mean reward: -19.000  (epsilon used: 0.50)\n",
      "Frame:47943 | Total games:48 | Mean reward: -18.800  (epsilon used: 0.49)\n",
      "Frame:48900 | Total games:49 | Mean reward: -18.800  (epsilon used: 0.48)\n",
      "Frame:50415 | Total games:50 | Mean reward: -18.200  (epsilon used: 0.47)\n",
      "Frame:51718 | Total games:51 | Mean reward: -18.000  (epsilon used: 0.46)\n",
      "Frame:53008 | Total games:52 | Mean reward: -17.900  (epsilon used: 0.45)\n",
      "Frame:54157 | Total games:53 | Mean reward: -18.000  (epsilon used: 0.44)\n",
      "Frame:56026 | Total games:54 | Mean reward: -17.900  (epsilon used: 0.43)\n",
      "Frame:57297 | Total games:55 | Mean reward: -18.400  (epsilon used: 0.42)\n",
      "Frame:59124 | Total games:56 | Mean reward: -18.600  (epsilon used: 0.41)\n",
      "Frame:60366 | Total games:57 | Mean reward: -18.800  (epsilon used: 0.40)\n",
      "Frame:61841 | Total games:58 | Mean reward: -18.900  (epsilon used: 0.40)\n",
      "Frame:63133 | Total games:59 | Mean reward: -19.000  (epsilon used: 0.39)\n",
      "Frame:64871 | Total games:60 | Mean reward: -19.300  (epsilon used: 0.38)\n",
      "Frame:66868 | Total games:61 | Mean reward: -19.000  (epsilon used: 0.37)\n",
      "Frame:68333 | Total games:62 | Mean reward: -19.000  (epsilon used: 0.36)\n",
      "Frame:70066 | Total games:63 | Mean reward: -19.000  (epsilon used: 0.35)\n",
      "Frame:71785 | Total games:64 | Mean reward: -18.900  (epsilon used: 0.34)\n",
      "Frame:73700 | Total games:65 | Mean reward: -18.400  (epsilon used: 0.33)\n",
      "Frame:76505 | Total games:66 | Mean reward: -17.200  (epsilon used: 0.32)\n",
      "Frame:77971 | Total games:67 | Mean reward: -17.200  (epsilon used: 0.31)\n",
      "Frame:79355 | Total games:68 | Mean reward: -17.300  (epsilon used: 0.30)\n",
      "Frame:80873 | Total games:69 | Mean reward: -17.100  (epsilon used: 0.30)\n",
      "Frame:82780 | Total games:70 | Mean reward: -17.000  (epsilon used: 0.29)\n",
      "Frame:84566 | Total games:71 | Mean reward: -17.300  (epsilon used: 0.28)\n",
      "Frame:86137 | Total games:72 | Mean reward: -17.200  (epsilon used: 0.27)\n",
      "Frame:87489 | Total games:73 | Mean reward: -17.400  (epsilon used: 0.27)\n",
      "Frame:89322 | Total games:74 | Mean reward: -17.300  (epsilon used: 0.26)\n",
      "Frame:91061 | Total games:75 | Mean reward: -17.500  (epsilon used: 0.26)\n",
      "Frame:93338 | Total games:76 | Mean reward: -18.300  (epsilon used: 0.25)\n",
      "Frame:94848 | Total games:77 | Mean reward: -18.300  (epsilon used: 0.24)\n",
      "Frame:96475 | Total games:78 | Mean reward: -18.100  (epsilon used: 0.24)\n",
      "Frame:98096 | Total games:79 | Mean reward: -18.100  (epsilon used: 0.23)\n",
      "Frame:99921 | Total games:80 | Mean reward: -18.000  (epsilon used: 0.22)\n",
      "Frame:101686 | Total games:81 | Mean reward: -17.900  (epsilon used: 0.22)\n",
      "Frame:103846 | Total games:82 | Mean reward: -17.900  (epsilon used: 0.21)\n",
      "Frame:105368 | Total games:83 | Mean reward: -17.600  (epsilon used: 0.21)\n",
      "Frame:107276 | Total games:84 | Mean reward: -17.500  (epsilon used: 0.20)\n",
      "Frame:109071 | Total games:85 | Mean reward: -17.300  (epsilon used: 0.19)\n",
      "Frame:110805 | Total games:86 | Mean reward: -17.400  (epsilon used: 0.19)\n",
      "Frame:112693 | Total games:87 | Mean reward: -17.200  (epsilon used: 0.18)\n",
      "Frame:114468 | Total games:88 | Mean reward: -17.000  (epsilon used: 0.18)\n",
      "Frame:116129 | Total games:89 | Mean reward: -17.100  (epsilon used: 0.18)\n",
      "Frame:117780 | Total games:90 | Mean reward: -17.300  (epsilon used: 0.17)\n",
      "Frame:119206 | Total games:91 | Mean reward: -17.500  (epsilon used: 0.17)\n",
      "Frame:120812 | Total games:92 | Mean reward: -17.600  (epsilon used: 0.16)\n",
      "Frame:122562 | Total games:93 | Mean reward: -17.500  (epsilon used: 0.16)\n",
      "Frame:124290 | Total games:94 | Mean reward: -17.600  (epsilon used: 0.15)\n",
      "Frame:126379 | Total games:95 | Mean reward: -17.400  (epsilon used: 0.15)\n",
      "Frame:128602 | Total games:96 | Mean reward: -17.100  (epsilon used: 0.15)\n",
      "Frame:130794 | Total games:97 | Mean reward: -16.600  (epsilon used: 0.14)\n",
      "Frame:133179 | Total games:98 | Mean reward: -16.600  (epsilon used: 0.14)\n",
      "Frame:135466 | Total games:99 | Mean reward: -15.800  (epsilon used: 0.13)\n",
      "Frame:137106 | Total games:100 | Mean reward: -16.100  (epsilon used: 0.13)\n",
      "Frame:138719 | Total games:101 | Mean reward: -16.100  (epsilon used: 0.12)\n",
      "Frame:140220 | Total games:102 | Mean reward: -16.100  (epsilon used: 0.12)\n",
      "Frame:141926 | Total games:103 | Mean reward: -16.100  (epsilon used: 0.12)\n",
      "Frame:143480 | Total games:104 | Mean reward: -16.200  (epsilon used: 0.12)\n",
      "Frame:145656 | Total games:105 | Mean reward: -16.100  (epsilon used: 0.11)\n",
      "Frame:147558 | Total games:106 | Mean reward: -16.400  (epsilon used: 0.11)\n",
      "Frame:149750 | Total games:107 | Mean reward: -16.700  (epsilon used: 0.11)\n",
      "Frame:151809 | Total games:108 | Mean reward: -16.500  (epsilon used: 0.10)\n",
      "Frame:154207 | Total games:109 | Mean reward: -16.600  (epsilon used: 0.10)\n",
      "Frame:155925 | Total games:110 | Mean reward: -16.200  (epsilon used: 0.10)\n",
      "Frame:157762 | Total games:111 | Mean reward: -16.200  (epsilon used: 0.09)\n",
      "Frame:159971 | Total games:112 | Mean reward: -15.700  (epsilon used: 0.09)\n",
      "Frame:162268 | Total games:113 | Mean reward: -15.700  (epsilon used: 0.09)\n",
      "Frame:165268 | Total games:114 | Mean reward: -15.400  (epsilon used: 0.08)\n",
      "Frame:167793 | Total games:115 | Mean reward: -15.100  (epsilon used: 0.08)\n",
      "Frame:169997 | Total games:116 | Mean reward: -15.100  (epsilon used: 0.08)\n",
      "Frame:172454 | Total games:117 | Mean reward: -14.900  (epsilon used: 0.08)\n",
      "Frame:175233 | Total games:118 | Mean reward: -14.800  (epsilon used: 0.07)\n",
      "Frame:177686 | Total games:119 | Mean reward: -14.500  (epsilon used: 0.07)\n",
      "Frame:179662 | Total games:120 | Mean reward: -14.700  (epsilon used: 0.07)\n",
      "Frame:182209 | Total games:121 | Mean reward: -13.900  (epsilon used: 0.07)\n",
      "Frame:184752 | Total games:122 | Mean reward: -13.600  (epsilon used: 0.06)\n",
      "Frame:187691 | Total games:123 | Mean reward: -12.900  (epsilon used: 0.06)\n",
      "Frame:189994 | Total games:124 | Mean reward: -13.000  (epsilon used: 0.06)\n",
      "Frame:192291 | Total games:125 | Mean reward: -13.700  (epsilon used: 0.06)\n",
      "Frame:195372 | Total games:126 | Mean reward: -12.700  (epsilon used: 0.05)\n",
      "Frame:197378 | Total games:127 | Mean reward: -12.900  (epsilon used: 0.05)\n",
      "Frame:200564 | Total games:128 | Mean reward: -11.900  (epsilon used: 0.05)\n",
      "Frame:203881 | Total games:129 | Mean reward: -11.600  (epsilon used: 0.05)\n",
      "Frame:206860 | Total games:130 | Mean reward: -10.100  (epsilon used: 0.04)\n",
      "Frame:209819 | Total games:131 | Mean reward: -9.200  (epsilon used: 0.04)\n",
      "Frame:213412 | Total games:132 | Mean reward: -8.500  (epsilon used: 0.04)\n",
      "Frame:216622 | Total games:133 | Mean reward: -6.800  (epsilon used: 0.04)\n",
      "Frame:218808 | Total games:134 | Mean reward: -4.300  (epsilon used: 0.04)\n",
      "Frame:221316 | Total games:135 | Mean reward: -3.300  (epsilon used: 0.04)\n",
      "Frame:223053 | Total games:136 | Mean reward: -4.100  (epsilon used: 0.04)\n",
      "Frame:225543 | Total games:137 | Mean reward: -1.800  (epsilon used: 0.03)\n",
      "Frame:227243 | Total games:138 | Mean reward: 0.600  (epsilon used: 0.03)\n",
      "Frame:230123 | Total games:139 | Mean reward: 0.900  (epsilon used: 0.03)\n",
      "Frame:232095 | Total games:140 | Mean reward: 2.900  (epsilon used: 0.03)\n",
      "Frame:234592 | Total games:141 | Mean reward: 4.000  (epsilon used: 0.03)\n",
      "Frame:236816 | Total games:142 | Mean reward: 5.700  (epsilon used: 0.03)\n",
      "Frame:240057 | Total games:143 | Mean reward: 5.200  (epsilon used: 0.03)\n",
      "Frame:242158 | Total games:144 | Mean reward: 6.000  (epsilon used: 0.03)\n",
      "Frame:244762 | Total games:145 | Mean reward: 7.300  (epsilon used: 0.03)\n",
      "Frame:246515 | Total games:146 | Mean reward: 10.800  (epsilon used: 0.02)\n",
      "Frame:248266 | Total games:147 | Mean reward: 11.900  (epsilon used: 0.02)\n",
      "Frame:250823 | Total games:148 | Mean reward: 11.100  (epsilon used: 0.02)\n",
      "Frame:252846 | Total games:149 | Mean reward: 13.100  (epsilon used: 0.02)\n",
      "Frame:254572 | Total games:150 | Mean reward: 13.400  (epsilon used: 0.02)\n",
      "Frame:256442 | Total games:151 | Mean reward: 14.300  (epsilon used: 0.02)\n",
      "Frame:258402 | Total games:152 | Mean reward: 14.500  (epsilon used: 0.02)\n",
      "Frame:260320 | Total games:153 | Mean reward: 16.300  (epsilon used: 0.02)\n",
      "Frame:262203 | Total games:154 | Mean reward: 16.400  (epsilon used: 0.02)\n",
      "Frame:264273 | Total games:155 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:266318 | Total games:156 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:268318 | Total games:157 | Mean reward: 16.700  (epsilon used: 0.02)\n",
      "Frame:271037 | Total games:158 | Mean reward: 16.100  (epsilon used: 0.02)\n",
      "Frame:273432 | Total games:159 | Mean reward: 15.600  (epsilon used: 0.02)\n",
      "Frame:275194 | Total games:160 | Mean reward: 15.500  (epsilon used: 0.02)\n",
      "Frame:277177 | Total games:161 | Mean reward: 15.600  (epsilon used: 0.02)\n",
      "Frame:278940 | Total games:162 | Mean reward: 15.900  (epsilon used: 0.02)\n",
      "Frame:281240 | Total games:163 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:283066 | Total games:164 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:285840 | Total games:165 | Mean reward: 14.600  (epsilon used: 0.02)\n",
      "Frame:288169 | Total games:166 | Mean reward: 14.300  (epsilon used: 0.02)\n",
      "Frame:289960 | Total games:167 | Mean reward: 14.600  (epsilon used: 0.02)\n",
      "Frame:292560 | Total games:168 | Mean reward: 14.700  (epsilon used: 0.02)\n",
      "Frame:294829 | Total games:169 | Mean reward: 15.000  (epsilon used: 0.02)\n",
      "Frame:296552 | Total games:170 | Mean reward: 15.200  (epsilon used: 0.02)\n",
      "Frame:298529 | Total games:171 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:300314 | Total games:172 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:301953 | Total games:173 | Mean reward: 16.000  (epsilon used: 0.02)\n",
      "Frame:303591 | Total games:174 | Mean reward: 16.300  (epsilon used: 0.02)\n",
      "Frame:305452 | Total games:175 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:307687 | Total games:176 | Mean reward: 17.000  (epsilon used: 0.02)\n",
      "Frame:310077 | Total games:177 | Mean reward: 16.400  (epsilon used: 0.02)\n",
      "Frame:312956 | Total games:178 | Mean reward: 16.400  (epsilon used: 0.02)\n",
      "Frame:315017 | Total games:179 | Mean reward: 16.400  (epsilon used: 0.02)\n",
      "Frame:316844 | Total games:180 | Mean reward: 16.200  (epsilon used: 0.02)\n",
      "Frame:318705 | Total games:181 | Mean reward: 16.100  (epsilon used: 0.02)\n",
      "Frame:320471 | Total games:182 | Mean reward: 16.100  (epsilon used: 0.02)\n",
      "Frame:322111 | Total games:183 | Mean reward: 16.100  (epsilon used: 0.02)\n",
      "Frame:324036 | Total games:184 | Mean reward: 15.700  (epsilon used: 0.02)\n",
      "Frame:325896 | Total games:185 | Mean reward: 15.700  (epsilon used: 0.02)\n",
      "Frame:327999 | Total games:186 | Mean reward: 16.500  (epsilon used: 0.02)\n",
      "Frame:329906 | Total games:187 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:332087 | Total games:188 | Mean reward: 17.500  (epsilon used: 0.02)\n",
      "Frame:333822 | Total games:189 | Mean reward: 18.100  (epsilon used: 0.02)\n",
      "Frame:336041 | Total games:190 | Mean reward: 17.500  (epsilon used: 0.02)\n",
      "Frame:338185 | Total games:191 | Mean reward: 17.200  (epsilon used: 0.02)\n",
      "Frame:339821 | Total games:192 | Mean reward: 17.400  (epsilon used: 0.02)\n",
      "Frame:341895 | Total games:193 | Mean reward: 17.100  (epsilon used: 0.02)\n",
      "Frame:343535 | Total games:194 | Mean reward: 17.500  (epsilon used: 0.02)\n",
      "Frame:345340 | Total games:195 | Mean reward: 17.600  (epsilon used: 0.02)\n",
      "Frame:347216 | Total games:196 | Mean reward: 17.700  (epsilon used: 0.02)\n",
      "Frame:348951 | Total games:197 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:350740 | Total games:198 | Mean reward: 18.300  (epsilon used: 0.02)\n",
      "Frame:352991 | Total games:199 | Mean reward: 17.700  (epsilon used: 0.02)\n",
      "Frame:354741 | Total games:200 | Mean reward: 18.500  (epsilon used: 0.02)\n",
      "Frame:356631 | Total games:201 | Mean reward: 18.900  (epsilon used: 0.02)\n",
      "Frame:358536 | Total games:202 | Mean reward: 18.600  (epsilon used: 0.02)\n",
      "Frame:360707 | Total games:203 | Mean reward: 18.200  (epsilon used: 0.02)\n",
      "Frame:362751 | Total games:204 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:365358 | Total games:205 | Mean reward: 15.900  (epsilon used: 0.02)\n",
      "Frame:367543 | Total games:206 | Mean reward: 15.600  (epsilon used: 0.02)\n",
      "Frame:369337 | Total games:207 | Mean reward: 15.500  (epsilon used: 0.02)\n",
      "Frame:371472 | Total games:208 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:374256 | Total games:209 | Mean reward: 15.000  (epsilon used: 0.02)\n",
      "Frame:375895 | Total games:210 | Mean reward: 15.100  (epsilon used: 0.02)\n",
      "Frame:377535 | Total games:211 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:379871 | Total games:212 | Mean reward: 15.200  (epsilon used: 0.02)\n",
      "Frame:381672 | Total games:213 | Mean reward: 15.600  (epsilon used: 0.02)\n",
      "Frame:383602 | Total games:214 | Mean reward: 15.500  (epsilon used: 0.02)\n",
      "Frame:385334 | Total games:215 | Mean reward: 17.600  (epsilon used: 0.02)\n",
      "Frame:387152 | Total games:216 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:388992 | Total games:217 | Mean reward: 17.800  (epsilon used: 0.02)\n",
      "Frame:391064 | Total games:218 | Mean reward: 17.800  (epsilon used: 0.02)\n",
      "Frame:393406 | Total games:219 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:395201 | Total games:220 | Mean reward: 17.700  (epsilon used: 0.02)\n",
      "Frame:397069 | Total games:221 | Mean reward: 17.400  (epsilon used: 0.02)\n",
      "Frame:398709 | Total games:222 | Mean reward: 17.800  (epsilon used: 0.02)\n",
      "Frame:400349 | Total games:223 | Mean reward: 18.100  (epsilon used: 0.02)\n",
      "Frame:402181 | Total games:224 | Mean reward: 18.200  (epsilon used: 0.02)\n",
      "Frame:404136 | Total games:225 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:405840 | Total games:226 | Mean reward: 18.000  (epsilon used: 0.02)\n",
      "Frame:407577 | Total games:227 | Mean reward: 18.200  (epsilon used: 0.02)\n",
      "Frame:409217 | Total games:228 | Mean reward: 18.700  (epsilon used: 0.02)\n",
      "Frame:410995 | Total games:229 | Mean reward: 19.400  (epsilon used: 0.02)\n",
      "SOLVED in 410995 frames and 229 games\n"
     ]
    }
   ],
   "source": [
    "# create Env\n",
    "env = make_env(ENV_NAME)\n",
    "\n",
    "# create Agent\n",
    "net = make_DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = make_DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    " \n",
    "buffer = ExperienceReplay(EXPERIENCE_REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = EPS_START\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_number = 0  \n",
    "\n",
    "while True:\n",
    "    frame_number += 1\n",
    "    epsilon = max(epsilon*EPS_DECAY, EPS_MIN)\n",
    "\n",
    "    reward = agent.step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "\n",
    "        mean_reward = np.mean(total_rewards[-NUMBER_OF_REWARDS_TO_AVERAGE:])\n",
    "        print(f\"Frame:{frame_number} | Total games:{len(total_rewards)} | Mean reward: {mean_reward:.3f}  (epsilon used: {epsilon:.2f})\")\n",
    "        wandb.log({\"epsilon\": epsilon, \"reward_100\": mean_reward, \"reward\": reward}, step=frame_number)\n",
    "\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(f\"SOLVED in {frame_number} frames and {len(total_rewards)} games\")\n",
    "            break\n",
    "\n",
    "    if len(buffer) < EXPERIENCE_REPLAY_SIZE:\n",
    "        continue\n",
    "\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    states_, actions_, rewards_, dones_, next_states_ = batch\n",
    "\n",
    "    states = torch.tensor(states_).to(device)\n",
    "    next_states = torch.tensor(next_states_).to(device)\n",
    "    actions = torch.tensor(actions_).to(device)\n",
    "    rewards = torch.tensor(rewards_).to(device)\n",
    "    dones = torch.BoolTensor(dones_).to(device)\n",
    "\n",
    "    Q_values = net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    next_state_values = target_net(next_states).max(1)[0]\n",
    "    next_state_values[dones] = 0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_Q_values = next_state_values * GAMMA + rewards\n",
    "    loss = nn.MSELoss()(Q_values, expected_Q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if frame_number % SYNC_TARGET_NETWORK == 0:\n",
    "        target_net.load_state_dict(net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T12:15:13.857087Z",
     "iopub.status.busy": "2024-10-24T12:15:13.856696Z",
     "iopub.status.idle": "2024-10-24T12:15:13.883336Z",
     "shell.execute_reply": "2024-10-24T12:15:13.882502Z",
     "shell.execute_reply.started": "2024-10-24T12:15:13.857046Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), ENV_NAME + \".dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T12:15:13.884906Z",
     "iopub.status.busy": "2024-10-24T12:15:13.884567Z",
     "iopub.status.idle": "2024-10-24T12:15:13.891597Z",
     "shell.execute_reply": "2024-10-24T12:15:13.890584Z",
     "shell.execute_reply.started": "2024-10-24T12:15:13.884868Z"
    },
    "id": "MZPkszw66cmO",
    "outputId": "8fe2e7e1-0610-4ce7-ae35-e694b9bc0f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training ends at  2024-10-24 12:15:13.886531\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T12:15:13.894320Z",
     "iopub.status.busy": "2024-10-24T12:15:13.893121Z",
     "iopub.status.idle": "2024-10-24T12:15:15.400990Z",
     "shell.execute_reply": "2024-10-24T12:15:15.400168Z",
     "shell.execute_reply.started": "2024-10-24T12:15:13.894270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.032 MB of 0.032 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>██▇▆▆▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▂▃▁▁▂▂▂▄█▇█▆█▇▇▇▇█▇▇█</td></tr><tr><td>reward_100</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▄▄▆▆▆▇▇██▇▇▇▇███████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.02</td></tr><tr><td>reward</td><td>19</td></tr><tr><td>reward_100</td><td>19.4</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-armadillo-3</strong> at: <a href='https://wandb.ai/jcasasr/M3-2_Example_2a/runs/09rfa0pg' target=\"_blank\">https://wandb.ai/jcasasr/M3-2_Example_2a/runs/09rfa0pg</a><br/> View project at: <a href='https://wandb.ai/jcasasr/M3-2_Example_2a' target=\"_blank\">https://wandb.ai/jcasasr/M3-2_Example_2a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241024_110932-09rfa0pg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Capitulo09.ipynb",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
