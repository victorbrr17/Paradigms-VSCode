{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d726ad-50f7-419b-8df5-c59440a47f93",
   "metadata": {},
   "source": [
    "# **Deep Reinforcement Learning**\n",
    "\n",
    "# M3-1 Introduction to Approximate Solutions\n",
    "\n",
    "## Simple NN-based Agent \n",
    "\n",
    "Below we will see a simple example that will allow us to understand the concepts introduced in this module. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc77958-c55b-49ee-8807-d5d4a1066176",
   "metadata": {},
   "source": [
    "## 1. CartPole Environment\n",
    "\n",
    "In this exercise we are going to load the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment and perform some tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ada49-f1b7-4f67-98a2-fa58c8e90dfc",
   "metadata": {},
   "source": [
    "The following code loads the necessary packages for the example, creates the environment using the `make` method and prints on the screen the dimension of the:\n",
    "- **action space** (two actions: 0 = left and 1 = right), \n",
    "- **observations space** (four observations : cart position, cart speed, pole angle, and pole speed at the tip) \n",
    "- range of the **reward** variable (from minus infinity to plus infinity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dbba1de-3861-4c46-83c3-55c19a750fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gymnasium version is 1.2.1 \n",
      "Action space is Discrete(2) \n",
      "Observation space is Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32) \n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(\"Gymnasium version is {} \".format(gym.__version__))\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87472b6-d9cd-478b-8c4c-cd22862e4c37",
   "metadata": {},
   "source": [
    "## 2. Defining a simple NN-based Agent\n",
    "\n",
    "\n",
    "<u>Notes</u>:\n",
    "- This code is based on [Deep-Reinforcement-Learning-Hands-On-Second-Edition, published by Packt](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33a03a74-3df1-4781-8b93-9337c48323b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98daae52-6cbd-47f9-9b2a-3fb2adeba874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\victo\\OneDrive\\Escriptori\\3r\\Paradigms\\examples\\wandb\\run-20251014_164807-rcecfzaa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1/runs/rcecfzaa' target=\"_blank\">stellar-wildflower-3</a></strong> to <a href='https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1' target=\"_blank\">https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1/runs/rcecfzaa' target=\"_blank\">https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1/runs/rcecfzaa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1/runs/rcecfzaa?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1ecab6e62c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(project=\"M3-1_Example_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7c6f0-f1a4-465b-ab9b-4acafac0ca2b",
   "metadata": {},
   "source": [
    "We define constants at the top of the file and they include the count of neurons in the hidden layer, the count of episodes we play on every iteration (16), and the percentile of episodes' total rewards that we use for elite episode filtering. We'll take the 70th percentile, which means that we'll leave the top 30% of episodes sorted by reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eb8f844-ea62-4a4b-8bef-33665d5725de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab1bad-007c-4e65-bd0a-998277fe3c6d",
   "metadata": {},
   "source": [
    "Our model's core is a one-hidden-layer neural network, with ReLU and 128 hidden neurons (which is absolutely arbitrary). Other hyperparameters are also set almost randomly and aren't tuned, as the method is robust and converges very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343dbcb0-e6c0-4dd3-8423-2deb77ddcb3f",
   "metadata": {},
   "source": [
    "There is nothing special about our network:\n",
    "- It takes a single observation from the environment as an input vector and outputs a number for every action we can perform. \n",
    "- The output from the network is a probability distribution over actions, so a straightforward way to proceed would be to include softmax nonlinearity after the last layer. However, in the following network we don't apply softmax to increase the numerical stability of the training process. \n",
    "\n",
    "Rather than calculating softmax (which uses exponentiation) and then calculating cross-entropy loss (which uses logarithm of probabilities), we'll use the PyTorch class, `nn.CrossEntropyLoss`, which combines both softmax and cross-entropy in a single, more numerically stable expression. `CrossEntropyLoss` requires raw, unnormalized values from the network (also called _logits_), and the downside of this is that we need to remember to apply softmax every time we need to get probabilities from our network's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3a4fd40-d7f5-44ff-9a62-d0a40272ce4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c50c8-5e19-4a77-b18c-9c33a7caa0d9",
   "metadata": {},
   "source": [
    "Here we will define two helper classes that are named tuples from the collections package in the standard library:\n",
    "- `EpisodeStep`: This will be used to represent one single step that our agent made in the episode, and it stores the observation from the environment and what action the agent completed. We'll use episode steps from elite episodes as training data.\n",
    "- `Episode`: This is a single episode stored as total undiscounted reward and a collection of EpisodeStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6680969-b736-42d6-9723-ff804ada1fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22fc17-3e3b-47c2-ae19-0ce04d9990ee",
   "metadata": {},
   "source": [
    "Let's look at a function that generates batches with episodes:\n",
    "\n",
    "- The function accepts the environment (the `Env` class instance from the Gymnasium library), our neural network, and the count of episodes it should generate on every iteration. \n",
    "- The `batch` variable will be used to accumulate our batch (which is a list of the `Episode` instances). \n",
    "- We also declare a reward counter for the current episode and its list of steps (the `EpisodeStep` objects). \n",
    "\n",
    "Then we reset our environment to obtain the first observation and create a softmax layer, which will be used to convert the network's output to a probability distribution of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab43bb52-736a-4603-b90c-6c6da5ef95fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()[0]\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor(np.array([obs]))\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        is_done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        \n",
    "        if is_done:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()[0]\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d6f3c-76c8-4869-9c18-6102001e0a13",
   "metadata": {},
   "source": [
    "At every iteration (`while True`, line 7), we convert our current observation to a PyTorch tensor and pass it to the network to obtain action probabilities. There are several things to note here:\n",
    "- All `nn.Module` instances in PyTorch expect a batch of data items and the same is true for our network, so we convert our observation (which is a vector of four numbers in CartPole) into a tensor of size $1 \\times 4$ (to achieve this we pass an observation in a single-element list).\n",
    "\n",
    "- As we haven't used nonlinearity at the output of our network, it outputs raw action scores, which we need to feed through the softmax function.\n",
    "\n",
    "- Both our network and the softmax layer return tensors which track gradients, so we need to unpack this by accessing the `tensor.data` field and then converting the tensor into a NumPy array. This array will have the same two-dimensional structure as the input, with the batch dimension on axis 0, so we need to get the first batch element to obtain a one-dimensional vector of action probabilities:\n",
    "\n",
    "> action = np.random.choice(len(act_probs), p=act_probs)\n",
    "\n",
    "> next_obs, reward, is_done, _ = env.step(action)\n",
    "\n",
    "- Now that we have the probability distribution of actions, we can use this distribution to obtain the actual action for the current step by sampling this distribution using NumPy's function, `random.choice()`. After this, we will pass this action to the environment to get our next observation, our reward, and the indication of the episode ending:\n",
    "\n",
    "> episode_reward += reward\n",
    "\n",
    "> episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "\n",
    "- Reward is added to the current episode's total reward, and our list of episode steps is also extended with an (observation, action) pair. Note that we save the observation that was used to choose the action, but not the observation returned by the environment as a result of the action. These are the tiny but important details that you need to keep in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479042f-8668-4cfa-89ea-946c113c9464",
   "metadata": {},
   "source": [
    "At the end of the episode (`if is_done`, line 17):\n",
    "\n",
    "- This is how we handle the situation when the current episode is over. \n",
    "- We append the finalized episode to the batch, saving the total reward (as the episode has been completed and we've accumulated all reward) and steps we've taken. \n",
    "- Then we reset our total reward accumulator and clean the list of steps. After that, we reset our environment to start over.\n",
    "- In case our batch has reached the desired count of episodes, we return it to the caller for processing, using `yield`. Our function is a generator, so every time the `yield` operator is executed, the control is transferred to the outer iteration loop and then continues after the `yield` line. \n",
    "- After processing, we will clean up the batch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cd71c-9504-4310-b53d-880d92036b80",
   "metadata": {},
   "source": [
    "The last, but very important, step in our loop is to assign an observation obtained from the environment to our current observation variable. After that, everything repeats infinitely: we pass the observation to the net, sample the action to perform, ask the environment to process the action, and remember the result of this processing.\n",
    "\n",
    "One very important fact to understand in this function logic is that the training of our network and the generation of our episodes are performed at the same time. They are not completely in parallel, but every time our loop accumulates enough episodes (16), it passes control to this function caller, which is supposed to train the network using the gradient descent. So, when `yield` is returned, the network will have different, slightly better (we hope) behavior.\n",
    "\n",
    "We don't need to explore proper synchronization, as our training and data gathering activities are performed at the same thread of execution, but you need to understand those constant jumps from network training to its utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1e09e-f93b-4372-a864-008cd0ed44d9",
   "metadata": {},
   "source": [
    "We need to define yet another function and we'll be ready to switch to the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a0435c8-723e-4e16-8640-3c57abf5ba16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(np.array(train_obs))\n",
    "    train_act_v = torch.LongTensor(np.array(train_act))\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a441b-d6df-45ee-a601-7f38b1da9531",
   "metadata": {},
   "source": [
    "This function is at the core of the cross-entropy method: \n",
    "- from the given batch of episodes and percentile value, it calculates a boundary reward, which is used to filter elite episodes to train on. \n",
    "\n",
    "To obtain the boundary reward, we're using NumPy's `percentile` function, which from the list of values and the desired percentile, calculates the percentile's value. Then we will calculate mean reward, which is used only for monitoring.\n",
    "\n",
    "Next, we will filter off our episodes. For every episode in the batch, we will check that the episode has a higher total reward than our boundary and if it has, we will populate lists of observations and actions that we will train on.\n",
    "\n",
    "As the final step of the function, we will convert our observations and actions from elite episodes into tensors, and return a tuple of four: observations, actions, the boundary of reward, and the mean reward. The last two values will be used only to write them into TensorBoard to check the performance of our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe53df3-15c5-4c75-ad72-c2f65912f198",
   "metadata": {},
   "source": [
    "The final chunk of code that glues everything together and mostly consists of the training loop is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99e4e828-cce6-49a0-a854-5d74fa4536e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss=0.678, reward_mean=19.3, reward_bound=20.0\n",
      "Iteration 1: loss=0.678, reward_mean=18.9, reward_bound=20.5\n",
      "Iteration 2: loss=0.693, reward_mean=25.8, reward_bound=24.5\n",
      "Iteration 3: loss=0.660, reward_mean=32.6, reward_bound=40.0\n",
      "Iteration 4: loss=0.669, reward_mean=36.2, reward_bound=38.5\n",
      "Iteration 5: loss=0.656, reward_mean=27.9, reward_bound=33.5\n",
      "Iteration 6: loss=0.659, reward_mean=33.2, reward_bound=33.5\n",
      "Iteration 7: loss=0.644, reward_mean=27.9, reward_bound=28.0\n",
      "Iteration 8: loss=0.627, reward_mean=29.0, reward_bound=31.0\n",
      "Iteration 9: loss=0.640, reward_mean=38.9, reward_bound=41.0\n",
      "Iteration 10: loss=0.635, reward_mean=44.0, reward_bound=54.0\n",
      "Iteration 11: loss=0.600, reward_mean=49.5, reward_bound=55.0\n",
      "Iteration 12: loss=0.635, reward_mean=45.4, reward_bound=45.0\n",
      "Iteration 13: loss=0.607, reward_mean=54.4, reward_bound=58.0\n",
      "Iteration 14: loss=0.587, reward_mean=49.1, reward_bound=54.0\n",
      "Iteration 15: loss=0.596, reward_mean=68.4, reward_bound=85.0\n",
      "Iteration 16: loss=0.594, reward_mean=43.5, reward_bound=45.0\n",
      "Iteration 17: loss=0.595, reward_mean=44.9, reward_bound=58.0\n",
      "Iteration 18: loss=0.599, reward_mean=46.6, reward_bound=58.0\n",
      "Iteration 19: loss=0.572, reward_mean=53.7, reward_bound=61.0\n",
      "Iteration 20: loss=0.564, reward_mean=53.1, reward_bound=70.5\n",
      "Iteration 21: loss=0.572, reward_mean=50.9, reward_bound=57.0\n",
      "Iteration 22: loss=0.540, reward_mean=63.4, reward_bound=65.5\n",
      "Iteration 23: loss=0.539, reward_mean=67.1, reward_bound=86.5\n",
      "Iteration 24: loss=0.547, reward_mean=65.8, reward_bound=70.0\n",
      "Iteration 25: loss=0.510, reward_mean=88.2, reward_bound=101.0\n",
      "Iteration 26: loss=0.546, reward_mean=81.6, reward_bound=94.0\n",
      "Iteration 27: loss=0.532, reward_mean=69.5, reward_bound=76.5\n",
      "Iteration 28: loss=0.533, reward_mean=69.7, reward_bound=80.5\n",
      "Iteration 29: loss=0.547, reward_mean=69.1, reward_bound=66.5\n",
      "Iteration 30: loss=0.523, reward_mean=78.1, reward_bound=72.5\n",
      "Iteration 31: loss=0.507, reward_mean=77.1, reward_bound=81.0\n",
      "Iteration 32: loss=0.519, reward_mean=88.1, reward_bound=93.0\n",
      "Iteration 33: loss=0.509, reward_mean=81.3, reward_bound=84.0\n",
      "Iteration 34: loss=0.500, reward_mean=86.1, reward_bound=100.0\n",
      "Iteration 35: loss=0.499, reward_mean=86.0, reward_bound=100.0\n",
      "Iteration 36: loss=0.516, reward_mean=114.6, reward_bound=138.5\n",
      "Iteration 37: loss=0.498, reward_mean=87.6, reward_bound=98.5\n",
      "Iteration 38: loss=0.505, reward_mean=104.6, reward_bound=112.5\n",
      "Iteration 39: loss=0.488, reward_mean=104.1, reward_bound=109.5\n",
      "Iteration 40: loss=0.499, reward_mean=136.3, reward_bound=144.5\n",
      "Iteration 41: loss=0.489, reward_mean=118.6, reward_bound=128.5\n",
      "Iteration 42: loss=0.496, reward_mean=136.4, reward_bound=171.0\n",
      "Iteration 43: loss=0.495, reward_mean=106.2, reward_bound=115.5\n",
      "Iteration 44: loss=0.500, reward_mean=131.5, reward_bound=163.0\n",
      "Iteration 45: loss=0.510, reward_mean=118.4, reward_bound=137.0\n",
      "Iteration 46: loss=0.489, reward_mean=145.1, reward_bound=170.0\n",
      "Iteration 47: loss=0.475, reward_mean=123.8, reward_bound=153.5\n",
      "Iteration 48: loss=0.498, reward_mean=173.6, reward_bound=197.5\n",
      "Iteration 49: loss=0.498, reward_mean=167.9, reward_bound=177.0\n",
      "Iteration 50: loss=0.492, reward_mean=185.5, reward_bound=236.5\n",
      "Iteration 51: loss=0.478, reward_mean=193.4, reward_bound=212.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52: loss=0.480, reward_mean=215.3, reward_bound=234.5\n",
      "Solved!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▇██▇▇▇▆▆▆▅▅▅▅▅▅▃▃▂▃▃▃▃▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂▂▂▁</td></tr><tr><td>reward_bound</td><td>▁▁▁▂▁▂▂▂▂▂▃▂▂▂▂▂▂▃▄▃▃▃▃▃▃▄▅▄▄▄▅▆▄▆▅▅▇▆██</td></tr><tr><td>reward_mean</td><td>▁▁▁▁▂▂▁▂▂▂▂▂▃▂▂▂▂▃▃▃▃▃▃▃▃▃▄▃▄▄▅▅▄▅▅▅▇▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.48026</td></tr><tr><td>reward_bound</td><td>234.5</td></tr><tr><td>reward_mean</td><td>215.3125</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-wildflower-3</strong> at: <a href='https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1/runs/rcecfzaa' target=\"_blank\">https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1/runs/rcecfzaa</a><br> View project at: <a href='https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1' target=\"_blank\">https://wandb.ai/victorbrr17-universitat-aut-noma-de-barcelona/M3-1_Example_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251014_164807-rcecfzaa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # log metrics to wandb\n",
    "    print(\"Iteration %d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    wandb.log({\"loss\": loss_v.item(), \"reward_mean\": reward_m, \"reward_bound\": reward_b}, step=iter_no)\n",
    "    \n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "        \n",
    "# Finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14e956-08fa-4303-be23-f7f6f11311cd",
   "metadata": {},
   "source": [
    "In the beginning, we will create all the required objects: the environment, our neural network, the objective function, the optimizer, and the summary writer for TensorBoard. The commented line creates a monitor to write videos of your agent's performance.\n",
    "\n",
    "In the training loop, we will iterate our batches (which are a list of `Episode` objects), then we perform filtering of the elite episodes using the `filter_batch` function. The result is variables of observations and taken actions, the reward boundary used for filtering and the mean reward. After that, we zero gradients of our network and pass observations to the network, obtaining its action scores. These scores are passed to the objective function, which calculates cross-entropy between the network output and the actions that the agent took. The idea of this is to reinforce our network to carry out those \"elite\" actions which have led to good rewards. Then, we will calculate gradients on the loss and ask the optimizer to adjust our network.\n",
    "\n",
    "The rest of the loop is mostly the monitoring of progress. On the console, we show iteration number, loss, the mean reward of the batch, and the reward boundary. We also write the same values to TensorBoard, to get a nice chart of the agent's learning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d1452-f680-49bd-b36b-918aac83a042",
   "metadata": {},
   "source": [
    "The last check in the loop is the comparison of the mean rewards of our batch episodes. When this becomes greater than `199`, we stop our training. Why `199`? In Gymnasium, the CartPole environment is considered to be solved when the mean reward for last 100 episodes is greater than 195, but our method converges so quickly that 100 episodes are usually what we need. The properly trained agent can balance the stick infinitely long (obtaining any amount of score), but the length of the episode in CartPole is limited to 200 steps (if you look at the environment variable of CartPole, you may notice the `TimeLimit` wrapper, which stops the episode after 200 steps). With all this in mind, we will stop training after the mean reward in the batch is greater than `199`, which is a good indication that our agent knows how to balance the stick as a pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae3242-fb22-492a-a5d1-1bcebfb3367c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
