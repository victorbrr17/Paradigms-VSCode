{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_g80ouHWKmS"
   },
   "source": [
    "# M4 - Reinforcement Learning\n",
    "\n",
    "## Example of the CartPole environment\n",
    "\n",
    "Below we will see a simple example that will allow us to understand the concepts introduced in this section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will install the [Gymnasium](https://gymnasium.farama.org/) library (if we do not have it installed):\n",
    "\n",
    "> !pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backward compatibility with code created for OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3fWjGowWKmV"
   },
   "source": [
    "## CartPole\n",
    "\n",
    "In this first example we are going to load the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment and perform some tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1yuotpsWKmV"
   },
   "source": [
    "### 1. Data load\n",
    "\n",
    "The following code loads the necessary packages for the example, creates the environment using the `make` method and prints on the screen the dimension of the:\n",
    "- **action space** (two actions: 0 = left and 1 = right), \n",
    "- **observations space** (four observations : cart position, cart speed, pole angle, and pole speed at the tip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664576950762,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "nebWbOt2WKmW",
    "outputId": "5144887a-0bcb-4b7c-d463-cb55c808f59f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is      : Discrete(2) \n",
      "Observation space is : Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32) \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print(\"Action space is      : {} \".format(env.action_space))\n",
    "print(\"Observation space is : {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nActINMkWKmX"
   },
   "source": [
    "Next, we **reset the environment** (an action that must always be performed after its creation) and initialize the variables that will store:\n",
    "- the number of steps executed (t), \n",
    "- the accumulated reward (`total_reward`) and \n",
    "- the variable that will tell us when an episode ends (`done`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1664576956034,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "wY_e55sMWKmX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Environment reset\n",
    "obs, info = env.reset()\n",
    "t, total_reward, done = 0, 0, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFIN7bAzWKmX"
   },
   "source": [
    "### 2. Running an episode\n",
    "\n",
    "Next, we will run an episode of the `CartPole` environment using an agent that **selects actions randomly**.\n",
    "\n",
    "- The following code runs an episode of the environment (it ends when the `done` variable takes the value `True`). \n",
    "- The agent is implemented using the `env.action_space.sample()` method which selects a random action. \n",
    "- For each step (_time step_), the observation generated by the environment (the four values discussed above), the selected action and the reward obtained in that step (+1 in each action until the episode ends) are printed on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1664576982372,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "8voYzctFWKmY",
    "outputId": "63bfa313-405b-432e-a1f9-ba8b60754fac",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: [-0.049  0.048  0.001  0.028] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.048  0.243  0.002 -0.265] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.043  0.048 -0.004  0.028] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.042  0.243 -0.003 -0.265] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.037  0.438 -0.008 -0.559] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.029  0.634 -0.02  -0.854] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.016  0.829 -0.037 -1.153] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 1.000e-03  1.025e+00 -6.000e-02 -1.457e+00] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.021  0.83  -0.089 -1.183] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.038  0.636 -0.112 -0.92 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.05   0.833 -0.131 -1.246] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.067  0.64  -0.156 -0.997] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.08   0.836 -0.176 -1.334] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.097  1.033 -0.202 -1.676] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.117  0.841 -0.236 -1.453] -> Action: 0 and reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "    # Get random action (this is the implementation of the agent)\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Execute action and get response\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n",
    "    \n",
    "    obs = new_obs\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    \n",
    "total_reward += reward\n",
    "t += 1\n",
    "print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPMRzVa-WKmY"
   },
   "source": [
    "Finally, we print the results and **close the environment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1664576988460,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "YBznpx4DWKmZ",
    "outputId": "08db4aba-1db7-4e23-fc36-1729d1fc8759",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 15 timesteps and reward was 15.0 \n"
     ]
    }
   ],
   "source": [
    "print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAAZ6MESWKmZ"
   },
   "source": [
    "### 3. Simulating several episodes\n",
    "\n",
    "The following code fragment repeats the process from the previous section for the number of episodes defined in the `num_episodes` variable. \n",
    "\n",
    "- The episodes are rendered to display them on the screen in an external window.\n",
    "\n",
    "<u>Notes</u>:\n",
    "- The parameter `render_mode=\"human\"` renders it to an external window, but it does not work in **Google Colab**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1664577001709,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "uMmAjhv9WKmZ",
    "outputId": "7ab6176c-0c74-4bef-a5a0-b2e78973d03e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 1 \n",
      "Obs: [ 0.031 -0.016  0.042  0.029] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.03   0.179  0.043 -0.25 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.034 -0.017  0.038  0.056] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.034 -0.213  0.039  0.361] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.029 -0.018  0.046  0.081] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.029 -0.214  0.048  0.388] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.025 -0.41   0.056  0.695] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.017 -0.215  0.07   0.42 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.012 -0.411  0.078  0.734] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.004 -0.608  0.093  1.05 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.008 -0.804  0.114  1.37 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.024 -0.61   0.141  1.115] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.036 -0.807  0.163  1.449] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.053 -1.004  0.192  1.788] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.073 -1.2    0.228  2.133] -> Action: 0 and reward: 1.0\n",
      "Episode 1 finished after 15 timesteps and reward was 15.0 \n",
      "\n",
      "Running episode 2 \n",
      "Obs: [ 0.001 -0.005 -0.017 -0.014] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.001  0.191 -0.017 -0.312] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.004 -0.004 -0.024 -0.025] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.004 -0.199 -0.024  0.26 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.    -0.004 -0.019 -0.04 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.    -0.198 -0.02   0.247] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.004 -0.003 -0.015 -0.052] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.004  0.192 -0.016 -0.349] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.     0.388 -0.023 -0.647] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.008  0.583 -0.036 -0.947] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.019  0.388 -0.055 -0.665] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.027  0.584 -0.068 -0.975] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.039  0.39  -0.087 -0.704] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.047  0.196 -0.102 -0.44 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.051  0.003 -0.11  -0.181] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.051 -0.191 -0.114  0.075] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.047 -0.384 -0.112  0.329] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.039 -0.187 -0.106  0.003] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.036  0.009 -0.106 -0.321] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.036 -0.184 -0.112 -0.063] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.032 -0.378 -0.113  0.192] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.024 -0.571 -0.11   0.447] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.013 -0.764 -0.101  0.703] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.002 -0.568 -0.087  0.38 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.014 -0.762 -0.079  0.645] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.029 -0.566 -0.066  0.328] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.04  -0.76  -0.06   0.599] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.055 -0.564 -0.048  0.288] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.067 -0.368 -0.042 -0.019] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.074 -0.173 -0.042 -0.324] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.077 -0.367 -0.049 -0.045] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.085 -0.171 -0.05  -0.353] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.088 -0.366 -0.057 -0.076] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.096 -0.56  -0.058  0.198] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.107 -0.364 -0.054 -0.113] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.114 -0.168 -0.056 -0.422] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.117 -0.362 -0.065 -0.147] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.125 -0.166 -0.068 -0.46 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.128 -0.361 -0.077 -0.189] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.135 -0.164 -0.081 -0.505] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.138  0.032 -0.091 -0.822] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.138  0.228 -0.107 -1.142] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.133  0.424 -0.13  -1.467] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.125  0.231 -0.16  -1.217] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.12   0.428 -0.184 -1.555] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.112  0.235 -0.215 -1.325] -> Action: 0 and reward: 1.0\n",
      "Episode 2 finished after 46 timesteps and reward was 46.0 \n",
      "\n",
      "Running episode 3 \n",
      "Obs: [-0.003 -0.046  0.021  0.034] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.004 -0.241  0.022  0.333] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.008 -0.047  0.029  0.047] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.009  0.148  0.03  -0.236] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.006  0.343  0.025 -0.519] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.     0.538  0.015 -0.804] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.011  0.342 -0.001 -0.507] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.018  0.147 -0.012 -0.215] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.021  0.342 -0.016 -0.511] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.028  0.148 -0.026 -0.223] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.031 -0.047 -0.031  0.061] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.03   0.148 -0.029 -0.241] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.033  0.344 -0.034 -0.543] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.04   0.539 -0.045 -0.846] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.051  0.345 -0.062 -0.568] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.057  0.151 -0.073 -0.295] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.06   0.347 -0.079 -0.61 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.067  0.153 -0.091 -0.344] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.07   0.349 -0.098 -0.664] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.077  0.546 -0.112 -0.986] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.088  0.352 -0.131 -0.73 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.095  0.159 -0.146 -0.481] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.099 -0.034 -0.155 -0.238] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.098 -0.226 -0.16   0.002] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.093 -0.029 -0.16  -0.337] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.093 -0.222 -0.167 -0.098] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.088 -0.025 -0.169 -0.439] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.088 -0.217 -0.178 -0.204] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.083 -0.02  -0.182 -0.547] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.083  0.177 -0.193 -0.891] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.087 -0.015 -0.211 -0.664] -> Action: 0 and reward: 1.0\n",
      "Episode 3 finished after 31 timesteps and reward was 31.0 \n",
      "\n",
      "Running episode 4 \n",
      "Obs: [-0.007 -0.003  0.018  0.041] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.007  0.192  0.019 -0.246] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.003  0.387  0.014 -0.532] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.005  0.191  0.003 -0.235] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.008  0.386 -0.002 -0.527] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.016  0.582 -0.012 -0.82 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.028  0.387 -0.029 -0.531] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.036  0.582 -0.039 -0.833] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.047  0.388 -0.056 -0.553] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.055  0.193 -0.067 -0.278] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.059 -0.001 -0.072 -0.007] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.059 -0.195 -0.073  0.262] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.055  0.001 -0.067 -0.053] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.055  0.197 -0.068 -0.366] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.059  0.393 -0.076 -0.68 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.067  0.589 -0.089 -0.995] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.079  0.396 -0.109 -0.732] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.086  0.592 -0.124 -1.057] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.098  0.399 -0.145 -0.806] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.106  0.596 -0.161 -1.14 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.118  0.792 -0.184 -1.479] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.134  0.6   -0.214 -1.249] -> Action: 0 and reward: 1.0\n",
      "Episode 4 finished after 22 timesteps and reward was 22.0 \n",
      "\n",
      "Running episode 5 \n",
      "Obs: [ 0.017  0.002  0.018 -0.001] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.017 -0.194  0.018  0.298] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.013 -0.389  0.023  0.596] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.006 -0.584  0.035  0.896] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.006 -0.39   0.053  0.614] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.014 -0.586  0.066  0.923] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.026 -0.391  0.084  0.652] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.033 -0.588  0.097  0.97 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.045 -0.784  0.116  1.291] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.061 -0.59   0.142  1.037] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.073 -0.397  0.163  0.792] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.081 -0.594  0.179  1.132] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.092 -0.402  0.202  0.9  ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.101 -0.599  0.22   1.249] -> Action: 0 and reward: 1.0\n",
      "Episode 5 finished after 14 timesteps and reward was 14.0 \n",
      "\n",
      "Running episode 6 \n",
      "Obs: [-0.016 -0.01   0.036  0.043] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.016  0.184  0.036 -0.239] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.012  0.379  0.032 -0.519] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.005  0.574  0.021 -0.802] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.007  0.378  0.005 -0.503] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.014  0.573 -0.005 -0.794] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.026  0.768 -0.021 -1.088] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.041  0.574 -0.042 -0.802] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.053  0.379 -0.058 -0.523] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.06   0.575 -0.069 -0.833] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.072  0.771 -0.086 -1.147] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.087  0.967 -0.108 -1.465] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.107  1.163 -0.138 -1.79 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.13   1.36  -0.174 -2.122] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.157  1.556 -0.216 -2.463] -> Action: 1 and reward: 1.0\n",
      "Episode 6 finished after 15 timesteps and reward was 15.0 \n",
      "\n",
      "Running episode 7 \n",
      "Obs: [-0.018  0.021 -0.041 -0.028] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.017  0.216 -0.041 -0.333] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.013  0.412 -0.048 -0.638] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.005  0.218 -0.061 -0.361] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.     0.414 -0.068 -0.673] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.008  0.22  -0.082 -0.402] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.012  0.026 -0.09  -0.136] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.013 -0.168 -0.092  0.127] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.009  0.028 -0.09  -0.193] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.01   0.225 -0.094 -0.513] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.014  0.421 -0.104 -0.834] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.023  0.617 -0.121 -1.157] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.035  0.424 -0.144 -0.904] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.044  0.621 -0.162 -1.239] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.056  0.817 -0.187 -1.577] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.072  0.625 -0.218 -1.348] -> Action: 0 and reward: 1.0\n",
      "Episode 7 finished after 16 timesteps and reward was 16.0 \n",
      "\n",
      "Running episode 8 \n",
      "Obs: [-0.028 -0.018  0.028  0.021] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.028 -0.213  0.029  0.322] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.033 -0.409  0.035  0.624] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.041 -0.604  0.048  0.928] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.053 -0.8    0.066  1.235] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.069 -0.996  0.091  1.548] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.089 -0.802  0.122  1.285] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.105 -0.609  0.148  1.033] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.117 -0.805  0.168  1.368] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.133 -1.002  0.196  1.708] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.153 -0.81   0.23   1.482] -> Action: 1 and reward: 1.0\n",
      "Episode 8 finished after 11 timesteps and reward was 11.0 \n",
      "\n",
      "Running episode 9 \n",
      "Obs: [-0.044 -0.047  0.044  0.022] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.045 -0.243  0.045  0.329] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.05  -0.048  0.051  0.05 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.051 -0.244  0.052  0.359] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.056 -0.05   0.059  0.083] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.057 -0.246  0.061  0.394] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.062 -0.442  0.069  0.705] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.071 -0.638  0.083  1.019] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.084 -0.834  0.103  1.336] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.1   -0.64   0.13   1.078] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.113 -0.447  0.152  0.829] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.122 -0.254  0.168  0.587] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.127 -0.062  0.18   0.352] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.128  0.13   0.187  0.121] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.126 -0.067  0.189  0.466] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.127 -0.264  0.199  0.812] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.132 -0.072  0.215  0.588] -> Action: 1 and reward: 1.0\n",
      "Episode 9 finished after 17 timesteps and reward was 17.0 \n",
      "\n",
      "Running episode 10 \n",
      "Obs: [ 0.038 -0.044  0.009  0.029] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.037  0.151  0.01  -0.26 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.04  -0.044  0.005  0.035] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.04   0.151  0.006 -0.256] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.043  0.346  0.    -0.547] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.05   0.541 -0.01  -0.839] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.06   0.737 -0.027 -1.135] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.075  0.542 -0.05  -0.851] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.086  0.347 -0.067 -0.575] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.093  0.543 -0.078 -0.888] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.104  0.349 -0.096 -0.621] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.111  0.546 -0.109 -0.942] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.122  0.352 -0.128 -0.685] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.129  0.549 -0.141 -1.015] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.14   0.356 -0.162 -0.77 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.147  0.553 -0.177 -1.109] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.158  0.75  -0.199 -1.451] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.173  0.947 -0.228 -1.799] -> Action: 1 and reward: 1.0\n",
      "Episode 10 finished after 18 timesteps and reward was 18.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make(\"CartPole-v1\", render_mode=\"human\") # rendering\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Environment reset\n",
    "    obs, info = env.reset()\n",
    "    t, total_reward, done = 0, 0, False\n",
    "    \n",
    "    print('Running episode {} '.format(episode+1))\n",
    "    \n",
    "    while not done:\n",
    "    \n",
    "        # Get random action (this is the implementation of the agent)\n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "        # Execute action and get response\n",
    "        new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n",
    "    \n",
    "        obs = new_obs\n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "        \n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n",
    "    print(\"Episode {} finished after {} timesteps and reward was {} \".format(episode+1, t, total_reward))\n",
    "    print('')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gymnasium_v1-2-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
