{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_g80ouHWKmS"
   },
   "source": [
    "# M4 - Reinforcement Learning\n",
    "\n",
    "## Example of the CartPole environment\n",
    "\n",
    "Below we will see a simple example that will allow us to understand the concepts introduced in this section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will install the [Gymnasium](https://gymnasium.farama.org/) library (if we do not have it installed):\n",
    "\n",
    "> !pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backward compatibility with code created for OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3fWjGowWKmV"
   },
   "source": [
    "## CartPole\n",
    "\n",
    "In this first example we are going to load the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment and perform some tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1yuotpsWKmV"
   },
   "source": [
    "### 1. Data load\n",
    "\n",
    "The following code loads the necessary packages for the example, creates the environment using the `make` method and prints on the screen the dimension of the:\n",
    "- **action space** (two actions: 0 = left and 1 = right), \n",
    "- **observations space** (four observations : cart position, cart speed, pole angle, and pole speed at the tip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664576950762,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "nebWbOt2WKmW",
    "outputId": "5144887a-0bcb-4b7c-d463-cb55c808f59f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is      : Discrete(2) \n",
      "Observation space is : Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32) \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print(\"Action space is      : {} \".format(env.action_space))\n",
    "print(\"Observation space is : {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nActINMkWKmX"
   },
   "source": [
    "Next, we **reset the environment** (an action that must always be performed after its creation) and initialize the variables that will store:\n",
    "- the number of steps executed (t), \n",
    "- the accumulated reward (`total_reward`) and \n",
    "- the variable that will tell us when an episode ends (`done`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1664576956034,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "wY_e55sMWKmX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Environment reset\n",
    "obs, info = env.reset()\n",
    "t, total_reward, done = 0, 0, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFIN7bAzWKmX"
   },
   "source": [
    "### 2. Running an episode\n",
    "\n",
    "Next, we will run an episode of the `CartPole` environment using an agent that **selects actions randomly**.\n",
    "\n",
    "- The following code runs an episode of the environment (it ends when the `done` variable takes the value `True`). \n",
    "- The agent is implemented using the `env.action_space.sample()` method which selects a random action. \n",
    "- For each step (_time step_), the observation generated by the environment (the four values discussed above), the selected action and the reward obtained in that step (+1 in each action until the episode ends) are printed on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1664576982372,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "8voYzctFWKmY",
    "outputId": "63bfa313-405b-432e-a1f9-ba8b60754fac",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: [-0.019 -0.027 -0.024 -0.031] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.02   0.168 -0.024 -0.331] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.016  0.364 -0.031 -0.631] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.009  0.169 -0.044 -0.348] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.006 -0.025 -0.051 -0.07 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.006 -0.22  -0.052  0.207] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.01  -0.414 -0.048  0.483] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.019 -0.218 -0.038  0.175] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.023 -0.413 -0.035  0.456] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.031 -0.217 -0.026  0.152] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.036 -0.022 -0.023 -0.149] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.036 -0.217 -0.026  0.137] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.04  -0.021 -0.023 -0.164] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.041  0.174 -0.026 -0.464] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.037  0.37  -0.035 -0.764] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.03   0.175 -0.051 -0.483] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.027  0.371 -0.06  -0.791] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.019  0.567 -0.076 -1.102] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.008  0.373 -0.098 -0.834] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.     0.569 -0.115 -1.156] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.011  0.376 -0.138 -0.902] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.019  0.572 -0.156 -1.234] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.03   0.38  -0.181 -0.994] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.038  0.187 -0.201 -0.763] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.041 -0.005 -0.216 -0.54 ] -> Action: 0 and reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "    # Get random action (this is the implementation of the agent)\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Execute action and get response\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n",
    "    \n",
    "    obs = new_obs\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    \n",
    "total_reward += reward\n",
    "t += 1\n",
    "print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPMRzVa-WKmY"
   },
   "source": [
    "Finally, we print the results and **close the environment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1664576988460,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "YBznpx4DWKmZ",
    "outputId": "08db4aba-1db7-4e23-fc36-1729d1fc8759",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 25 timesteps and reward was 25.0 \n"
     ]
    }
   ],
   "source": [
    "print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAAZ6MESWKmZ"
   },
   "source": [
    "### 3. Simulating several episodes\n",
    "\n",
    "The following code fragment repeats the process from the previous section for the number of episodes defined in the `num_episodes` variable. \n",
    "\n",
    "- The episodes are rendered to display them on the screen in an external window.\n",
    "\n",
    "<u>Notes</u>:\n",
    "- The parameter `render_mode=\"human\"` renders it to an external window, but it does not work in **Google Colab**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1664577001709,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "uMmAjhv9WKmZ",
    "outputId": "7ab6176c-0c74-4bef-a5a0-b2e78973d03e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 1 \n",
      "Obs: [-0.03  -0.001 -0.03   0.015] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.03   0.195 -0.03  -0.287] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.026  0.39  -0.036 -0.589] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.018  0.586 -0.048 -0.892] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.006  0.391 -0.065 -0.615] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.001  0.197 -0.078 -0.344] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.005  0.393 -0.085 -0.66 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.013  0.59  -0.098 -0.978] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.025  0.396 -0.117 -0.718] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.033  0.592 -0.132 -1.045] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.045  0.399 -0.153 -0.796] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.053  0.596 -0.169 -1.133] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.065  0.404 -0.191 -0.897] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.073  0.601 -0.209 -1.244] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.085  0.409 -0.234 -1.023] -> Action: 0 and reward: 1.0\n",
      "Episode 1 finished after 15 timesteps and reward was 15.0 \n",
      "\n",
      "Running episode 2 \n",
      "Obs: [0.023 0.004 0.012 0.043] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.023  0.199  0.013 -0.246] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.027  0.394  0.008 -0.534] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.035  0.199 -0.003 -0.239] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.039  0.004 -0.007  0.053] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.039  0.199 -0.006 -0.242] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.043  0.394 -0.011 -0.537] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.051  0.199 -0.022 -0.248] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.055  0.394 -0.027 -0.547] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.063  0.59  -0.038 -0.848] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.074  0.786 -0.055 -1.153] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.09   0.591 -0.078 -0.878] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.102  0.397 -0.095 -0.611] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.11   0.204 -0.108 -0.349] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.114  0.01  -0.115 -0.092] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.114  0.207 -0.116 -0.419] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.118  0.403 -0.125 -0.746] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.126  0.21  -0.14  -0.495] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.131  0.017 -0.15  -0.249] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.131  0.214 -0.155 -0.585] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.135  0.021 -0.166 -0.345] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.136 -0.171 -0.173 -0.109] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.132  0.026 -0.175 -0.451] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.133  0.223 -0.184 -0.794] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.137  0.031 -0.2   -0.564] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.138  0.228 -0.212 -0.913] -> Action: 1 and reward: 1.0\n",
      "Episode 2 finished after 26 timesteps and reward was 26.0 \n",
      "\n",
      "Running episode 3 \n",
      "Obs: [-0.027  0.026  0.037 -0.033] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.027  0.22   0.037 -0.314] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.022  0.415  0.03  -0.595] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.014  0.219  0.018 -0.293] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.01   0.024  0.013  0.006] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.009 -0.171  0.013  0.302] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.013  0.024  0.019  0.014] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.012  0.218  0.019 -0.273] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.008  0.023  0.014  0.026] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.007 -0.172  0.014  0.323] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.011  0.023  0.021  0.034] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.01   0.217  0.021 -0.252] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.006  0.412  0.016 -0.538] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.002  0.217  0.005 -0.24 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.007  0.412  0.001 -0.531] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.015  0.217 -0.01  -0.238] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.019  0.022 -0.015  0.052] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.02  -0.173 -0.014  0.34 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.016  0.022 -0.007  0.043] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.017 -0.173 -0.006  0.333] -> Action: 1 and reward: 1.0\n",
      "Obs: [0.013 0.022 0.001 0.039] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.014  0.218  0.001 -0.254] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.018  0.022 -0.004  0.039] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.018  0.218 -0.003 -0.255] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.023  0.413 -0.008 -0.548] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.031  0.218 -0.019 -0.258] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.035  0.023 -0.024  0.029] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.036  0.218 -0.024 -0.272] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.04   0.414 -0.029 -0.572] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.048  0.219 -0.04  -0.288] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.053  0.025 -0.046 -0.009] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.053  0.22  -0.046 -0.316] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.058  0.026 -0.053 -0.038] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.058 -0.168 -0.053  0.238] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.055 -0.363 -0.049  0.513] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.048 -0.557 -0.038  0.79 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.036 -0.752 -0.023  1.071] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.021 -0.556 -0.001  0.771] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.01  -0.361  0.014  0.478] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.003 -0.557  0.024  0.775] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.008 -0.362  0.039  0.49 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.015 -0.557  0.049  0.795] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.027 -0.753  0.065  1.102] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.042 -0.559  0.087  0.831] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.053 -0.755  0.104  1.149] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.068 -0.951  0.127  1.473] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.087 -0.758  0.156  1.222] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.102 -0.955  0.181  1.559] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.121 -1.152  0.212  1.902] -> Action: 0 and reward: 1.0\n",
      "Episode 3 finished after 49 timesteps and reward was 49.0 \n",
      "\n",
      "Running episode 4 \n",
      "Obs: [ 0.039  0.033 -0.005  0.043] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.039 -0.162 -0.004  0.334] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.036 -0.357  0.003  0.625] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.029 -0.162  0.016  0.334] -> Action: 1 and reward: 1.0\n",
      "Obs: [0.026 0.033 0.022 0.046] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.026 -0.163  0.023  0.345] -> Action: 1 and reward: 1.0\n",
      "Obs: [0.023 0.032 0.03  0.06 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.024 -0.163  0.031  0.362] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.02  -0.359  0.038  0.665] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.013 -0.555  0.052  0.969] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.002 -0.36   0.071  0.693] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.005 -0.166  0.085  0.424] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.008  0.028  0.093  0.159] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.008 -0.169  0.097  0.479] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.011  0.025  0.106  0.219] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.011  0.218  0.111 -0.039] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.006  0.412  0.11  -0.294] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.002  0.605  0.104 -0.551] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.014  0.799  0.093 -0.809] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.03   0.602  0.077 -0.488] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.042  0.406  0.067 -0.173] -> Action: 0 and reward: 1.0\n",
      "Obs: [0.05  0.21  0.064 0.141] -> Action: 0 and reward: 1.0\n",
      "Obs: [0.054 0.014 0.066 0.453] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.055 -0.182  0.075  0.765] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.051 -0.378  0.091  1.081] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.043 -0.184  0.112  0.818] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.04  -0.38   0.129  1.144] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.032 -0.577  0.152  1.474] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.021 -0.774  0.181  1.81 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.005 -0.581  0.217  1.578] -> Action: 1 and reward: 1.0\n",
      "Episode 4 finished after 30 timesteps and reward was 30.0 \n",
      "\n",
      "Running episode 5 \n",
      "Obs: [-0.002  0.019  0.028  0.037] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.002  0.214  0.029 -0.247] -> Action: 0 and reward: 1.0\n",
      "Obs: [0.003 0.018 0.024 0.055] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.003  0.213  0.025 -0.23 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.007  0.408  0.02  -0.515] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.015  0.212  0.01  -0.216] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.02   0.407  0.006 -0.506] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.028  0.602 -0.004 -0.796] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.04   0.407 -0.02  -0.505] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.048  0.603 -0.03  -0.804] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.06   0.408 -0.046 -0.521] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.068  0.214 -0.057 -0.243] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.072  0.019 -0.062  0.031] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.073  0.215 -0.061 -0.281] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.077  0.021 -0.067 -0.008] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.078  0.217 -0.067 -0.321] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.082  0.023 -0.073 -0.05 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.082  0.219 -0.074 -0.365] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.087  0.025 -0.082 -0.097] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.087  0.221 -0.084 -0.414] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.092  0.027 -0.092 -0.149] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.092 -0.166 -0.095  0.114] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.089 -0.36  -0.093  0.375] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.082 -0.164 -0.085  0.055] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.078 -0.357 -0.084  0.319] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.071 -0.161 -0.078  0.002] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.068 -0.355 -0.077  0.269] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.061 -0.549 -0.072  0.536] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.05  -0.353 -0.061  0.222] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.043 -0.547 -0.057  0.494] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.032 -0.741 -0.047  0.768] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.017 -0.936 -0.032  1.046] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.002 -0.74  -0.011  0.744] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.016 -0.935  0.004  1.033] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.035 -1.131  0.025  1.327] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.058 -0.936  0.051  1.042] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.076 -1.131  0.072  1.35 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.099 -0.937  0.099  1.081] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.118 -1.134  0.121  1.403] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.14  -1.33   0.149  1.731] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.167 -1.526  0.183  2.066] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.198 -1.723  0.225  2.409] -> Action: 0 and reward: 1.0\n",
      "Episode 5 finished after 42 timesteps and reward was 42.0 \n",
      "\n",
      "Running episode 6 \n",
      "Obs: [-0.037  0.041  0.018 -0.001] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.036 -0.154  0.018  0.297] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.039 -0.349  0.024  0.596] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.046 -0.155  0.036  0.311] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.05  -0.35   0.042  0.615] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.057 -0.546  0.055  0.92 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.067 -0.352  0.073  0.645] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.075 -0.158  0.086  0.377] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.078  0.036  0.094  0.112] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.077  0.23   0.096 -0.15 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.072  0.034  0.093  0.172] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.072  0.227  0.096 -0.09 ] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.067  0.031  0.094  0.231] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.067  0.225  0.099 -0.03 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.062  0.418  0.098 -0.29 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.054  0.612  0.093 -0.55 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.041  0.805  0.082 -0.812] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.025  0.609  0.065 -0.495] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.013  0.413  0.055 -0.183] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.005  0.217  0.052  0.127] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.001  0.022  0.054  0.436] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.    -0.174  0.063  0.745] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.004 -0.37   0.078  1.057] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.011 -0.176  0.099  0.789] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.014  0.017  0.115  0.529] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.014 -0.179  0.125  0.856] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.018 -0.376  0.143  1.185] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.025 -0.183  0.166  0.941] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.029 -0.38   0.185  1.281] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.036 -0.187  0.211  1.051] -> Action: 1 and reward: 1.0\n",
      "Episode 6 finished after 30 timesteps and reward was 30.0 \n",
      "\n",
      "Running episode 7 \n",
      "Obs: [-0.043 -0.002 -0.003  0.046] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.043 -0.197 -0.002  0.338] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.047 -0.002  0.004  0.044] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.047 -0.197  0.005  0.338] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.051 -0.002  0.012  0.047] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.051  0.193  0.013 -0.242] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.047  0.388  0.008 -0.53 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.04   0.583 -0.003 -0.821] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.028  0.778 -0.019 -1.114] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.012  0.974 -0.041 -1.413] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.007  1.169 -0.07  -1.718] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.03   0.975 -0.104 -1.448] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.05   1.171 -0.133 -1.771] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.073  1.368 -0.168 -2.102] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.101  1.564 -0.21  -2.442] -> Action: 1 and reward: 1.0\n",
      "Episode 7 finished after 15 timesteps and reward was 15.0 \n",
      "\n",
      "Running episode 8 \n",
      "Obs: [ 0.024 -0.027 -0.049  0.036] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.024  0.169 -0.049 -0.272] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.027  0.365 -0.054 -0.58 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.035  0.56  -0.066 -0.889] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.046  0.366 -0.083 -0.617] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.053  0.172 -0.096 -0.352] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.056  0.369 -0.103 -0.673] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.064  0.565 -0.116 -0.997] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.075  0.762 -0.136 -1.324] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.09   0.958 -0.163 -1.656] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.11   0.765 -0.196 -1.418] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.125  0.962 -0.224 -1.765] -> Action: 1 and reward: 1.0\n",
      "Episode 8 finished after 12 timesteps and reward was 12.0 \n",
      "\n",
      "Running episode 9 \n",
      "Obs: [ 0.001  0.003 -0.024 -0.048] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.002  0.199 -0.025 -0.348] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.006  0.394 -0.032 -0.649] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.013  0.2   -0.045 -0.366] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.017  0.005 -0.052 -0.088] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.017 -0.189 -0.054  0.188] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.014  0.007 -0.05  -0.121] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.014 -0.188 -0.053  0.155] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.01   0.008 -0.049 -0.154] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.01  -0.186 -0.053  0.123] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.007 -0.38  -0.05   0.399] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.001 -0.185 -0.042  0.091] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.005 -0.379 -0.04   0.37 ] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.012 -0.184 -0.033  0.065] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.016  0.012 -0.032 -0.238] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.016  0.208 -0.036 -0.541] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.012  0.403 -0.047 -0.845] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.004  0.599 -0.064 -1.152] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.008  0.795 -0.087 -1.464] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.024  0.991 -0.116 -1.782] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.044  0.797 -0.152 -1.528] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.06   0.994 -0.183 -1.864] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.08   0.801 -0.22  -1.633] -> Action: 0 and reward: 1.0\n",
      "Episode 9 finished after 23 timesteps and reward was 23.0 \n",
      "\n",
      "Running episode 10 \n",
      "Obs: [-0.017 -0.018 -0.01  -0.042] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.017 -0.213 -0.01   0.247] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.021 -0.018 -0.005 -0.048] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.022  0.177 -0.006 -0.343] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.018  0.372 -0.013 -0.638] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.011  0.177 -0.026 -0.349] -> Action: 0 and reward: 1.0\n",
      "Obs: [-0.007 -0.017 -0.033 -0.065] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.008  0.178 -0.034 -0.368] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.004  0.374 -0.042 -0.671] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.003  0.179 -0.055 -0.392] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.007 -0.015 -0.063 -0.117] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.007 -0.209 -0.065  0.155] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.003 -0.403 -0.062  0.427] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.006 -0.207 -0.054  0.115] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.01  -0.011 -0.051 -0.194] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.01   0.184 -0.055 -0.502] -> Action: 1 and reward: 1.0\n",
      "Obs: [-0.006  0.38  -0.065 -0.812] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 1.000e-03  5.760e-01 -8.100e-02 -1.124e+00] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.013  0.772 -0.104 -1.441] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.028  0.579 -0.133 -1.183] -> Action: 1 and reward: 1.0\n",
      "Obs: [ 0.04   0.775 -0.156 -1.514] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.055  0.582 -0.187 -1.274] -> Action: 0 and reward: 1.0\n",
      "Obs: [ 0.067  0.39  -0.212 -1.045] -> Action: 0 and reward: 1.0\n",
      "Episode 10 finished after 23 timesteps and reward was 23.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\") # rendering\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Environment reset\n",
    "    obs, info = env.reset()\n",
    "    t, total_reward, done = 0, 0, False\n",
    "    \n",
    "    print('Running episode {} '.format(episode+1))\n",
    "    \n",
    "    while not done:\n",
    "    \n",
    "        # Get random action (this is the implementation of the agent)\n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "        # Execute action and get response\n",
    "        new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n",
    "    \n",
    "        obs = new_obs\n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "        \n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n",
    "    print(\"Episode {} finished after {} timesteps and reward was {} \".format(episode+1, t, total_reward))\n",
    "    print('')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gym_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
