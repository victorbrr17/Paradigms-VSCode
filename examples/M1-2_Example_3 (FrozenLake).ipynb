{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_g80ouHWKmS"
   },
   "source": [
    "# M4 - Reinforcement Learning\n",
    "\n",
    "## Example of the ForzenLake environment\n",
    "\n",
    "Below we will see a simple example that will allow us to understand the concepts introduced in this section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will install the [Gymnasium](https://gymnasium.farama.org/) library (if we do not have it installed):\n",
    "\n",
    "> !pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backward compatibility with code created for OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIz74xD_WKma"
   },
   "source": [
    "## Frozen Lake\n",
    "\n",
    "In this example we are going to load the [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment and run some tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okEuELnbWKma"
   },
   "source": [
    "### 1. Data load\n",
    "\n",
    "The following code loads the packages necessary for the example, creates the environment using the `make` method and prints on the screen the dimension of:\n",
    "- the **action space** (0 = left, 1 = down, 2 = right and 3 = up), \n",
    "- the **space of observations** (a number from 0 to 15 that indicates the position of the agent in the environment) and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1664577011165,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "2vn7klp2WKmb",
    "outputId": "75a1bc93-6e7b-4f7b-d82c-abeb07655baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(4) \n",
      "Observation space is Discrete(16) \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe the default map:\n",
    " - S = starting cell\n",
    " - G = destination cell\n",
    " - H = hole\n",
    " - F = ice cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "print(env.unwrapped.desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhYrUP27WKmb"
   },
   "source": [
    "### 2. Running an episode\n",
    "\n",
    "Next, we will execute an episode of the FrozenLake environment using an agent that **selects actions randomly**.\n",
    "\n",
    "In the following code we initialize the environment, define the maximum number of steps per episode (`max_steps`) and execute an episode of the environment:\n",
    "- This ends when the `done` variable takes the value `True` or when the stipulated maximum number of steps. \n",
    "- We use an agent that implements a completely random policy (`env.action_space.sample()`). \n",
    "- Using the `env.render()` method we can see the evolution of the agent in the environment from the departure box S until it reaches the destination box G or falls into a hole H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1336,
     "status": "ok",
     "timestamp": 1664577018087,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "z1F072TfWKmb",
    "outputId": "2cd5c177-fc82-477e-a5d7-c12934266d14",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 2 timesteps and reward was 0.0 \n"
     ]
    }
   ],
   "source": [
    "# Environment reset\n",
    "obs, info = env.reset()\n",
    "t, total_reward, done = 0, 0, False\n",
    "max_steps = 100\n",
    "\n",
    "while t < max_steps:\n",
    "    # Get random action (this is the implementation of the agent)\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Execute action and get response\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "        \n",
    "    t += 1\n",
    "    if done:\n",
    "        break\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"Episode finished after {} timesteps and reward was {} \".format(t, reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpcCYXplWKmc"
   },
   "source": [
    "### 3. Simulating several episodes\n",
    "\n",
    "The following code fragment repeats the process from the previous section for the number of episodes defined in the `num_episodes` variable. \n",
    "\n",
    "- The episodes are rendered to display them on the screen in an external window (does not work in **Google Colab**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9799,
     "status": "ok",
     "timestamp": 1664577033307,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "fQStXZCxWKmc",
    "outputId": "5b7e4a10-f705-4cbf-ae3e-d057941df01b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 1 \n",
      "Episode 1 finished after 24 timesteps and reward was 0.0 \n",
      "\n",
      "Running episode 2 \n",
      "Episode 2 finished after 3 timesteps and reward was 0.0 \n",
      "\n",
      "Running episode 3 \n",
      "Episode 3 finished after 9 timesteps and reward was 0.0 \n",
      "\n",
      "Running episode 4 \n",
      "Episode 4 finished after 8 timesteps and reward was 0.0 \n",
      "\n",
      "Running episode 5 \n",
      "Episode 5 finished after 17 timesteps and reward was 0.0 \n",
      "\n",
      "Running episode 6 \n",
      "Episode 6 finished after 2 timesteps and reward was 0.0 \n",
      "\n",
      "Running episode 7 \n",
      "Episode 7 finished after 9 timesteps and reward was 0.0 \n",
      "\n",
      "Running episode 8 \n",
      "Episode 8 finished after 8 timesteps and reward was 0.0 \n",
      "\n",
      "Running episode 9 \n",
      "Episode 9 finished after 13 timesteps and reward was 0.0 \n",
      "\n",
      "Running episode 10 \n",
      "Episode 10 finished after 3 timesteps and reward was 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# render_mode = \"human\" (kernel crash using notebooks), \"ansi\"  \n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    # Environment reset\n",
    "    obs, info = env.reset()\n",
    "    t, done = 0, False\n",
    "    \n",
    "    print('Running episode {} '.format(episode+1))\n",
    "\n",
    "    while t < max_steps:\n",
    "        # Get random action (this is the implementation of the agent)\n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "        # Execute action and get response\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        t += 1\n",
    "        if done:\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "      \n",
    "    print(\"Episode {} finished after {} timesteps and reward was {} \".format(episode+1, t, reward))\n",
    "    print('')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEVWy8HXWKmc"
   },
   "source": [
    "### 4. Calculating the total reward of multiple episodes\n",
    "\n",
    "To measure the efficiency of the agent, we can calculate the total reward of several episodes. \n",
    "\n",
    "- Given that in each episode the accumulated reward is 0 if the destination cell is not reached and 1 if the objective is achieved, measuring the **total accumulated reward** for a number of episodes gives us a measure of the success rate of our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz-1buTNWKmd"
   },
   "source": [
    "The following code fragment repeats the process from the previous section for the number of episodes defined in the `num_episodes` variable and calculates the agent's success rate. \n",
    "\n",
    "- Rendering of the environment is omitted in order to speed up execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1664577047069,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "SdPY9xQVWKmd",
    "outputId": "8612774c-0805-4b0d-e7bb-b733d6d23d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0 successes in 1000 episodes: 1.4 % of success\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "num_episodes = 1000\n",
    "total_reward = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    # Environment reset\n",
    "    obs, info = env.reset()\n",
    "    t, done = 0, False\n",
    "    \n",
    "    while t < max_steps:\n",
    "        # Get random action (this is the implementation of the agent)\n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "        # Execute action and get response\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "success_rate = total_reward*100/num_episodes\n",
    "print(\"{} successes in {} episodes: {} % of success\".format(total_reward, num_episodes, success_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68WrxlGfWKmd"
   },
   "source": [
    "### 5. Training an agent in a deterministic environment\n",
    "\n",
    "As we have seen in the previous section, since the agent used chooses the actions at random, it is almost impossible to reach the destination square G with this policy (the success rate is 1% or 2%). \n",
    "\n",
    "- First of all, we will start using a **deterministic environment** (instead of the stochastic environment we have used until now).\n",
    "- Thus, it will be possible to define an **optimal policy** in a very simple way. \n",
    "- We will use a one-dimensional `ndarray`, where each position (corresponding to a single map cell) indicates the direction we must follow to find the correct exit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Define the enviroment using the `is_slippery=False` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
    "\n",
    "print(env.unwrapped.desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create an array to indicate the **optimal policy** to be followed by the agent.\n",
    "\n",
    "The actions are:\n",
    "- 0: Move left\n",
    "- 1: Move down\n",
    "- 2: Move right\n",
    "- 3: Move up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: [1 0 0 0 1 0 0 0 2 2 1 0 0 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "optimal_policy = np.array([1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 1, 0, 0, 2, 2, 2])\n",
    "\n",
    "print(\"Optimal policy: {}\".format(optimal_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: 0 -> Action: 1 and reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 4 -> Action: 1 and reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 8 -> Action: 2 and reward: 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "\n",
      "Obs: 9 -> Action: 2 and reward: 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "\n",
      "Obs: 10 -> Action: 1 and reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "Obs: 14 -> Action: 2 and reward: 1.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Episode finished after 6 timesteps and reward was 1.0 \n"
     ]
    }
   ],
   "source": [
    "# Environment reset\n",
    "obs, info = env.reset()\n",
    "t, done = 0, False\n",
    "\n",
    "while not done:\n",
    "    # Select the action to be performed\n",
    "    action = optimal_policy[obs]\n",
    "    \n",
    "    # Execute action and get response\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n",
    "        \n",
    "    t += 1\n",
    "    obs = new_obs\n",
    "    print(env.render())\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"Episode finished after {} timesteps and reward was {} \".format(t, reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training an agent in a stochastic environment\n",
    "\n",
    "Finally, we apply the **optimal policy** found in the previous section to a **stochastic version of the environment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: 0 -> Action: 1 and reward: 0.0\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 0 -> Action: 1 and reward: 0.0\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 1 -> Action: 0 and reward: 0.0\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 1 -> Action: 0 and reward: 0.0\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 1 -> Action: 0 and reward: 0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 0 -> Action: 1 and reward: 0.0\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 0 -> Action: 1 and reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 4 -> Action: 1 and reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "\n",
      "Obs: 8 -> Action: 2 and reward: 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "\n",
      "Obs: 9 -> Action: 2 and reward: 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Episode finished after 10 timesteps and reward was 0.0 \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"ansi\")\n",
    "\n",
    "# Environment reset\n",
    "obs, info = env.reset()\n",
    "t, done = 0, False\n",
    "\n",
    "while not done:\n",
    "    # Select the action to be performed\n",
    "    action = optimal_policy[obs]\n",
    "    \n",
    "    # Execute action and get response\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n",
    "        \n",
    "    t += 1\n",
    "    obs = new_obs\n",
    "    print(env.render())\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"Episode finished after {} timesteps and reward was {} \".format(t, reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gymnasium_v1-2-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
