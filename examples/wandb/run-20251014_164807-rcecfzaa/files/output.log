Iteration 0: loss=0.678, reward_mean=19.3, reward_bound=20.0
Iteration 1: loss=0.678, reward_mean=18.9, reward_bound=20.5
Iteration 2: loss=0.693, reward_mean=25.8, reward_bound=24.5
Iteration 3: loss=0.660, reward_mean=32.6, reward_bound=40.0
Iteration 4: loss=0.669, reward_mean=36.2, reward_bound=38.5
Iteration 5: loss=0.656, reward_mean=27.9, reward_bound=33.5
Iteration 6: loss=0.659, reward_mean=33.2, reward_bound=33.5
Iteration 7: loss=0.644, reward_mean=27.9, reward_bound=28.0
Iteration 8: loss=0.627, reward_mean=29.0, reward_bound=31.0
Iteration 9: loss=0.640, reward_mean=38.9, reward_bound=41.0
Iteration 10: loss=0.635, reward_mean=44.0, reward_bound=54.0
Iteration 11: loss=0.600, reward_mean=49.5, reward_bound=55.0
Iteration 12: loss=0.635, reward_mean=45.4, reward_bound=45.0
Iteration 13: loss=0.607, reward_mean=54.4, reward_bound=58.0
Iteration 14: loss=0.587, reward_mean=49.1, reward_bound=54.0
Iteration 15: loss=0.596, reward_mean=68.4, reward_bound=85.0
Iteration 16: loss=0.594, reward_mean=43.5, reward_bound=45.0
Iteration 17: loss=0.595, reward_mean=44.9, reward_bound=58.0
Iteration 18: loss=0.599, reward_mean=46.6, reward_bound=58.0
Iteration 19: loss=0.572, reward_mean=53.7, reward_bound=61.0
Iteration 20: loss=0.564, reward_mean=53.1, reward_bound=70.5
Iteration 21: loss=0.572, reward_mean=50.9, reward_bound=57.0
Iteration 22: loss=0.540, reward_mean=63.4, reward_bound=65.5
Iteration 23: loss=0.539, reward_mean=67.1, reward_bound=86.5
Iteration 24: loss=0.547, reward_mean=65.8, reward_bound=70.0
Iteration 25: loss=0.510, reward_mean=88.2, reward_bound=101.0
Iteration 26: loss=0.546, reward_mean=81.6, reward_bound=94.0
Iteration 27: loss=0.532, reward_mean=69.5, reward_bound=76.5
Iteration 28: loss=0.533, reward_mean=69.7, reward_bound=80.5
Iteration 29: loss=0.547, reward_mean=69.1, reward_bound=66.5
Iteration 30: loss=0.523, reward_mean=78.1, reward_bound=72.5
Iteration 31: loss=0.507, reward_mean=77.1, reward_bound=81.0
Iteration 32: loss=0.519, reward_mean=88.1, reward_bound=93.0
Iteration 33: loss=0.509, reward_mean=81.3, reward_bound=84.0
Iteration 34: loss=0.500, reward_mean=86.1, reward_bound=100.0
Iteration 35: loss=0.499, reward_mean=86.0, reward_bound=100.0
Iteration 36: loss=0.516, reward_mean=114.6, reward_bound=138.5
Iteration 37: loss=0.498, reward_mean=87.6, reward_bound=98.5
Iteration 38: loss=0.505, reward_mean=104.6, reward_bound=112.5
Iteration 39: loss=0.488, reward_mean=104.1, reward_bound=109.5
Iteration 40: loss=0.499, reward_mean=136.3, reward_bound=144.5
Iteration 41: loss=0.489, reward_mean=118.6, reward_bound=128.5
Iteration 42: loss=0.496, reward_mean=136.4, reward_bound=171.0
Iteration 43: loss=0.495, reward_mean=106.2, reward_bound=115.5
Iteration 44: loss=0.500, reward_mean=131.5, reward_bound=163.0
Iteration 45: loss=0.510, reward_mean=118.4, reward_bound=137.0
Iteration 46: loss=0.489, reward_mean=145.1, reward_bound=170.0
Iteration 47: loss=0.475, reward_mean=123.8, reward_bound=153.5
Iteration 48: loss=0.498, reward_mean=173.6, reward_bound=197.5
Iteration 49: loss=0.498, reward_mean=167.9, reward_bound=177.0
Iteration 50: loss=0.492, reward_mean=185.5, reward_bound=236.5
Iteration 51: loss=0.478, reward_mean=193.4, reward_bound=212.5
Iteration 52: loss=0.480, reward_mean=215.3, reward_bound=234.5
Solved!
[34m[1mwandb[0m: [32m[41mERROR[0m The nbformat package was not found. It is required to save notebook history.
