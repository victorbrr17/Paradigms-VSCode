Iteration 0: loss=0.693, reward_mean=19.4, reward_bound=22.5
Iteration 1: loss=0.691, reward_mean=20.6, reward_bound=22.5
Iteration 2: loss=0.688, reward_mean=20.5, reward_bound=26.0
Iteration 3: loss=0.676, reward_mean=22.9, reward_bound=28.5
Iteration 4: loss=0.680, reward_mean=24.3, reward_bound=27.5
Iteration 5: loss=0.677, reward_mean=27.4, reward_bound=30.0
Iteration 6: loss=0.664, reward_mean=31.4, reward_bound=34.5
Iteration 7: loss=0.665, reward_mean=30.2, reward_bound=33.5
Iteration 8: loss=0.637, reward_mean=36.4, reward_bound=41.0
Iteration 9: loss=0.646, reward_mean=38.4, reward_bound=46.5
Iteration 10: loss=0.624, reward_mean=40.5, reward_bound=46.0
Iteration 11: loss=0.620, reward_mean=36.1, reward_bound=42.0
Iteration 12: loss=0.618, reward_mean=59.1, reward_bound=56.0
Iteration 13: loss=0.613, reward_mean=40.8, reward_bound=45.0
Iteration 14: loss=0.613, reward_mean=51.4, reward_bound=58.5
Iteration 15: loss=0.592, reward_mean=45.1, reward_bound=54.0
Iteration 16: loss=0.601, reward_mean=54.8, reward_bound=58.0
Iteration 17: loss=0.592, reward_mean=52.5, reward_bound=65.0
Iteration 18: loss=0.586, reward_mean=46.0, reward_bound=47.5
Iteration 19: loss=0.566, reward_mean=49.1, reward_bound=57.5
Iteration 20: loss=0.573, reward_mean=49.1, reward_bound=59.5
Iteration 21: loss=0.558, reward_mean=55.3, reward_bound=61.5
Iteration 22: loss=0.549, reward_mean=54.4, reward_bound=69.0
Iteration 23: loss=0.573, reward_mean=65.2, reward_bound=61.5
Iteration 24: loss=0.541, reward_mean=71.4, reward_bound=85.5
Iteration 25: loss=0.545, reward_mean=78.6, reward_bound=90.5
Iteration 26: loss=0.550, reward_mean=75.7, reward_bound=71.0
Iteration 27: loss=0.544, reward_mean=76.8, reward_bound=91.5
Iteration 28: loss=0.536, reward_mean=80.4, reward_bound=86.0
Iteration 29: loss=0.525, reward_mean=74.3, reward_bound=78.5
Iteration 30: loss=0.527, reward_mean=90.4, reward_bound=107.0
Iteration 31: loss=0.527, reward_mean=75.9, reward_bound=85.0
Iteration 32: loss=0.512, reward_mean=89.1, reward_bound=118.5
Iteration 33: loss=0.527, reward_mean=74.4, reward_bound=80.5
Iteration 34: loss=0.517, reward_mean=78.5, reward_bound=78.0
Iteration 35: loss=0.511, reward_mean=82.6, reward_bound=98.5
Iteration 36: loss=0.522, reward_mean=97.5, reward_bound=104.5
Iteration 37: loss=0.510, reward_mean=107.2, reward_bound=120.5
Iteration 38: loss=0.521, reward_mean=97.8, reward_bound=106.0
Iteration 39: loss=0.507, reward_mean=92.3, reward_bound=103.0
Iteration 40: loss=0.507, reward_mean=111.4, reward_bound=118.5
Iteration 41: loss=0.504, reward_mean=141.1, reward_bound=165.0
Iteration 42: loss=0.507, reward_mean=130.7, reward_bound=175.0
Iteration 43: loss=0.512, reward_mean=142.5, reward_bound=165.0
Iteration 44: loss=0.493, reward_mean=137.9, reward_bound=157.5
Iteration 45: loss=0.513, reward_mean=151.7, reward_bound=165.0
Iteration 46: loss=0.503, reward_mean=147.6, reward_bound=157.5
Iteration 47: loss=0.492, reward_mean=163.6, reward_bound=199.0
Iteration 48: loss=0.491, reward_mean=151.7, reward_bound=164.5
Iteration 49: loss=0.502, reward_mean=156.0, reward_bound=169.0
Iteration 50: loss=0.502, reward_mean=157.1, reward_bound=167.5
Iteration 51: loss=0.487, reward_mean=155.0, reward_bound=163.0
Iteration 52: loss=0.481, reward_mean=156.9, reward_bound=158.5
Iteration 53: loss=0.491, reward_mean=183.8, reward_bound=198.5
Iteration 54: loss=0.480, reward_mean=154.2, reward_bound=172.5
Iteration 55: loss=0.494, reward_mean=165.6, reward_bound=173.5
Iteration 56: loss=0.499, reward_mean=177.2, reward_bound=197.5
Iteration 57: loss=0.484, reward_mean=206.9, reward_bound=219.0
Solved!
[34m[1mwandb[0m: [32m[41mERROR[0m The nbformat package was not found. It is required to save notebook history.
