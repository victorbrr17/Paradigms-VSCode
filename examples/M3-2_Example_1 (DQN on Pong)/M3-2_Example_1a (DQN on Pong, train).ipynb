{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Yb47zJQglm"
   },
   "source": [
    "# **Deep Reinforcement Learning**\n",
    "\n",
    "# M3-2 Deep Q-Networks\n",
    "\n",
    "## Example of DQN implementation on Pong environment (Part 1, training)\n",
    "\n",
    "Below we will see a simple example that will allow us to understand the concepts introduced in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maCHXYw1G8Km"
   },
   "source": [
    "## Pong environment\n",
    "\n",
    "The [Pong](https://ale.farama.org/environments/pong/) environment is part of the Arcade Learning Environment environments. The [Arcade Learning Environment]((https://ale.farama.org/environments/)) (ALE), commonly referred to as Atari, is a framework that allows researchers and hobbyists to develop AI agents for Atari 2600 roms. \n",
    "\n",
    "Please read that page first for general information.\n",
    "\n",
    "You control the right paddle, you compete against the left paddle controlled by the computer. You each try to keep deflecting the ball away from your goal and into your opponent’s goal.\n",
    "\n",
    "<center><img src=\"https://ale.farama.org/_images/pong.gif\"/></center>\n",
    "\n",
    "For a more detailed documentation, see the [AtariAge page](https://atariage.com/manual_html_page.php?SoftwareLabelID=587)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the environment. It's important to note that we are specifically using **version 1.0.0** of the **Gymnasium** library.\n",
    "\n",
    "To install this version of the environment, run the following command:\n",
    "> pip install gymnasium==1.0.0\n",
    "\n",
    "This will also install all the related packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dependencies are installed, we load them and initialize the `PongNoFrameskip-v4` environment.\n",
    "\n",
    "There are several Pong environments, with minor differences among them. See [Pong](https://ale.farama.org/environments/pong/) page for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FA1Y5VCv20XZ",
    "outputId": "571f131d-20ca-4243-88c2-fa1f18d08d11",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gymnasium version 1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+6a7e0ae)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "# version\n",
    "print(\"Using Gymnasium version {}\".format(gym.__version__))\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "test_env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QDaXip14JBv",
    "outputId": "39c796c7-ae7e-4194-9b22-6626ae4fcd39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "print(test_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uzLQLz04z2i",
    "outputId": "ad189248-3bfe-4703-cb2c-536adad2df0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy3RBfwzHRQq"
   },
   "source": [
    "## Data preprocessing (Wrappers)\n",
    "\n",
    "The original observations, provided by the environment, are:\n",
    "- images of $210 \\times 160$ in RGB colour\n",
    "- Thus, we represent each observation using a numpy array of `(210, 160, 3) dtype=int8'\n",
    "\n",
    "However, we have some problems:\n",
    "1. The observations include **parts of the screen** that are not rellevant.\n",
    "2. Number of states is: $256^{(210 \\times 160 \\times 3)}$ = $256^{100800}$! So, **reducing the image** size could help!\n",
    "3. The images of the environment are in **color (RGB)**, but does color really provide any information?\n",
    "4. In a single image it is not possible to know the **dynamics of the game** (i.e. direction and speed of the ball). Therefore, we must consider a sequence of several consecutive images to understand what is happening in the game.\n",
    "\n",
    "We will use several wrappers to transform the observations. Specifically, we want to get (from the environment) observations with the following characteristics:\n",
    "- Grayscale images\n",
    "- Resolution $84 \\times 84$\n",
    "- Float images $\\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./figs/preprocessing-1.jpg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"./figs/preprocessing-2.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipObservation: (210, 160, 3)\n",
      "ResizeObservation    : (84, 84, 3)\n",
      "GrayscaleObservation : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "ReshapeObservation   : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import MaxAndSkipObservation, ResizeObservation, GrayscaleObservation, FrameStackObservation, ReshapeObservation\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gymnasium.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "    env = MaxAndSkipObservation(env, skip=4)\n",
    "    print(\"MaxAndSkipObservation: {}\".format(env.observation_space.shape))\n",
    "    #env = FireResetEnv(env)\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n",
    "    env = GrayscaleObservation(env, keep_dim=True)\n",
    "    print(\"GrayscaleObservation : {}\".format(env.observation_space.shape))\n",
    "    env = ImageToPyTorch(env)\n",
    "    print(\"ImageToPyTorch       : {}\".format(env.observation_space.shape))\n",
    "    env = ReshapeObservation(env, (84, 84))\n",
    "    print(\"ReshapeObservation   : {}\".format(env.observation_space.shape))\n",
    "    env = FrameStackObservation(env, stack_size=4)\n",
    "    print(\"FrameStackObservation: {}\".format(env.observation_space.shape))\n",
    "    env = ScaledFloatFrame(env)\n",
    "    print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env(ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_env` function applies all the wrappers to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Wrapped Environment ***\n",
      "Environment obs. : (4, 84, 84)\n",
      "Observation shape: (4, 84, 84), type: float32 and range [0.25882354378700256,0.7098039388656616]\n",
      "Observation sample:\n",
      "[[[0.25882354 0.25882354 0.25882354 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  ...\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]]\n",
      "\n",
      " [[0.25882354 0.25882354 0.25882354 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  ...\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]]\n",
      "\n",
      " [[0.25882354 0.25882354 0.25882354 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  ...\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]]\n",
      "\n",
      " [[0.25882354 0.25882354 0.25882354 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  ...\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]]]\n"
     ]
    }
   ],
   "source": [
    "def print_env_info(name, env):\n",
    "    obs, _ = env.reset()\n",
    "    print(\"*** {} Environment ***\".format(name))\n",
    "    print(\"Environment obs. : {}\".format(env.observation_space.shape))\n",
    "    print(\"Observation shape: {}, type: {} and range [{},{}]\".format(obs.shape, obs.dtype, np.min(obs), np.max(obs)))\n",
    "    print(\"Observation sample:\\n{}\".format(obs))\n",
    "\n",
    "print_env_info(\"Wrapped\", env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN5Ps-fUHHeG"
   },
   "source": [
    "## Neural network architecture\n",
    "\n",
    "The following code will implement the NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h6B8v-Qh5Ykk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn        \n",
    "import torch.optim as optim \n",
    "from torchsummary import summary\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvZMDEFBXAMC",
    "outputId": "a1e94895-74cb-4450-b1b9-4e97d4ac47ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipObservation: (210, 160, 3)\n",
      "ResizeObservation    : (84, 84, 3)\n",
      "GrayscaleObservation : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "ReshapeObservation   : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "def make_DQN(input_shape, output_shape):\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64*7*7, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, output_shape)\n",
    "    )\n",
    "    return net\n",
    "\n",
    "test_env = make_env(ENV_NAME)\n",
    "test_net = make_DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_nYIY1AHeqq"
   },
   "source": [
    "## Experience Replay and Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFaMmDKqYmo4"
   },
   "source": [
    "First, we design a class to implement the **experience replay buffer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y79CNYsjY4w0"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, BATCH_SIZE):\n",
    "        indices = np.random.choice(len(self.buffer), BATCH_SIZE, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        \n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARQJsNdbHwgX"
   },
   "source": [
    "## Deep Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhv3Yf-aW7UW"
   },
   "source": [
    "Define the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AGwHC9dyXoPd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "MEAN_REWARD_BOUND = 19.0 \n",
    "NUMBER_OF_REWARDS_TO_AVERAGE = 10          \n",
    "\n",
    "GAMMA = 0.99       \n",
    "\n",
    "BATCH_SIZE = 32  \n",
    "LEARNING_RATE = 1e-4           \n",
    "\n",
    "EXPERIENCE_REPLAY_SIZE = 10000            \n",
    "SYNC_TARGET_NETWORK = 1000     \n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 0.999985\n",
    "EPS_MIN = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQDV04ktY3xs"
   },
   "source": [
    "Define the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YdAKFiMWZw90"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_replay_buffer):\n",
    "        self.env = env\n",
    "        self.exp_replay_buffer = exp_replay_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.current_state = self.env.reset()[0]\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_ = np.array([self.current_state])\n",
    "            state = torch.tensor(state_).to(device)\n",
    "            q_vals = net(state)\n",
    "            _, act_ = torch.max(q_vals, dim=1)\n",
    "            action = int(act_.item())\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        is_done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.current_state, action, reward, is_done, new_state)\n",
    "        self.exp_replay_buffer.append(exp)\n",
    "        self.current_state = new_state\n",
    "        \n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        \n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMQVyeKhCCCM"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BCBQhXLfNeUG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcasasr\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jcasasr/Library/CloudStorage/Dropbox/UAB/GR IA/PML 24-25/M3_Deep RL/M3-2_DQN/Examples/M3-2_Example_1 (DQN on Pong)/wandb/run-20241024_125019-d8dcwy0q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcasasr/M3-2_Example_1a/runs/d8dcwy0q' target=\"_blank\">super-bee-6</a></strong> to <a href='https://wandb.ai/jcasasr/M3-2_Example_1a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcasasr/M3-2_Example_1a' target=\"_blank\">https://wandb.ai/jcasasr/M3-2_Example_1a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcasasr/M3-2_Example_1a/runs/d8dcwy0q' target=\"_blank\">https://wandb.ai/jcasasr/M3-2_Example_1a/runs/d8dcwy0q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcasasr/M3-2_Example_1a/runs/d8dcwy0q?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x176b89220>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# login\n",
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(project=\"M3-2_Example_1a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipurwYpa6iKn",
    "outputId": "e79482cd-312a-47e0-eb34-72836d033356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training starts at  2024-10-24 12:50:20.446644\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\">>> Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgpmAtchZwM_"
   },
   "source": [
    "Main bucle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEoc2PWmM2mu",
    "outputId": "0e5f5940-9f64-4566-a8c8-85fa509c6080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipObservation: (210, 160, 3)\n",
      "ResizeObservation    : (84, 84, 3)\n",
      "GrayscaleObservation : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "ReshapeObservation   : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n",
      "Frame:842 | Total games:1 | Mean reward: -20.000  (epsilon used: 0.99)\n",
      "Frame:1687 | Total games:2 | Mean reward: -20.500  (epsilon used: 0.98)\n",
      "Frame:2599 | Total games:3 | Mean reward: -20.667  (epsilon used: 0.96)\n",
      "Frame:3532 | Total games:4 | Mean reward: -20.750  (epsilon used: 0.95)\n",
      "Frame:4404 | Total games:5 | Mean reward: -20.800  (epsilon used: 0.94)\n",
      "Frame:5275 | Total games:6 | Mean reward: -20.667  (epsilon used: 0.92)\n",
      "Frame:6247 | Total games:7 | Mean reward: -20.714  (epsilon used: 0.91)\n",
      "Frame:7152 | Total games:8 | Mean reward: -20.625  (epsilon used: 0.90)\n",
      "Frame:7976 | Total games:9 | Mean reward: -20.667  (epsilon used: 0.89)\n",
      "Frame:8796 | Total games:10 | Mean reward: -20.700  (epsilon used: 0.88)\n",
      "Frame:9651 | Total games:11 | Mean reward: -20.800  (epsilon used: 0.87)\n",
      "Frame:10499 | Total games:12 | Mean reward: -20.800  (epsilon used: 0.85)\n",
      "Frame:11369 | Total games:13 | Mean reward: -20.700  (epsilon used: 0.84)\n",
      "Frame:12268 | Total games:14 | Mean reward: -20.600  (epsilon used: 0.83)\n",
      "Frame:13032 | Total games:15 | Mean reward: -20.600  (epsilon used: 0.82)\n",
      "Frame:14008 | Total games:16 | Mean reward: -20.600  (epsilon used: 0.81)\n",
      "Frame:14800 | Total games:17 | Mean reward: -20.600  (epsilon used: 0.80)\n",
      "Frame:15666 | Total games:18 | Mean reward: -20.600  (epsilon used: 0.79)\n",
      "Frame:16478 | Total games:19 | Mean reward: -20.600  (epsilon used: 0.78)\n",
      "Frame:17321 | Total games:20 | Mean reward: -20.600  (epsilon used: 0.77)\n",
      "Frame:18423 | Total games:21 | Mean reward: -20.400  (epsilon used: 0.76)\n",
      "Frame:19326 | Total games:22 | Mean reward: -20.300  (epsilon used: 0.75)\n",
      "Frame:20152 | Total games:23 | Mean reward: -20.400  (epsilon used: 0.74)\n",
      "Frame:21064 | Total games:24 | Mean reward: -20.500  (epsilon used: 0.73)\n",
      "Frame:21828 | Total games:25 | Mean reward: -20.500  (epsilon used: 0.72)\n",
      "Frame:22620 | Total games:26 | Mean reward: -20.600  (epsilon used: 0.71)\n",
      "Frame:23767 | Total games:27 | Mean reward: -20.400  (epsilon used: 0.70)\n",
      "Frame:24665 | Total games:28 | Mean reward: -20.400  (epsilon used: 0.69)\n",
      "Frame:25573 | Total games:29 | Mean reward: -20.400  (epsilon used: 0.68)\n",
      "Frame:26485 | Total games:30 | Mean reward: -20.400  (epsilon used: 0.67)\n",
      "Frame:27337 | Total games:31 | Mean reward: -20.600  (epsilon used: 0.66)\n",
      "Frame:28424 | Total games:32 | Mean reward: -20.500  (epsilon used: 0.65)\n",
      "Frame:29218 | Total games:33 | Mean reward: -20.500  (epsilon used: 0.65)\n",
      "Frame:30259 | Total games:34 | Mean reward: -20.400  (epsilon used: 0.64)\n",
      "Frame:31249 | Total games:35 | Mean reward: -20.300  (epsilon used: 0.63)\n",
      "Frame:32297 | Total games:36 | Mean reward: -20.200  (epsilon used: 0.62)\n",
      "Frame:33280 | Total games:37 | Mean reward: -20.300  (epsilon used: 0.61)\n",
      "Frame:34280 | Total games:38 | Mean reward: -20.400  (epsilon used: 0.60)\n",
      "Frame:35397 | Total games:39 | Mean reward: -20.200  (epsilon used: 0.59)\n",
      "Frame:36685 | Total games:40 | Mean reward: -20.000  (epsilon used: 0.58)\n",
      "Frame:37663 | Total games:41 | Mean reward: -19.900  (epsilon used: 0.57)\n",
      "Frame:38789 | Total games:42 | Mean reward: -20.000  (epsilon used: 0.56)\n",
      "Frame:39799 | Total games:43 | Mean reward: -19.900  (epsilon used: 0.55)\n",
      "Frame:40937 | Total games:44 | Mean reward: -19.900  (epsilon used: 0.54)\n",
      "Frame:42218 | Total games:45 | Mean reward: -19.700  (epsilon used: 0.53)\n",
      "Frame:43343 | Total games:46 | Mean reward: -19.800  (epsilon used: 0.52)\n",
      "Frame:44824 | Total games:47 | Mean reward: -19.700  (epsilon used: 0.51)\n",
      "Frame:46308 | Total games:48 | Mean reward: -19.300  (epsilon used: 0.50)\n",
      "Frame:47466 | Total games:49 | Mean reward: -19.300  (epsilon used: 0.49)\n",
      "Frame:48795 | Total games:50 | Mean reward: -19.300  (epsilon used: 0.48)\n",
      "Frame:50102 | Total games:51 | Mean reward: -19.100  (epsilon used: 0.47)\n",
      "Frame:51615 | Total games:52 | Mean reward: -18.900  (epsilon used: 0.46)\n",
      "Frame:53232 | Total games:53 | Mean reward: -18.700  (epsilon used: 0.45)\n",
      "Frame:54380 | Total games:54 | Mean reward: -18.700  (epsilon used: 0.44)\n",
      "Frame:55978 | Total games:55 | Mean reward: -18.700  (epsilon used: 0.43)\n",
      "Frame:57713 | Total games:56 | Mean reward: -18.200  (epsilon used: 0.42)\n",
      "Frame:58778 | Total games:57 | Mean reward: -18.200  (epsilon used: 0.41)\n",
      "Frame:60236 | Total games:58 | Mean reward: -18.200  (epsilon used: 0.41)\n",
      "Frame:61729 | Total games:59 | Mean reward: -18.200  (epsilon used: 0.40)\n",
      "Frame:63092 | Total games:60 | Mean reward: -18.300  (epsilon used: 0.39)\n",
      "Frame:64698 | Total games:61 | Mean reward: -18.400  (epsilon used: 0.38)\n",
      "Frame:66211 | Total games:62 | Mean reward: -18.300  (epsilon used: 0.37)\n",
      "Frame:67766 | Total games:63 | Mean reward: -18.100  (epsilon used: 0.36)\n",
      "Frame:69378 | Total games:64 | Mean reward: -18.000  (epsilon used: 0.35)\n",
      "Frame:71025 | Total games:65 | Mean reward: -17.800  (epsilon used: 0.34)\n",
      "Frame:72588 | Total games:66 | Mean reward: -17.900  (epsilon used: 0.34)\n",
      "Frame:74331 | Total games:67 | Mean reward: -17.700  (epsilon used: 0.33)\n",
      "Frame:75848 | Total games:68 | Mean reward: -17.800  (epsilon used: 0.32)\n",
      "Frame:77703 | Total games:69 | Mean reward: -17.500  (epsilon used: 0.31)\n",
      "Frame:79646 | Total games:70 | Mean reward: -17.300  (epsilon used: 0.30)\n",
      "Frame:81587 | Total games:71 | Mean reward: -17.000  (epsilon used: 0.29)\n",
      "Frame:83249 | Total games:72 | Mean reward: -16.700  (epsilon used: 0.29)\n",
      "Frame:84700 | Total games:73 | Mean reward: -17.100  (epsilon used: 0.28)\n",
      "Frame:86131 | Total games:74 | Mean reward: -17.100  (epsilon used: 0.27)\n",
      "Frame:87798 | Total games:75 | Mean reward: -17.300  (epsilon used: 0.27)\n",
      "Frame:89624 | Total games:76 | Mean reward: -17.100  (epsilon used: 0.26)\n",
      "Frame:91704 | Total games:77 | Mean reward: -16.900  (epsilon used: 0.25)\n",
      "Frame:93406 | Total games:78 | Mean reward: -16.700  (epsilon used: 0.25)\n",
      "Frame:95161 | Total games:79 | Mean reward: -16.900  (epsilon used: 0.24)\n",
      "Frame:97288 | Total games:80 | Mean reward: -16.300  (epsilon used: 0.23)\n",
      "Frame:99084 | Total games:81 | Mean reward: -16.600  (epsilon used: 0.23)\n",
      "Frame:100935 | Total games:82 | Mean reward: -16.700  (epsilon used: 0.22)\n",
      "Frame:102865 | Total games:83 | Mean reward: -16.300  (epsilon used: 0.21)\n",
      "Frame:104792 | Total games:84 | Mean reward: -15.600  (epsilon used: 0.21)\n",
      "Frame:106902 | Total games:85 | Mean reward: -15.500  (epsilon used: 0.20)\n",
      "Frame:108482 | Total games:86 | Mean reward: -15.800  (epsilon used: 0.20)\n",
      "Frame:110663 | Total games:87 | Mean reward: -15.800  (epsilon used: 0.19)\n",
      "Frame:112711 | Total games:88 | Mean reward: -15.700  (epsilon used: 0.18)\n",
      "Frame:114487 | Total games:89 | Mean reward: -15.800  (epsilon used: 0.18)\n",
      "Frame:116202 | Total games:90 | Mean reward: -16.600  (epsilon used: 0.17)\n",
      "Frame:118039 | Total games:91 | Mean reward: -16.500  (epsilon used: 0.17)\n",
      "Frame:120876 | Total games:92 | Mean reward: -15.700  (epsilon used: 0.16)\n",
      "Frame:123025 | Total games:93 | Mean reward: -15.500  (epsilon used: 0.16)\n",
      "Frame:124907 | Total games:94 | Mean reward: -16.200  (epsilon used: 0.15)\n",
      "Frame:127013 | Total games:95 | Mean reward: -16.000  (epsilon used: 0.15)\n",
      "Frame:129035 | Total games:96 | Mean reward: -15.700  (epsilon used: 0.14)\n",
      "Frame:130649 | Total games:97 | Mean reward: -16.000  (epsilon used: 0.14)\n",
      "Frame:132724 | Total games:98 | Mean reward: -16.100  (epsilon used: 0.14)\n",
      "Frame:134704 | Total games:99 | Mean reward: -15.800  (epsilon used: 0.13)\n",
      "Frame:136435 | Total games:100 | Mean reward: -15.500  (epsilon used: 0.13)\n",
      "Frame:139007 | Total games:101 | Mean reward: -15.100  (epsilon used: 0.12)\n",
      "Frame:140923 | Total games:102 | Mean reward: -16.100  (epsilon used: 0.12)\n",
      "Frame:143389 | Total games:103 | Mean reward: -16.100  (epsilon used: 0.12)\n",
      "Frame:145577 | Total games:104 | Mean reward: -15.700  (epsilon used: 0.11)\n",
      "Frame:147773 | Total games:105 | Mean reward: -15.700  (epsilon used: 0.11)\n",
      "Frame:150228 | Total games:106 | Mean reward: -15.400  (epsilon used: 0.11)\n",
      "Frame:152325 | Total games:107 | Mean reward: -15.200  (epsilon used: 0.10)\n",
      "Frame:155119 | Total games:108 | Mean reward: -14.600  (epsilon used: 0.10)\n",
      "Frame:157815 | Total games:109 | Mean reward: -14.200  (epsilon used: 0.09)\n",
      "Frame:160372 | Total games:110 | Mean reward: -13.700  (epsilon used: 0.09)\n",
      "Frame:162942 | Total games:111 | Mean reward: -13.800  (epsilon used: 0.09)\n",
      "Frame:165294 | Total games:112 | Mean reward: -13.300  (epsilon used: 0.08)\n",
      "Frame:168236 | Total games:113 | Mean reward: -12.600  (epsilon used: 0.08)\n",
      "Frame:171128 | Total games:114 | Mean reward: -12.000  (epsilon used: 0.08)\n",
      "Frame:172460 | Total games:115 | Mean reward: -12.300  (epsilon used: 0.08)\n",
      "Frame:174941 | Total games:116 | Mean reward: -12.500  (epsilon used: 0.07)\n",
      "Frame:177648 | Total games:117 | Mean reward: -11.900  (epsilon used: 0.07)\n",
      "Frame:180543 | Total games:118 | Mean reward: -11.700  (epsilon used: 0.07)\n",
      "Frame:183240 | Total games:119 | Mean reward: -11.500  (epsilon used: 0.06)\n",
      "Frame:185525 | Total games:120 | Mean reward: -11.700  (epsilon used: 0.06)\n",
      "Frame:187573 | Total games:121 | Mean reward: -11.700  (epsilon used: 0.06)\n",
      "Frame:191037 | Total games:122 | Mean reward: -10.900  (epsilon used: 0.06)\n",
      "Frame:194463 | Total games:123 | Mean reward: -11.000  (epsilon used: 0.05)\n",
      "Frame:197120 | Total games:124 | Mean reward: -11.000  (epsilon used: 0.05)\n",
      "Frame:199965 | Total games:125 | Mean reward: -9.900  (epsilon used: 0.05)\n",
      "Frame:202609 | Total games:126 | Mean reward: -9.300  (epsilon used: 0.05)\n",
      "Frame:206196 | Total games:127 | Mean reward: -8.400  (epsilon used: 0.05)\n",
      "Frame:209525 | Total games:128 | Mean reward: -7.300  (epsilon used: 0.04)\n",
      "Frame:212518 | Total games:129 | Mean reward: -6.800  (epsilon used: 0.04)\n",
      "Frame:215051 | Total games:130 | Mean reward: -4.300  (epsilon used: 0.04)\n",
      "Frame:218544 | Total games:131 | Mean reward: -2.600  (epsilon used: 0.04)\n",
      "Frame:221214 | Total games:132 | Mean reward: -1.300  (epsilon used: 0.04)\n",
      "Frame:223990 | Total games:133 | Mean reward: 0.500  (epsilon used: 0.03)\n",
      "Frame:226387 | Total games:134 | Mean reward: 2.800  (epsilon used: 0.03)\n",
      "Frame:229120 | Total games:135 | Mean reward: 4.500  (epsilon used: 0.03)\n",
      "Frame:231981 | Total games:136 | Mean reward: 6.400  (epsilon used: 0.03)\n",
      "Frame:234231 | Total games:137 | Mean reward: 8.000  (epsilon used: 0.03)\n",
      "Frame:236924 | Total games:138 | Mean reward: 8.900  (epsilon used: 0.03)\n",
      "Frame:239307 | Total games:139 | Mean reward: 11.000  (epsilon used: 0.03)\n",
      "Frame:241306 | Total games:140 | Mean reward: 11.400  (epsilon used: 0.03)\n",
      "Frame:244248 | Total games:141 | Mean reward: 11.700  (epsilon used: 0.03)\n",
      "Frame:246073 | Total games:142 | Mean reward: 12.700  (epsilon used: 0.02)\n",
      "Frame:248448 | Total games:143 | Mean reward: 13.000  (epsilon used: 0.02)\n",
      "Frame:251012 | Total games:144 | Mean reward: 12.700  (epsilon used: 0.02)\n",
      "Frame:253002 | Total games:145 | Mean reward: 13.400  (epsilon used: 0.02)\n",
      "Frame:254917 | Total games:146 | Mean reward: 14.000  (epsilon used: 0.02)\n",
      "Frame:257159 | Total games:147 | Mean reward: 13.700  (epsilon used: 0.02)\n",
      "Frame:259204 | Total games:148 | Mean reward: 14.300  (epsilon used: 0.02)\n",
      "Frame:260938 | Total games:149 | Mean reward: 14.500  (epsilon used: 0.02)\n",
      "Frame:263114 | Total games:150 | Mean reward: 14.700  (epsilon used: 0.02)\n",
      "Frame:265258 | Total games:151 | Mean reward: 15.600  (epsilon used: 0.02)\n",
      "Frame:267032 | Total games:152 | Mean reward: 15.800  (epsilon used: 0.02)\n",
      "Frame:268878 | Total games:153 | Mean reward: 16.300  (epsilon used: 0.02)\n",
      "Frame:270515 | Total games:154 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:272466 | Total games:155 | Mean reward: 17.100  (epsilon used: 0.02)\n",
      "Frame:274929 | Total games:156 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:277067 | Total games:157 | Mean reward: 15.500  (epsilon used: 0.02)\n",
      "Frame:279064 | Total games:158 | Mean reward: 15.400  (epsilon used: 0.02)\n",
      "Frame:281134 | Total games:159 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:283164 | Total games:160 | Mean reward: 15.200  (epsilon used: 0.02)\n",
      "Frame:285385 | Total games:161 | Mean reward: 15.100  (epsilon used: 0.02)\n",
      "Frame:287117 | Total games:162 | Mean reward: 15.000  (epsilon used: 0.02)\n",
      "Frame:288814 | Total games:163 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:290482 | Total games:164 | Mean reward: 15.200  (epsilon used: 0.02)\n",
      "Frame:292247 | Total games:165 | Mean reward: 15.500  (epsilon used: 0.02)\n",
      "Frame:294235 | Total games:166 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:296166 | Total games:167 | Mean reward: 17.000  (epsilon used: 0.02)\n",
      "Frame:297864 | Total games:168 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:299501 | Total games:169 | Mean reward: 17.700  (epsilon used: 0.02)\n",
      "Frame:301354 | Total games:170 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:303906 | Total games:171 | Mean reward: 17.500  (epsilon used: 0.02)\n",
      "Frame:306340 | Total games:172 | Mean reward: 16.700  (epsilon used: 0.02)\n",
      "Frame:308036 | Total games:173 | Mean reward: 16.700  (epsilon used: 0.02)\n",
      "Frame:309690 | Total games:174 | Mean reward: 16.800  (epsilon used: 0.02)\n",
      "Frame:311425 | Total games:175 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:313260 | Total games:176 | Mean reward: 17.400  (epsilon used: 0.02)\n",
      "Frame:315147 | Total games:177 | Mean reward: 17.700  (epsilon used: 0.02)\n",
      "Frame:317079 | Total games:178 | Mean reward: 17.400  (epsilon used: 0.02)\n",
      "Frame:318962 | Total games:179 | Mean reward: 17.100  (epsilon used: 0.02)\n",
      "Frame:320632 | Total games:180 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:322731 | Total games:181 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:325009 | Total games:182 | Mean reward: 18.200  (epsilon used: 0.02)\n",
      "Frame:326779 | Total games:183 | Mean reward: 18.000  (epsilon used: 0.02)\n",
      "Frame:328416 | Total games:184 | Mean reward: 18.000  (epsilon used: 0.02)\n",
      "Frame:330165 | Total games:185 | Mean reward: 18.000  (epsilon used: 0.02)\n",
      "Frame:332253 | Total games:186 | Mean reward: 17.700  (epsilon used: 0.02)\n",
      "Frame:334237 | Total games:187 | Mean reward: 17.200  (epsilon used: 0.02)\n",
      "Frame:336931 | Total games:188 | Mean reward: 15.400  (epsilon used: 0.02)\n",
      "Frame:338831 | Total games:189 | Mean reward: 15.200  (epsilon used: 0.02)\n",
      "Frame:340847 | Total games:190 | Mean reward: 14.700  (epsilon used: 0.02)\n",
      "Frame:342545 | Total games:191 | Mean reward: 15.300  (epsilon used: 0.02)\n",
      "Frame:344496 | Total games:192 | Mean reward: 15.600  (epsilon used: 0.02)\n",
      "Frame:346193 | Total games:193 | Mean reward: 15.800  (epsilon used: 0.02)\n",
      "Frame:348179 | Total games:194 | Mean reward: 15.200  (epsilon used: 0.02)\n",
      "Frame:350008 | Total games:195 | Mean reward: 15.200  (epsilon used: 0.02)\n",
      "Frame:352321 | Total games:196 | Mean reward: 14.900  (epsilon used: 0.02)\n",
      "Frame:354572 | Total games:197 | Mean reward: 14.900  (epsilon used: 0.02)\n",
      "Frame:356487 | Total games:198 | Mean reward: 16.700  (epsilon used: 0.02)\n",
      "Frame:358670 | Total games:199 | Mean reward: 16.600  (epsilon used: 0.02)\n",
      "Frame:360619 | Total games:200 | Mean reward: 16.800  (epsilon used: 0.02)\n",
      "Frame:362791 | Total games:201 | Mean reward: 16.000  (epsilon used: 0.02)\n",
      "Frame:364458 | Total games:202 | Mean reward: 16.200  (epsilon used: 0.02)\n",
      "Frame:366215 | Total games:203 | Mean reward: 16.100  (epsilon used: 0.02)\n",
      "Frame:367912 | Total games:204 | Mean reward: 16.700  (epsilon used: 0.02)\n",
      "Frame:369960 | Total games:205 | Mean reward: 16.400  (epsilon used: 0.02)\n",
      "Frame:371687 | Total games:206 | Mean reward: 17.100  (epsilon used: 0.02)\n",
      "Frame:373385 | Total games:207 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:375438 | Total games:208 | Mean reward: 17.700  (epsilon used: 0.02)\n",
      "Frame:377334 | Total games:209 | Mean reward: 18.100  (epsilon used: 0.02)\n",
      "Frame:379508 | Total games:210 | Mean reward: 17.800  (epsilon used: 0.02)\n",
      "Frame:381477 | Total games:211 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:383427 | Total games:212 | Mean reward: 17.600  (epsilon used: 0.02)\n",
      "Frame:385318 | Total games:213 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:387705 | Total games:214 | Mean reward: 16.500  (epsilon used: 0.02)\n",
      "Frame:389402 | Total games:215 | Mean reward: 17.000  (epsilon used: 0.02)\n",
      "Frame:391069 | Total games:216 | Mean reward: 17.100  (epsilon used: 0.02)\n",
      "Frame:392884 | Total games:217 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:394643 | Total games:218 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:396310 | Total games:219 | Mean reward: 17.400  (epsilon used: 0.02)\n",
      "Frame:398560 | Total games:220 | Mean reward: 17.700  (epsilon used: 0.02)\n",
      "Frame:401149 | Total games:221 | Mean reward: 17.100  (epsilon used: 0.02)\n",
      "Frame:403001 | Total games:222 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:404929 | Total games:223 | Mean reward: 17.200  (epsilon used: 0.02)\n",
      "Frame:406722 | Total games:224 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:408751 | Total games:225 | Mean reward: 17.500  (epsilon used: 0.02)\n",
      "Frame:410517 | Total games:226 | Mean reward: 17.500  (epsilon used: 0.02)\n",
      "Frame:412721 | Total games:227 | Mean reward: 17.000  (epsilon used: 0.02)\n",
      "Frame:414384 | Total games:228 | Mean reward: 17.100  (epsilon used: 0.02)\n",
      "Frame:416260 | Total games:229 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:418142 | Total games:230 | Mean reward: 17.000  (epsilon used: 0.02)\n",
      "Frame:420131 | Total games:231 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:422142 | Total games:232 | Mean reward: 17.500  (epsilon used: 0.02)\n",
      "Frame:424913 | Total games:233 | Mean reward: 17.000  (epsilon used: 0.02)\n",
      "Frame:426677 | Total games:234 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:428432 | Total games:235 | Mean reward: 17.200  (epsilon used: 0.02)\n",
      "Frame:430866 | Total games:236 | Mean reward: 16.300  (epsilon used: 0.02)\n",
      "Frame:432616 | Total games:237 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:434263 | Total games:238 | Mean reward: 17.000  (epsilon used: 0.02)\n",
      "Frame:436268 | Total games:239 | Mean reward: 16.800  (epsilon used: 0.02)\n",
      "Frame:437920 | Total games:240 | Mean reward: 17.100  (epsilon used: 0.02)\n",
      "Frame:439999 | Total games:241 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:442045 | Total games:242 | Mean reward: 16.800  (epsilon used: 0.02)\n",
      "Frame:443842 | Total games:243 | Mean reward: 17.700  (epsilon used: 0.02)\n",
      "Frame:445923 | Total games:244 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:448087 | Total games:245 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:449834 | Total games:246 | Mean reward: 17.800  (epsilon used: 0.02)\n",
      "Frame:451553 | Total games:247 | Mean reward: 17.800  (epsilon used: 0.02)\n",
      "Frame:453754 | Total games:248 | Mean reward: 16.600  (epsilon used: 0.02)\n",
      "Frame:455481 | Total games:249 | Mean reward: 17.000  (epsilon used: 0.02)\n",
      "Frame:457550 | Total games:250 | Mean reward: 16.500  (epsilon used: 0.02)\n",
      "Frame:459362 | Total games:251 | Mean reward: 16.900  (epsilon used: 0.02)\n",
      "Frame:461192 | Total games:252 | Mean reward: 17.400  (epsilon used: 0.02)\n",
      "Frame:463111 | Total games:253 | Mean reward: 17.300  (epsilon used: 0.02)\n",
      "Frame:464808 | Total games:254 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:466867 | Total games:255 | Mean reward: 17.900  (epsilon used: 0.02)\n",
      "Frame:468521 | Total games:256 | Mean reward: 18.000  (epsilon used: 0.02)\n",
      "Frame:470355 | Total games:257 | Mean reward: 17.800  (epsilon used: 0.02)\n",
      "Frame:472279 | Total games:258 | Mean reward: 18.800  (epsilon used: 0.02)\n",
      "Frame:474013 | Total games:259 | Mean reward: 18.800  (epsilon used: 0.02)\n",
      "Frame:475879 | Total games:260 | Mean reward: 19.000  (epsilon used: 0.02)\n",
      "Frame:477545 | Total games:261 | Mean reward: 19.100  (epsilon used: 0.02)\n",
      "SOLVED in 477545 frames and 261 games\n"
     ]
    }
   ],
   "source": [
    "env = make_env(ENV_NAME)\n",
    "\n",
    "net = make_DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = make_DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    " \n",
    "buffer = ExperienceReplay(EXPERIENCE_REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = EPS_START\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_number = 0  \n",
    "\n",
    "while True:\n",
    "    frame_number += 1\n",
    "    epsilon = max(epsilon * EPS_DECAY, EPS_MIN)\n",
    "\n",
    "    reward = agent.step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "\n",
    "        mean_reward = np.mean(total_rewards[-NUMBER_OF_REWARDS_TO_AVERAGE:])\n",
    "        print(f\"Frame:{frame_number} | Total games:{len(total_rewards)} | Mean reward: {mean_reward:.3f}  (epsilon used: {epsilon:.2f})\")\n",
    "        wandb.log({\"epsilon\": epsilon, \"reward_100\": mean_reward, \"reward\": reward}, step=frame_number)\n",
    "\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(f\"SOLVED in {frame_number} frames and {len(total_rewards)} games\")\n",
    "            break\n",
    "\n",
    "    if len(buffer) < EXPERIENCE_REPLAY_SIZE:\n",
    "        continue\n",
    "\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    states_, actions_, rewards_, dones_, next_states_ = batch\n",
    "\n",
    "    states = torch.tensor(states_).to(device)\n",
    "    next_states = torch.tensor(next_states_).to(device)\n",
    "    actions = torch.tensor(actions_).to(device)\n",
    "    rewards = torch.tensor(rewards_).to(device)\n",
    "    dones = torch.BoolTensor(dones_).to(device)\n",
    "\n",
    "    Q_values = net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    next_state_values = target_net(next_states).max(1)[0]\n",
    "    next_state_values[dones] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_Q_values = next_state_values * GAMMA + rewards\n",
    "    loss = nn.MSELoss()(Q_values, expected_Q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if frame_number % SYNC_TARGET_NETWORK == 0:\n",
    "        target_net.load_state_dict(net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), ENV_NAME + \".dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZPkszw66cmO",
    "outputId": "8fe2e7e1-0610-4ce7-ae35-e694b9bc0f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training ends at  2024-10-24 21:55:47.891256\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>█▇▇▇▇▆▅▅▅▄▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>▁▁▁▁▁▁▁▁▂▁▃▁▂▃▃▁▂▃▅▇▇███▄█▇██▇███▆█▇█▇██</td></tr><tr><td>reward_100</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▆███████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.02</td></tr><tr><td>reward</td><td>20</td></tr><tr><td>reward_100</td><td>19.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-bee-6</strong> at: <a href='https://wandb.ai/jcasasr/M3-2_Example_1a/runs/d8dcwy0q' target=\"_blank\">https://wandb.ai/jcasasr/M3-2_Example_1a/runs/d8dcwy0q</a><br/> View project at: <a href='https://wandb.ai/jcasasr/M3-2_Example_1a' target=\"_blank\">https://wandb.ai/jcasasr/M3-2_Example_1a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241024_125019-d8dcwy0q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Capitulo09.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
