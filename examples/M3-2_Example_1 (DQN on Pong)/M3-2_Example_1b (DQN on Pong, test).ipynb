{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Yb47zJQglm"
   },
   "source": [
    "# **Deep Reinforcement Learning**\n",
    "\n",
    "# M3-2 Deep Q-Networks\n",
    "\n",
    "## Example of DQN implementation on Pong environment (Part 2, testing)\n",
    "\n",
    "Below we will see a simple example that will allow us to understand the concepts introduced in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pong environment\n",
    "\n",
    "The [Pong](https://ale.farama.org/environments/pong/) environment is part of the Arcade Learning Environment environments. The [Arcade Learning Environment]((https://ale.farama.org/environments/)) (ALE), commonly referred to as Atari, is a framework that allows researchers and hobbyists to develop AI agents for Atari 2600 roms. \n",
    "\n",
    "Please read that page first for general information.\n",
    "\n",
    "You control the right paddle, you compete against the left paddle controlled by the computer. You each try to keep deflecting the ball away from your goal and into your opponentâ€™s goal.\n",
    "\n",
    "<center><img src=\"https://ale.farama.org/_images/pong.gif\"/></center>\n",
    "\n",
    "For a more detailed documentation, see the [AtariAge page](https://atariage.com/manual_html_page.php?SoftwareLabelID=587)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maCHXYw1G8Km"
   },
   "source": [
    "First, we will load the environment. It's important to note that we are specifically using **version 1.0.0** of the **Gymnasium** library.\n",
    "\n",
    "To install this version of the environment, run the following command:\n",
    "> pip install gymnasium==1.0.0\n",
    "\n",
    "This will also install all the related packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QIIk_u6f0MT",
    "outputId": "761efe8d-8987-4bbb-f192-cb13cb95b609",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dependencies are installed, we load them and initialize the `PongNoFrameskip-v4` environment.\n",
    "\n",
    "There are several Pong environments, with minor differences among them. See [Pong](https://ale.farama.org/environments/pong/) page for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FA1Y5VCv20XZ",
    "outputId": "35a6fbb0-5d3e-4941-e0c3-1f1a6674b0bf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gymnasium version 1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+6a7e0ae)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "# version\n",
    "print(\"Using Gymnasium version {}\".format(gym.__version__))\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "test_env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnXgUWhMf0MV"
   },
   "source": [
    "### Data preprocessing (Wrappers)\n",
    "\n",
    "We need to apply the **same set of wrappers** used during the model's training phase to ensure that the inputs to the model are consistent in shape, format, and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nPi1lHINMuSu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipObservation: (210, 160, 3)\n",
      "ResizeObservation    : (84, 84, 3)\n",
      "GrayscaleObservation : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "ReshapeObservation   : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import MaxAndSkipObservation, ResizeObservation, GrayscaleObservation, FrameStackObservation, ReshapeObservation\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gymnasium.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def make_env(env_name, render_mode=None):\n",
    "    env = gym.make(env_name, render_mode=render_mode)\n",
    "    print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "    env = MaxAndSkipObservation(env, skip=4)\n",
    "    print(\"MaxAndSkipObservation: {}\".format(env.observation_space.shape))\n",
    "    #env = FireResetEnv(env)\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n",
    "    env = GrayscaleObservation(env, keep_dim=True)\n",
    "    print(\"GrayscaleObservation : {}\".format(env.observation_space.shape))\n",
    "    env = ImageToPyTorch(env)\n",
    "    print(\"ImageToPyTorch       : {}\".format(env.observation_space.shape))\n",
    "    env = ReshapeObservation(env, (84, 84))\n",
    "    print(\"ReshapeObservation   : {}\".format(env.observation_space.shape))\n",
    "    env = FrameStackObservation(env, stack_size=4)\n",
    "    print(\"FrameStackObservation: {}\".format(env.observation_space.shape))\n",
    "    env = ScaledFloatFrame(env)\n",
    "    print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Wrapped Environment ***\n",
      "Environment obs. : (4, 84, 84)\n",
      "Observation shape: (4, 84, 84), type: float32 and range [0.25882354378700256,0.7098039388656616]\n",
      "Observation sample:\n",
      "[[[0.25882354 0.25882354 0.25882354 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  ...\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]]\n",
      "\n",
      " [[0.25882354 0.25882354 0.25882354 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  ...\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]]\n",
      "\n",
      " [[0.25882354 0.25882354 0.25882354 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  ...\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]]\n",
      "\n",
      " [[0.25882354 0.25882354 0.25882354 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  [0.43137255 0.43137255 0.43137255 ... 0.43137255 0.43137255 0.43137255]\n",
      "  ...\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]\n",
      "  [0.3137255  0.3137255  0.3137255  ... 0.3137255  0.3137255  0.3137255 ]]]\n"
     ]
    }
   ],
   "source": [
    "def print_env_info(name, env):\n",
    "    obs, _ = env.reset()\n",
    "    print(\"*** {} Environment ***\".format(name))\n",
    "    print(\"Environment obs. : {}\".format(env.observation_space.shape))\n",
    "    print(\"Observation shape: {}, type: {} and range [{},{}]\".format(obs.shape, obs.dtype, np.min(obs), np.max(obs)))\n",
    "    print(\"Observation sample:\\n{}\".format(obs))\n",
    "\n",
    "print_env_info(\"Wrapped\", env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN5Ps-fUHHeG"
   },
   "source": [
    "### Neural network architecture\n",
    "\n",
    "The following code will implement the NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6B8v-Qh5Ykk",
    "outputId": "f75efde2-e5a9-43ce-f3d7-98257ac93051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IvZMDEFBXAMC"
   },
   "outputs": [],
   "source": [
    "def make_DQN(input_shape, output_shape):\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64*7*7, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, output_shape)\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model, saved on previous notebook (`M3-2_Example_1a (DQN on Pong, train)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipObservation: (210, 160, 3)\n",
      "ResizeObservation    : (84, 84, 3)\n",
      "GrayscaleObservation : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "ReshapeObservation   : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "model = ENV_NAME + \".dat\"\n",
    "\n",
    "env = make_env(ENV_NAME, render_mode=\"rgb_array\")\n",
    "net = make_DQN(env.observation_space.shape, env.action_space.n)\n",
    "net.load_state_dict(torch.load(model, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p0jvxoC3m5W"
   },
   "source": [
    "### Test\n",
    "\n",
    "We play one episodes and save them (.gif or .mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvN4S8R53mJI",
    "outputId": "0cea21eb-9cca-4ed5-821a-97e3e53b11c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 21.00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "from PIL import Image\n",
    "\n",
    "# params\n",
    "visualize = True\n",
    "images = []\n",
    "\n",
    "state, _ = env.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "while True:\n",
    "    start_ts = time.time()\n",
    "    if visualize:\n",
    "        img = env.render()\n",
    "        images.append(Image.fromarray(img))\n",
    "\n",
    "    state_ = torch.tensor(np.array([state], copy=False))\n",
    "    q_vals = net(state_).data.numpy()[0]\n",
    "    action = np.argmax(q_vals)\n",
    "\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Total reward: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the episode to GIF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode export to video.gif\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "gif_file = \"video.gif\"\n",
    "\n",
    "# duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "images[0].save(gif_file, save_all=True, append_images=images[1:], duration=60, loop=0)\n",
    "\n",
    "print(\"Episode export to '{}'\".format(gif_file))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
