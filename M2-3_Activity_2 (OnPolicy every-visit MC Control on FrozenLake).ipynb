{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tabular Reinforcement Learning**\n",
    "\n",
    "# Monte Carlo methods on FrozenLake environment\n",
    "\n",
    "## Non-Evaluables Practical Exercices\n",
    "\n",
    "This is a non-evaluable practical exercise, but it is recommended that students complete it fully and individually, since it is an important part of the learning process.\n",
    "\n",
    "The solution will be available, although it is not recommended that students consult the solution until they have completed the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FrozenLake environment\n",
    "\n",
    "In this activity, we are going to implement the **Value Iteration** algorithm on [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment.\n",
    "\n",
    "Main characteristics:\n",
    "- The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "- Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
    "- The player makes moves until they reach the goal or fall in a hole.\n",
    "- The lake is slippery (unless disabled) so the player may move perpendicular to the intended direction sometimes (see _is_slippery_ param).\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo methods\n",
    "\n",
    "In this section, we implement an estimation of the **optimal policy** using **Monte Carlo methods**, specifically we will study the **_On-policy every-visit MC control algorithm (for $\\epsilon$-soft policies)_**.\n",
    "\n",
    "<u>Question 1</u>: : **Implement the algorithm** using the following parameters:\n",
    "\n",
    "- Number of episodes = 100,000\n",
    "- Discount factor = 1\n",
    "\n",
    "<u>Question 2</u>: : **Implement the epsilon decay factor** using the following equation and parameters:\n",
    "\n",
    "- Initial epsilon = 1\n",
    "- Epsilon decay factor (*epsilon decay*) = 0.999\n",
    "- Update epsilon according to: $\\textrm{max}(\\epsilon Â· \\epsilon_{\\textrm{decay}}, 0.01)$\n",
    "\n",
    "<u>Question 3</u>: Once you have coded the algorithm, try different **values for the hyperparameters** and comment the best ones (providing an empirical comparison):\n",
    "\n",
    "- Number of episodes\n",
    "- Initial epsilon\n",
    "- Epsilon decay factor (*epsilon decay*)\n",
    "- *discount factor* \n",
    "\n",
    "<u>Question 4</u>: Try to solve the same environment but using a _8 x 8_ grid (also in slippery mode):\n",
    "\n",
    "> gym.make(ENV_NAME, desc=None, map_name=\"8x8\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "h5y5sIqBPPxr",
    "outputId": "853c4fb1-0806-4393-f51c-a9098d9eddc5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(4) \n",
      "Observation space is Discrete(16) \n",
      "Reward range is (0, 1) \n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# params\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "# definig the environment\n",
    "env = gym.make(ENV_NAME, desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))\n",
    "print(\"Reward range is {} \".format(env.unwrapped.reward_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, num_Actions):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a Q and epsilon action value function\n",
    "    \n",
    "    Args:\n",
    "         Q: A dictionary whose correspondence is state -> action-values.\n",
    "            Each value is a numpy array of length num_Actions (see below)\n",
    "         epsilon: The probability of selecting a random action (float between 0 and 1).\n",
    "         num_Actions: Number of actions in the environment (in the case of WIndyGridWorld, it is 4)\n",
    "    \n",
    "    Returns:\n",
    "         A function that takes the observation as an argument and returns as a result\n",
    "         the probabilities of each action as a numpy array of length num_Actions.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "def mc_control_on_policy_epsilon_greedy(env, num_episodes, discount=1.0, epsilon=0.1, epsilon_decay = 0.9):\n",
    "    \"\"\"\n",
    "    Control by Monte Carlo methods using Epsilon-Greedy policies\n",
    "    Find an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "         env: Gymnasium environment.\n",
    "         num_episodes: Number of episodes in the sample.\n",
    "         discount: discount factor.\n",
    "         epsilon: The probability of selecting a random action (float between 0 and 1)\n",
    "    \n",
    "    Returns:\n",
    "         A tuple (Q, policy).\n",
    "         Q: A dictionary whose correspondence is state -> action-values.\n",
    "         policy: A function that takes the observation as an argument and returns as a result\n",
    "                 the probabilities of each action\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solution</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M2.883_PEC1_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
