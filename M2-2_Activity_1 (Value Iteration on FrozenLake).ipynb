{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xrRvDDNLUxK"
   },
   "source": [
    "# **Tabular Reinforcement Learning**\n",
    "\n",
    "# Value Iteration on FrozenLake\n",
    "\n",
    "## Non-Evaluables Practical Exercices\n",
    "\n",
    "This is a non-evaluable practical exercise, but it is recommended that students complete it fully and individually, since it is an important part of the learning process.\n",
    "\n",
    "The solution will be available, although it is not recommended that students consult the solution until they have completed the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gee4aY31LUxL"
   },
   "source": [
    "## The FrozenLake environment\n",
    "\n",
    "In this activity, we are going to implement the **Value Iteration** algorithm on [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment.\n",
    "\n",
    "Main characteristics:\n",
    "- The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "- Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
    "- The player makes moves until they reach the goal or fall in a hole.\n",
    "- The lake is slippery (unless disabled) so the player may move perpendicular to the intended direction sometimes (see _is_slippery_ param).\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# params\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "# definig the environment\n",
    "env = gym.make(ENV_NAME, desc=None, map_name=\"4x4\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Agent\n",
    "\n",
    "In order to implement an Agent and train it using the **Value Iteration** algorithm, we have to follow these steps:\n",
    "1. Populate a table with the transition probabilities from specific state (`transits`)\n",
    "2. Populate a table with the immediate rewards (`rewards`)\n",
    "3. Compute the Value Iteration algorithm and populate a table with the expected reward of an state (`values`)\n",
    "4. Define the policy and create a function to select an action from each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central data structures in this example are as follows: \n",
    "- **Reward table**: A dictionary with the composite key \"source state\" + \"action\" + \"target state\". The value is obtained from the immediate reward. \n",
    "- **Transitions table**: A dictionary keeping counters of the experienced transitions. The key is the composite \"state\" + \"action\" and the value is another dictionary that maps the target state into a count of times that we've seen it. \n",
    "\n",
    "For example, if in state 0 we execute action 1 ten times, after three times it leads us to state 4 and after seven times to state 5. Entry with the key (0, 1) in this table will be a `dict {4: 3, 5: 7}`. We use this table to estimate the probabilities of our transitions. \n",
    "\n",
    "- **Value table**: A dictionary that maps a state into the calculated value of this state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state = self.env.reset()[0]\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "        \n",
    "    def play_n_random_steps(self, count) -> None:\n",
    "        '''\n",
    "        Play n random steps to update info:\n",
    "        - self.rewards[(state, action, new_state)] = reward\n",
    "        - self.transits[(self.state, action)][new_state] += 1\n",
    "        '''\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    def calc_action_value(self, state, action) -> float:\n",
    "        '''\n",
    "        Calculate the action-state value function\n",
    "        '''\n",
    "        # TODO\n",
    "\n",
    "        \n",
    "    def select_action(self, state) -> int:\n",
    "        '''\n",
    "        Select the best action for a given state\n",
    "        '''\n",
    "        # TODO\n",
    "        \n",
    "    \n",
    "    def value_iteration(self) -> None:\n",
    "        '''\n",
    "        Calculate the state value\n",
    "        - self.values[state] = max(state_values)\n",
    "        '''\n",
    "        # TODO\n",
    "\n",
    "            \n",
    "    def play_episode(self, env) -> float:\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            is_done = terminated or truncated\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall logic of our code is simple: \n",
    "1. In the loop, we play 100 random steps from the environment, populating the reward and transition tables. \n",
    "2. After those 100 steps, we perform a value iteration loop over all states, updating our value table. \n",
    "3. Then we play several full episodes to check our improvements using the updated value table. \n",
    "4. If the average reward for those test episodes is above the 0.8 boundary, then we stop training. During test episodes, we also update our reward and transition tables to use all data from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.value_iteration()\n",
    "\n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(env)\n",
    "    \n",
    "    reward /= TEST_EPISODES\n",
    "    \n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "        best_reward = reward\n",
    "    \n",
    "    if reward > 0.80:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JCyYmwRLUxM"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solution</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
