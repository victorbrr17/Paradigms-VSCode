{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tabular Reinforcement Learning**\n",
    "\n",
    "# Q-Learning on FrozenLake environment\n",
    "\n",
    "## Non-Evaluables Practical Exercices\n",
    "\n",
    "This is a non-evaluable practical exercise, but it is recommended that students complete it fully and individually, since it is an important part of the learning process.\n",
    "\n",
    "The solution will be available, although it is not recommended that students consult the solution until they have completed the exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FrozenLake environment\n",
    "\n",
    "In this activity, we are going to solve the [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment.\n",
    "\n",
    "Main characteristics:\n",
    "- The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "- Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
    "- The player makes moves until they reach the goal or fall in a hole.\n",
    "- The lake is slippery (unless disabled) so the player may move perpendicular to the intended direction sometimes (see _is_slippery_ param).\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning algorithm\n",
    "\n",
    "<u>Question 1</u>: : **Implement the *Q-Learning* algorithm** using the following parameters:\n",
    "\n",
    "- Number of episodes = 15,000\n",
    "- *learning rate* = 0.8\n",
    "- *discount factor* = 1\n",
    "\n",
    "Additionally, implement two **$\\epsilon$-Greedy with decay factor** methods with the following parameters:\n",
    "\n",
    "**Exponential Decay Factor**:\n",
    "- *max_epsilon* = 1.0\n",
    "- *min_epsilon* = 0.01\n",
    "- *decay_rate* = 0.005\n",
    "\n",
    "**Linear Decay Factor**:\n",
    "- *max_epsilon* = 1.0\n",
    "- *min_epsilon* = 0.1\n",
    "- Number of frames to reach *min_epsilon* = 1000\n",
    "\n",
    "<u>Question 2</u>: Once you have coded the algorithm, try different **values for the hyperparameters** and comment the best ones (providing an empirical comparison):\n",
    "\n",
    "- Number of episodes\n",
    "- *learning rate* \n",
    "- *discount factor* \n",
    "- *epsilon* values (including min value and decay factor)\n",
    "\n",
    "<u>Question 3</u>: Try to solve the same environment but using a _8 x 8_ grid (also in slippery mode):\n",
    "> gym.make(ENV_NAME, desc=None, map_name=\"8x8\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gymnasium version is 1.2.0 \n",
      "Action space is Discrete(4) \n",
      "Observation space is Discrete(16) \n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# definig the environment\n",
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "print(\"Gymnasium version is {} \".format(gym.__version__))\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
    "    '''\n",
    "    Create a policy where epsilon dictates the probability of a random action being carried out.\n",
    "\n",
    "    :param Q: link state -> action value (dictionary)\n",
    "    :param state: state in which the agent is (int)\n",
    "    :param nA: number of actions (int)\n",
    "    :param epsilon: possibility of random movement (float)\n",
    "    :return: probability of each action (list) d\n",
    "    '''\n",
    "    probs = np.ones(nA) * epsilon / nA\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += 1.0 - epsilon\n",
    "\n",
    "    return probs\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 0.995  # Typical decay rate for exponential decay\n",
    "EPS_MIN = 0.01\n",
    "EPISODES = 15000\n",
    "LEARNING_RATE = 0.8\n",
    "DISCOUNT = 1.0\n",
    "\n",
    "\n",
    "def QLearning(episodes, learning_rate, discount, epsilon, epsilon_start, epsilon_decay, epsilon_min):\n",
    "    '''\n",
    "    Learn to solve the environment using the Q-Learning algorithm\n",
    "\n",
    "    :param episodes: Number of episodes (int)\n",
    "    :param learning_rate: Learning rate (float [0, 1])\n",
    "    :param discount: Discount factor (float [0, 1])\n",
    "    :param epsilon: chance that random movement is required (float [0, 1])\n",
    "    :return: x,y number of episodes and number of steps\n",
    "    :Q: action value function\n",
    "    '''\n",
    "\n",
    "    # Link actions to states\n",
    "\n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    # Number of episodes\n",
    "    x = np.arange(episodes)\n",
    "    y = np.zeros(episodes)\n",
    "    epsilon=epsilon_start\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        # Select and execute an action\n",
    "        probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
    "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        \n",
    "        done = False\n",
    "        step = 1\n",
    "                \n",
    "        while not done:\n",
    "            # Execute action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            # Select and execute action\n",
    "            probs = epsilon_greedy_policy(Q, next_state, env.action_space.n, epsilon)\n",
    "            next_action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "           \n",
    "            # Update TD\n",
    "            td_target = reward + discount * Q[next_state][next_action]\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += learning_rate * td_error\n",
    "                        \n",
    "            if done:\n",
    "                y[episode] = step\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            step += 1\n",
    "            epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    return x, y, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solution</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning with exponential epsilon decay\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 0.005\n",
    "EPS_MIN = 0.11\n",
    "EPISODES = 15000\n",
    "LEARNING_RATE = 0.8\n",
    "DISCOUNT = 1.0\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
    "    probs = np.ones(nA) * epsilon / nA\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += 1.0 - epsilon\n",
    "    return probs\n",
    "\n",
    "def QLearning(episodes, learning_rate, discount, epsilon_start, epsilon_decay, epsilon_min):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    x = np.arange(episodes)\n",
    "    y = np.zeros(episodes)\n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        step = 1\n",
    "        while not done:\n",
    "            probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + discount * Q[next_state][best_next_action]\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += learning_rate * td_error\n",
    "            if done:\n",
    "                y[episode] = step\n",
    "                break\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        # Exponential epsilon decay\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    return x, y, Q\n",
    "\n",
    "# Run Q-Learning\n",
    "x, y, Q = QLearning(EPISODES, LEARNING_RATE, DISCOUNT, EPS_START, EPS_DECAY, EPS_MIN)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M2.883_PEC1_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
