{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tabular Reinforcement Learning**\n",
    "\n",
    "# SARSA on FrozenLake environment\n",
    "\n",
    "## Non-Evaluables Practical Exercices\n",
    "\n",
    "This is a non-evaluable practical exercise, but it is recommended that students complete it fully and individually, since it is an important part of the learning process.\n",
    "\n",
    "The solution will be available, although it is not recommended that students consult the solution until they have completed the exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FrozenLake environment\n",
    "\n",
    "In this activity, we are going to implement the **Value Iteration** algorithm on [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment.\n",
    "\n",
    "Main characteristics:\n",
    "- The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "- Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
    "- The player makes moves until they reach the goal or fall in a hole.\n",
    "- The lake is slippery (unless disabled) so the player may move perpendicular to the intended direction sometimes (see _is_slippery_ param).\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "<u>Question 1</u>: : **Implement the *SARSA* algorithm** explained in the \"Temporal Difference Learning\" module using the following parameters:\n",
    "\n",
    "- Number of episodes = 1000000\n",
    "- *learning rate* = 0.5\n",
    "- *discount factor* = 1\n",
    "- *epsilon* = 0.05  \n",
    "\n",
    "<u>Question 2</u>: Once you have coded the algorithm, try different **values for the hyperparameters** and comment the best ones (providing an empirical comparison):\n",
    "\n",
    "- Number of episodes\n",
    "- *learning rate* \n",
    "- *discount factor* \n",
    "- *epsilon*\n",
    "\n",
    "<u>Question 3</u>: Try to solve the same environment but using a _8 x 8_ grid (also in slippery mode):\n",
    "> gym.make(ENV_NAME, desc=None, map_name=\"8x8\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gymnasium version is 1.2.0 \n",
      "Action space is Discrete(4) \n",
      "Observation space is Discrete(16) \n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# params\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 1000000\n",
    "\n",
    "# definig the environment\n",
    "env = gym.make(ENV_NAME, desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "print(\"Gymnasium version is {} \".format(gym.__version__))\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
    "    '''\n",
    "    Create a policy where epsilon dictates the probability of a random action being carried out.\n",
    "\n",
    "    :param Q: link state -> action value (dictionary)\n",
    "    :param state: state in which the agent is (int)\n",
    "    :param nA: number of actions (int)\n",
    "    :param epsilon: possibility of random movement (float)\n",
    "    :return: probability of each action (list) d\n",
    "    '''\n",
    "\n",
    "    probs = np.ones(nA) * epsilon / nA\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += 1.0 - epsilon\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "def SARSA(episodes, learning_rate, discount, epsilon):\n",
    "    '''\n",
    "    Learn to solve the environment using the SARSA algorithm\n",
    "\n",
    "    :param episodes: Number of episodes (int)\n",
    "    :param learning_rate: Learning rate (float [0, 1])\n",
    "    :param discount: Discount factor (float [0, 1])\n",
    "    :param epsilon: chance that random movement is required (float [0, 1])\n",
    "    :return: x,y number of episodes and number of steps\n",
    "    :Q: action value function\n",
    "    '''\n",
    "\n",
    "    # Link actions to states\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    # Number of episodes\n",
    "    x = np.arange(episodes)\n",
    "    y = np.zeros(episodes)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        # Select and execute an action\n",
    "        probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
    "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        \n",
    "        done = False\n",
    "        step = 1\n",
    "                \n",
    "        while not done:\n",
    "            # Execute action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            # Select and execute action\n",
    "            probs = epsilon_greedy_policy(Q, next_state, env.action_space.n, epsilon)\n",
    "            next_action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "           \n",
    "            # Update TD\n",
    "            td_target = reward + discount * Q[next_state][next_action]\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += learning_rate * td_error\n",
    "                        \n",
    "            if done:\n",
    "                y[episode] = step\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            step += 1\n",
    "                 \n",
    "    return x, y, Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solution</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, q = SARSA(episodes=100000, learning_rate=0.5, discount=1, epsilon=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution of an episode following the optimal policy\n",
    "def execute_episode_SARSA(q, env):\n",
    "    obs = env.reset()[0]\n",
    "    t, total_reward, done = 0, 0, False\n",
    "\n",
    "    print(\"Obs initial: {} \".format(obs))\n",
    "\n",
    "    switch_action = {\n",
    "            0: \"U\",\n",
    "            1: \"R\",\n",
    "            2: \"D\",\n",
    "            3: \"L\",\n",
    "        }\n",
    "\n",
    "    for t in range(1000): # We limit the number of time-steps in each episode to 1000\n",
    "        # Choose a stock following the optimal policy\n",
    "        arr = np.array(q[obs])\n",
    "        action = arr.argmax()\n",
    "       \n",
    "        # Execute the action and wait for the response from the environment\n",
    "        new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        obs = new_obs\n",
    "        print(\"Action: {} -> Obs: {} and reward: {}\".format(switch_action[action], obs, reward))\n",
    "\n",
    "        if t==999:\n",
    "            print(\"Number of time-septs exceeds 1000. STOP episode.\") \n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "        if done:\n",
    "            break\n",
    "   \n",
    "    print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs initial: 0 \n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 8 and reward: 0.0\n",
      "Action: L -> Obs: 8 and reward: 0.0\n",
      "Action: L -> Obs: 8 and reward: 0.0\n",
      "Action: L -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 0 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 8 and reward: 0.0\n",
      "Action: L -> Obs: 9 and reward: 0.0\n",
      "Action: R -> Obs: 8 and reward: 0.0\n",
      "Action: L -> Obs: 9 and reward: 0.0\n",
      "Action: R -> Obs: 8 and reward: 0.0\n",
      "Action: L -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 4 and reward: 0.0\n",
      "Action: U -> Obs: 8 and reward: 0.0\n",
      "Action: L -> Obs: 9 and reward: 0.0\n",
      "Action: R -> Obs: 10 and reward: 0.0\n",
      "Action: R -> Obs: 11 and reward: 0.0\n",
      "Episode finished after 36 timesteps and reward was 0.0 \n"
     ]
    }
   ],
   "source": [
    "execute_episode_SARSA(q, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M2.883_PEC1_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
