{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tabular Reinforcement Learning**\n",
    "\n",
    "# Monte Carlo methods on FrozenLake environment\n",
    "\n",
    "## Non-Evaluables Practical Exercices\n",
    "\n",
    "This is a non-evaluable practical exercise, but it is recommended that students complete it fully and individually, since it is an important part of the learning process.\n",
    "\n",
    "The solution will be available, although it is not recommended that students consult the solution until they have completed the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FrozenLake environment\n",
    "\n",
    "In this activity, we are going to implement the **Value Iteration** algorithm on [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment.\n",
    "\n",
    "Main characteristics:\n",
    "- The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "- Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
    "- The player makes moves until they reach the goal or fall in a hole.\n",
    "- The lake is slippery (unless disabled) so the player may move perpendicular to the intended direction sometimes (see _is_slippery_ param).\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo methods\n",
    "\n",
    "In this section, we implement an estimation of the **optimal policy** using **Monte Carlo methods**, specifically we will study the **_On-policy every-visit MC control algorithm (for $\\epsilon$-soft policies)_**.\n",
    "\n",
    "<u>Question 1</u>: : **Implement the algorithm** using the following parameters:\n",
    "\n",
    "- Number of episodes = 100,000\n",
    "- Discount factor = 1\n",
    "\n",
    "<u>Question 2</u>: : **Implement the epsilon decay factor** using the following equation and parameters:\n",
    "\n",
    "- Initial epsilon = 1\n",
    "- Epsilon decay factor (*epsilon decay*) = 0.999\n",
    "- Update epsilon according to: $\\textrm{max}(\\epsilon Â· \\epsilon_{\\textrm{decay}}, 0.01)$\n",
    "\n",
    "<u>Question 3</u>: Once you have coded the algorithm, try different **values for the hyperparameters** and comment the best ones (providing an empirical comparison):\n",
    "\n",
    "- Number of episodes\n",
    "- Initial epsilon\n",
    "- Epsilon decay factor (*epsilon decay*)\n",
    "- *discount factor* \n",
    "\n",
    "<u>Question 4</u>: Try to solve the same environment but using a _8 x 8_ grid (also in slippery mode):\n",
    "\n",
    "> gym.make(ENV_NAME, desc=None, map_name=\"8x8\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "h5y5sIqBPPxr",
    "outputId": "853c4fb1-0806-4393-f51c-a9098d9eddc5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(4) \n",
      "Observation space is Discrete(16) \n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# params\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "# definig the environment\n",
    "env = gym.make(ENV_NAME, desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, num_Actions):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a Q and epsilon action value function\n",
    "    \n",
    "    Args:\n",
    "         Q: A dictionary whose correspondence is state -> action-values.\n",
    "            Each value is a numpy array of length num_Actions (see below)\n",
    "         epsilon: The probability of selecting a random action (float between 0 and 1).\n",
    "         num_Actions: Number of actions in the environment (in the case of WIndyGridWorld, it is 4)\n",
    "    \n",
    "    Returns:\n",
    "         A function that takes the observation as an argument and returns as a result\n",
    "         the probabilities of each action as a numpy array of length num_Actions.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "def mc_control_on_policy_epsilon_greedy(env, num_episodes, discount=1.0, epsilon=0.1, epsilon_decay = 0.9):\n",
    "    \"\"\"\n",
    "    Control by Monte Carlo methods using Epsilon-Greedy policies\n",
    "    Find an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "         env: Gymnasium environment.\n",
    "         num_episodes: Number of episodes in the sample.\n",
    "         discount: discount factor.\n",
    "         epsilon: The probability of selecting a random action (float between 0 and 1)\n",
    "    \n",
    "    Returns:\n",
    "         A tuple (Q, policy).\n",
    "         Q: A dictionary whose correspondence is state -> action-values.\n",
    "         policy: A function that takes the observation as an argument and returns as a result\n",
    "                 the probabilities of each action\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solution</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# writer\n",
    "writer = SummaryWriter(comment=\"-mc_control\")\n",
    "\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, num_Actions):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a Q and epsilon action value function\n",
    "    \n",
    "    Args:\n",
    "         Q: A dictionary whose correspondence is state -> action-values.\n",
    "            Each value is a numpy array of length num_Actions (see below)\n",
    "         epsilon: The probability of selecting a random action (float between 0 and 1).\n",
    "         num_Actions: Number of actions in the environment. (in the case of WIndyGridWorld it is 4)\n",
    "    \n",
    "    Returns:\n",
    "         A function that takes the observation as an argument and returns as a result\n",
    "         the probabilities of each action as a numpy array of length num_Actions.\n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(num_Actions, dtype=float) * epsilon / num_Actions\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "def mc_control_on_policy_epsilon_greedy(env, num_episodes, discount=1.0, epsilon=0.1, epsilon_decay=0.9):\n",
    "    \"\"\"\n",
    "    Control by Monte Carlo methods using Epsilon-Greedy policies\n",
    "    Find an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "         env: Gymnasium environment.\n",
    "         num_episodes: Number of episodes in the sample.\n",
    "         discount: discount factor.\n",
    "         epsilon: The probability of selecting a random action (float between 0 and 1)\n",
    "    \n",
    "    Returns:\n",
    "         A tuple (Q, policy).\n",
    "         Q: A dictionary whose correspondence is state -> action-values.\n",
    "         policy: A function that takes the observation as an argument and returns as a result\n",
    "                 the probabilities of each action\n",
    "    \"\"\"\n",
    "    \n",
    "    # We store the sum and number of returns for each state to calculate the average. \n",
    "    # We could use an array to store all the returns, but it is inefficient in terms of memory.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The Q action value function.\n",
    "    # A nested dictionary whose correspondence is state -> (action -> action-value).\n",
    "    # Initially we initialize it to zero\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Rewards\n",
    "    y = np.zeros(num_episodes, dtype=np.float16)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # The policy we are following\n",
    "        policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "        \n",
    "        # We update epsilon\n",
    "        epsilon = max(epsilon * epsilon_decay, 0.01)\n",
    "\n",
    "        # We generate an episode and store it\n",
    "        # An episode is an array of tuples (state, action, reward)\n",
    "        episode = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode.append((state, action, reward))\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        y[i_episode] = total_reward\n",
    "        idx = 0\n",
    "        \n",
    "        # We visited all the states of the episode\n",
    "        G = 0\n",
    "        for i in range(len(episode)-1, -1, -1):\n",
    "            x = episode[i]\n",
    "            state = x[0]\n",
    "            action = x[1]\n",
    "            reward = x[2]\n",
    "            sa_pair = (state, action)\n",
    "            # We calculate the return of each state\n",
    "            G = reward + discount * G\n",
    "            # We calculate the average return for this state across all sampled episodes.   \n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "            idx = idx + 1\n",
    "        \n",
    "        # write to summary\n",
    "        writer.add_scalar(\"reward\", total_reward, i_episode)\n",
    "        \n",
    "        # We print which episode we are in, useful for debugging.\n",
    "        if i_episode % 100 == 0 and i_episode > 0:\n",
    "            print(\"\\rEpisode {:8d}/{:8d} - Average reward {:.2f}\".format(i_episode, num_episodes, np.average(y[(i_episode-100):i_episode])), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # The policy is implicitly improved by changing the values of Q\n",
    "    \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    49900/   50000 - Average reward 0.99"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "Q, policy = mc_control_on_policy_epsilon_greedy(env, num_episodes=50000, discount=1, epsilon=1, epsilon_decay=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# execution of an episode following the optimal policy\n",
    "def execute_episode_MC(q, env, debug=False):\n",
    "    obs, _ = env.reset()\n",
    "    t, total_reward, done = 0, 0, False\n",
    "\n",
    "    if debug:\n",
    "        print(\"Obs initial: {} \".format(obs))\n",
    "\n",
    "    while True: \n",
    "        # Choose an action following the optimal policy (no epsilon-greedy)\n",
    "        arr = np.array(q[obs])\n",
    "        action = arr.argmax()\n",
    "       \n",
    "        # Execute the action and wait for the response from the environment\n",
    "        new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        obs = new_obs\n",
    "        if debug:\n",
    "            print(\"Action: {} -> Obs: {} and reward: {}\".format(action, obs, reward))\n",
    "\n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "        if done:\n",
    "            break\n",
    "   \n",
    "    print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs initial: 0 \n",
      "Action: 1 -> Obs: 4 and reward: 0.0\n",
      "Action: 1 -> Obs: 8 and reward: 0.0\n",
      "Action: 2 -> Obs: 9 and reward: 0.0\n",
      "Action: 1 -> Obs: 13 and reward: 0.0\n",
      "Action: 2 -> Obs: 14 and reward: 0.0\n",
      "Action: 2 -> Obs: 15 and reward: 1.0\n",
      "Episode finished after 6 timesteps and reward was 1.0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_episode_MC(Q, env, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Episode finished after 6 timesteps and reward was 1.0 \n",
      "Average reward is 1.0 (20 episodes)\n",
      "Environment solved!\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(comment=\"-mc_test\")\n",
    "\n",
    "total_reward = 0.0\n",
    "for iter_no in range(TEST_EPISODES):\n",
    "    reward = execute_episode_MC(Q, env)\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    total_reward += reward\n",
    "\n",
    "avg_reward = total_reward / TEST_EPISODES\n",
    "print(\"Average reward is {} ({} episodes)\".format(avg_reward, TEST_EPISODES))\n",
    "if reward > 0.80:\n",
    "    print(\"Environment solved!\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M2.883_PEC1_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gymnasium_v1-2-0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
